第章数据本章讨论一些与数据相关的问题,它们对于数据挖掘的成败至关重要。数据类型数据集的不同表现在多方面例如,用来描述数据对象的属性可以具有不同的类型定量的或定性的,并且数据集可能具有特定的性质,例如,某些数据集包含时间序列或彼此之间具有明显联系的对象。毫不奇怪,数据的类型决定我们应使用何种工具和技术来分析数据。此外,数据挖掘研究常常是为了适应新的应用领域和新的数据类型的需要而展开的。数据的质量数据通常远非完美。尽管大部分数据挖掘技术可以忍受某种程度的数据不完美,但是注重理解和提高数据质量将改进分析结果的质量。通常必须解决的数据质量问题包括存在噪声和离群点,数据遗漏、不一致或重复,数据有偏差或者不能代表它应该描述的现象或总体情况。使数据适合挖掘的预处理步骤通常,原始数据必须加以处理才能适合于分析。处理一方面是要提高数据的质量,另一方面要让数据更好地适应特定的数据挖掘技术或工具。例如,可能需要将连续值属性(如长度)转换成具有离散的分类值的属性(如短、中、长),以便应用特定的技术。又如,数据集属性的数目常常需要减少,因为属性较少时许多技术用起来更加有效。根据数据联系分析数据数据分析的一种方法是找出数据对象之间的联系,之后使用这些联系而不是数据对象本身来进行其余的分析。例如,我们可以计算对象之间的相似度或距离,然后根据这种相似度或距离进行分析—聚类、分类或异常检测。诸如此类的相似性或距离度量很多,要根据数据的类型和特定的应用做出正确的选择。例2.1与数据相关的问题为了进一步解释这些问题的重要性,考虑下面的假想情况。你收到某个医学研究者发来的电子邮件,是关于你想要研究的一个项目的。邮件的内容如下:你好,我已附上先前邮件提及的数据文件。每行包含一个病人的信息,由5个字段组成我们想使用前面4个字段预测最后一个字段。因为我要出去几天,所以没有时间为你提供关于这些数据的更多信息,但希望不会耽误你太多时间。如果你不介意的话,我回来之后是否可以开会讨论你的初步结果?我可能会邀请我们小组的其他成员参加。谢谢!几天之后见!尽管有些疑虑,你还是开始着手分析这些数据。文件的前几行如下: POG01223233.5010.7
02012116.92210.102716524.00427.6粗略观察这些数据并未发现什么不对。你抛开疑虑,并开始分析。数据文件只有1000行,比你希望的小,仅仅两天之后你认为你已经取得一些进展。你去参加会议,在等待其他人时,你开始与一位参与该项目工作的统计人员交谈。当听说你正在分析该项目的数据时,她请你向她简略介绍你的结果。统计人员:哦,你得到了所有病人的数据?数据挖掘者:是的。我还没有足够的时间分析,但是我的确有了一些有趣的结果。统计人员:真棒。病人数据集的数据问题太多,我没什么进展。数据挖掘者:啊?我没有听到任何问题。统计人员:喔,首先是字段5,这是我们要预测的变量。分析这类数据的人都知道,如果使用这些值的日志,结果会更好,但是我们后来才发现这一点。他们告诉你了吗?数据挖掘者:没有。统计人员:你一定听说过字段4的问题了吧?它的测量范围应当是1到10,而0表示有遗漏的值。但是,由于数据输入错误,所有的10都变成了0可是,由于有些病人这个字段的值有遗漏,所以不能确定该字段上的0实际是0还是10不少记录都存在此问题。数据挖掘者:有意思。还有其他问题吗?统计人员:是的。字段2和3也有不少问题。我猜想你可能已经注意到了。数据挖掘者:是的。但是,这些字段只是字段5的弱预测子统计人员:无论如何,尽管有这些问题,你还能够完成一些分析,这真让人吃惊。数据挖掘者:实际上,我的结果相当好。字段1是字段5的很强的预测子我很奇怪以前怎么没人注意到。统计人员:什么?字段1只是一个标识号。数据挖掘者:无论如何,我的结果在那儿。统计人员:啊,不!我才想起来。在按字段5排序记录之后,我们加上了一个ID号它们之间是存在很强的联系,但毫无意义。很抱歉!口尽管这一场景代表一种极端情况,但它强调“了解数据”的重要性。为此,本章将处理上面提到的四个问题,列举一些基本难点和标准解决方法。2.1数据类型通常,数据集可以看作数据对象的集合。数据对象有时也叫做记录、点、向量、模式、事件、案例、样本、观测或实体。数据对象用一组刻画对象基本特性(如物体质量或事件发生时间)的例2.2学生信息通常,数据集是一个文件,其中对象是文件的记录(或行),而每个字PDG属性描述。属性有时也叫做变量、特性、字段、特征或维。段(或列)对应于一个属性。例如,表21显示包含学生信息的数据集。每行对应于一个学生,
而每列是一个属性,描述学生的某一方面,如平均成绩(GPA)或标识号(ID)表2-1包含学生信息的样本数据集学生ID年级平均成绩(GPA)1034262四年级3.241052663二年级3.511082246一年级362口基于记录的数据集在平展文件或关系数据库系统中是最常见的,但是还有其他类型的数据集和存储数据的系统。在2.1.2节,我们将讨论数据挖掘经常遇到的其他类型的数据集。然而,我们先考虑属性。2.1.1属性与度量本节我们考虑使用何种类型的属性描述数据对象,来处理描述数据的问题。我们首先定义属性,然后考虑属性类型的含义,最后介绍经常遇到的属性类型。1.什么是属性我们先更详细地定义属性。定义2.1属性(attribute)是对象的性质或特性,它因对象而异,或随时间而变化。例如,眼球颜色因人而异,而物体的温度随时间而变。注意:眼球颜色是一种符号属性,具有少量可能的值{棕色,黑色,蓝色,绿色,淡褐色,…而温度是数值属性,可以取无穷多个值。追根溯源,属性并非数字或符号。然而,为了讨论和精细地分析对象的特性,我们为它们赋予了数字或符号。为了用一种明确定义的方式做到这一点,我们需要测量标度。定义2.2测量标度( measurement scale)是将数值或符号值与对象的属性相关联的规则(函数)。形式上,测量过程是使用测量标度将一个值与一个特定对象的特定属性相关联。这看上去有点抽象,但是任何时候,我们总在进行这样的测量过程。例如,踏上浴室的磅秤称体重;将人分为男女;清点会议室的椅子数目,确定是否能够为所有与会者提供足够的座位。在所有这些情况下,对象属性的“物理值”都被映射到数值或符号值。有了这些背景,现在我们可以讨论属性类型,这对于确定特定的数据分析技术是否适用于某种具体的属性是一个重要的概念。2.属性类型从前面的讨论显而易见,属性的性质不必与用来度量它的值的性质相同。换句话说,用来代表属性的值可能具有不同于属性本身的性质,并且反之亦然。我们用两个例子解释。例2.3雇员年龄和D号与雇员有关的两个属性是和年龄,这两个属性都可以用整数表示。然而,谈论雇员的平均年龄是有意义的但是谈论雇员的平均却毫无意义。的确,我们希望ID属性所表达的唯一方面是它们互不相同。因而,对雇员的唯一合法操作就是判定它
们是否相等。但在使用整数表示雇员ID时,并没暗示有此限制。对于年龄属性而言,用来表示年龄的整数的性质与该属性的性质大同小异。尽管如此,这种对应仍不完备,例如,年龄有最大值,而整数没有。口例2.4线段长度考虑图2-1,它展示一些线段对象和如何用两种不同的方法将这些对象的长度属性映射到整数。从上到下,每条后继线段都是通过最上面的线段自我添加而形成的。这样,第二条线段是最上面的线段两次相连而形成的,第三条线段是最上面的线段三次相连而形成的,依次类推。从物理意义上讲,所有的线段都是第一条线段的倍数。这个事实由图右边的测量捕获,但未被左边的测量捕获。更准确地说,左边的测量标度仅仅捕获长度属性的序,而右边的标度同时捕获序和可加性的性质。因此,属性可以用一种不描述属性全部性质的方式测量。口1←13←27→38←-→410←-------5长度到整数的映射,仅长度到整数的映射,同时捕捕获长度的序性质获长度的序性质和可加性图21两种不同的测量标度下的线段长度测量属性的类型告诉我们,属性的哪些性质反映在用于测量它的值中。知道属性的类型是重要的,因为它告诉我们测量值的哪些性质与属性的基本性质一致,从而使得我们可以避免诸如计算雇员的平均ID这样的愚蠢行为。注意,通常将属性的类型称作测量标度的类型。3.属性的不同类型一种指定属性类型的有用(和简单)的办法是,确定对应于属性基本性质的数值的性质。例如,长度的属性可以有数值的许多性质。按照长度比较对象,确定对象的排序,以及谈论长度的差和比例都是有意义的。数值的如下性质(操作)常常用来描述属性(1)相异性=和≠(2)序<、≤、>和≥(3)加法+和一(4)乘法*和给定这些性质,我们可以定义四种属性类型:标称( nominal)、序数(ordinal)、区间(interval)和比率(ratio)表2-2给出这些类型的定义,以及每种类型上有哪些合法的统计操作等信息。每种属性类型拥有其上方属性类型上的所有性质和操作因此,对于标称、序数和区间属性合法的任何性质或操作,对于比率属性也合法。换句话说,属性类型的定义是累积的。当然,对于某种属性类型合适的操作,对其上方的属性类型就不一定合适。
表2-2不同的属性类型属性类型描述例子操作标称标称属性的值仅仅只是不邮政编码、雇员众数、熵、列联相关、同的名字,即标称值只提供足号、眼球颜色、性别x2检验够的信息以区分对象分类的(定性的)序数(=,≠)序数属性的值提供足够的矿石硬度、{好,好,中值、百分位、秩相关、信息确定对象的序最好}、成绩、街道号码游程检验、符号检验(<,>)区间对于区间属性,值之间的差日历日期、摄氏或华均值、标准差、皮尔逊是有意义的,即存在测量单位氏温度相关、t和F检验数值的(+,(定量的)比率对于比率变量,差和比率都绝对温度、货币量、几何平均、调和平均、是有意义的计数、年龄、质量、长百分比变差(*,度、电流标称和序数属性统称分类的(categorical)或定性的(qualitative)属性。顾名思义,定性属性(如雇员ID)不具有数的大部分性质。即便使用数(即整数)表示,也应当像对待符号一样对待它们。其余两种类型的属性,即区间和比率属性,统称定量的(quantitative)或数值的( numeric)属性。定量属性用数表示,并且具有数的大部分性质。注意:定量属性可以是整数值或连续值。属性的类型也可以用不改变属性意义的变换来描述。实际上,心理学家s. Smith Stevens最先用允许的变换(permissible transformation)定义了表2-2所示的属性类型。例如,如果长度分别用米和英尺度量,其属性的意义并未改变。对特定的属性类型有意义的统计操作是这样一些操作,当使用保持属性意义的变换对属性进行变换时,它们产生的结果相同。例如,用米和英尺为单位进行度量时,同一组对象的平均长度数值是不同的,但是两个平均值都代表相同的长度。表2-3给出表2-2中四种属性类型的允许的(保持意义的)变换。表2-3定义属性层次的变换属性类型变换注释标称任何一对一变换,例如值的一个排列如果所有雇员的号都重新赋值,不会出现任何不同分类的定性的)序数值的保序变换即包括好、较好、最好的属性可以完全等新值=f(旧值)价地用值(1,2,3}或用(0.5,1,10}表示其中f是单调函数数值的区间新值=a旧值+b华氏和摄氏温度的零度的位置不同,1(定量的)其中a、b是常数度的大小(即单位长度)也不同比率新值=a*旧值长度可以用米或英尺度量例2.5温度标度温度可以很好地解释前面介绍的一些概念。首先,温度可以是区间属性或比率属性,这取决于其测量标度。当温度用绝对标度测量时,从物理意义上讲,2°的温度是1°的两倍;当温度用华氏或摄氏标度测量时则并非如此,因为这时1°温度与2°温度相差并不太多。问题是从物理意义上讲,华氏和摄氏标度的零点是硬性规定的,因此,华氏或摄氏温度的比率并无物理意义。4.用值的个数描述属性 PDG口区分属性的一种独立方法是根据属性可能取值的个数来判断。
·离散的(discrete)离散属性具有有限个值或无限可数个值。这样的属性可以是分类的,如邮政编码或ID号,也可以是数值的,如计数。通常,离散属性用整数变量表示。二元属性(binary attribute)是离散属性的一种特殊情况,并只接受两个值,如真/假、是否、男/女或0/1。通常,二元属性用布尔变量表示,或者用只取两个值0或1的整型变量表示●连续的(continuous)连续属性是取实数值的属性。如温度、高度或重量等属性。通常,连续属性用浮点变量表示。实践中,实数值只能用有限的精度测量和表示。从理论上讲,任何测量标度类型(标称的序数的、区间的和比率的)都可以与基于属性值个数的任意类型(二元的、离散的和连续的)组合。然而,有些组合并不常出现,或者没有什么意义。例如,很难想象一个实际数据集包含连续的二元属性。通常,标称和序数属性是二元的或离散的,而区间和比率属性是连续的。然而,计数属性(count attribute)是离散的,也是比率属性。5.非对称的属性对于非对称的属性(asymmetric attribute),出现非零属性值才是重要的。考虑这样一个数据集,其中每个对象是一个学生,而每个属性记录学生是否选修大学的某个课程。对于某个学生,如果他选修了对应于某属性的课程,该属性取值1,否则取值0。由于学生只选修所有可选课程中的很小一部分,这种数据集的大部分值为0。因此,关注非零值将更有意义、更有效。否则,如果在学生们不选修的课程上作比较,则大部分学生都非常相似。只有非零值才重要的二元属性是非对称的二元属性。这类属性对于关联分析特别重要。关联分析在第6章讨论。也可能有离散的或连续的非对称特征。例如,如果记录每门课程的学分,则结果数据集将包含非对称的离散属性或连续属性。2.1.2数据集的类型数据集的类型有多种,并且随着数据挖掘的发展与成熟,还会有更多类型的数据集将用于分析。本节我们介绍一些很常见的类型。为方便起见,我们将数据集类型分成三组:记录数据、基于图形的数据和有序的数据。这些分类不能涵盖所有的可能性,肯定还存在其他的分组。1.数据集的一般特性在提供特定类型数据集的细节之前,我们先讨论适用于许多数据集的三个特性,它们对数据挖掘技术具有重要影响,它们是维度、稀疏性和分辨率。维度( dimensionality)数据集的维度是数据集中的对象具有的属性数目。低维度数据往往与中、高维度数据有质的不同。确实,分析高维数据有时会陷入所谓维灾难( curse of dimensionality)正因为如此,数据预处理的一个重要动机就是减少维度,称为维归约(dimensionality reduction)。这些问题在本章的后面会更深入地讨论。稀疏性( sparsity)有些数据集,如具有非对称特征的数据集,一个对象的大部分属性上的值都为0;在许多情况下,非零项还不到1%。实际上,稀疏性是一个优点,因为只有非零值才需要存储和处理。这将节省大量的计算时间和存储空间。此外,有些数据挖掘算法仅适合处理稀疏数据。分辨率( resolution)常常可以在不同的分辨率下得到数据,并且在不同的分辨率下数据的性质也不同。例如,在几米的分辨率下,地球表面看上去很不平坦,但在数十公里的分辨率下却
相对平坦。数据的模式也依赖于分辨率。如果分辨率太高,模式可能看不出,或者掩埋在噪声中;如果分辨率太低,模式可能不出现。例如,几小时记录一下气压变化可以反映出风暴等天气系统的移动;而在月的标度下,这些现象就检测不到。2.记录数据许多数据挖掘任务都假定数据集是记录(数据对象)的汇集,每个记录包含固定的数据字段(属性)集。见图2-2a。对于记录数据的大部分基本形式,记录之间或数据字段之间没有明显的联系,并且每个记录(对象)具有相同的属性集。记录数据通常存放在平展文件或关系数据库中。关系数据库当然不仅仅是记录的汇集,它还包含更多的信息,但是数据挖掘一般并不使用关系数据库的这些信息。更确切地说,数据库是查找记录的方便场所。下面介绍不同类型的记录数据,并用图2-2加以说明。 Tid Refund Marital Taxable Defaulted Status Income Borrower TID ITEMS Yes Single 125K No 2 No Married 100KNo Bread, Soda, Milk 3 No Single 70K No Yes Married 120KNo 2 Beer,Bread 5 No Divorced 95K Yes 6 No Married 6OK No 3 Beer, Soda, Diaper, Milk 7Yes Divorced 220K No 4 Beer, Bread, Diaper, Milk 8 No Single 85KYes No Married 75K No 5Soda, Diaper,Milk 10 No Single 90KYes(a)记录数据(b)事务数据 Projection of Projection of Distance Load Thickness x Load y Load豆10.235.2715.22271.2 Document 1 3 2 6 212.6562516.22221.1 Document 2 2 313.547.2317.34231.214.278.4318.45250.9 Document o 1 22 3 o(c)数据矩阵(d文档词矩阵图2-2记录数据的不同变体事务数据或购物篮数据事务数据(transaction data是一种特殊类型的记录数据,其中每个记录(事务)涉及一系列的项。考虑一个杂货店。顾客一次购物所购买的商品的集合就构成一个事务,而购买的商品是项。这种类型的数据称作购物篮数据( market basket data),因为记录中的项是顾客“购物篮”中的商品。事务数据是项的集合的集族,但是也能将它视为记录的集合,其中记录的字段是非对称的属性。这些属性常常是二元的,指出商品是否已买。更一般地,这些属性还可以是离散的或连续的,例如表示购买的商品数量或购买商品的花费。图2-2b展示了一个事务数据集,每一行代表一位顾客在特定时间购买的商品。数据矩阵如果一个数据集族中的所有数据对象都具有相同的数值属性集,则数据对象可以看作多维空间中的点(向量),其中每个维代表对象的一个不同属性。这样的数据对象集可以用一个m×n的矩阵表示,其中m行,一个对象一行;n列,一个属性一列。(也可以将数据对象
用列表示,属性用行表示。)这种矩阵称作数据矩阵(data matrix)或模式矩阵(pattern matrix数据矩阵是记录数据的变体,但是,由于它由数值属性组成,可以使用标准的矩阵操作对数据进行变换和处理,因此,对于大部分统计数据,数据矩阵是一种标准的数据格式。图2-2c示出一个样本数据矩阵。稀疏数据矩阵稀疏数据矩阵是数据矩阵的一种特殊情况,其中属性的类型相同并且是非对称的,即只有非零值才是重要的。事务数据是仅含0-1元素的稀疏数据矩阵的例子。另一个常见的例子是文档数据。特别地,如果忽略文档中词(术语)的次序,则文档可以用词向量表示,其中每个词是向量的一个分量(属性),而每个分量的值是对应词在文档中出现的次数。文档集合的这种表示通常称作文档-词矩阵(document-term matrix)图2-2d显示了一个文档-词矩阵。文档是该矩阵的行,而词是矩阵的列。实践应用时,仅存放稀疏数据矩阵的非零项。3.基于图形的数据有时,图形可以方便而有效地表示数据。我们考虑两种特殊情况:(1)图形捕获数据对象之间的联系,(2)数据对象本身用图形表示。带有对象之间联系的数据对象之间的联系常常携带重要信息。在这种情况下,数据常常用图形表示。一般把数据对象映射到图的结点,而对象之间的联系用对象之间的链和诸如方向、权值等链性质表示。考虑万维网上的网页,页面上包含文本和指向其他页面的链接。为了处理搜索查询,eb搜索引擎收集并处理网页,提取它们的内容。然而,众所周知,指向或出自每个页面的链接包含了大量该页面与查询相关程度的信息,因而必须考虑。图2-3a显示了相互链接的网页集。 Useful Links: Bibliugrarkiy Knowledge Discovery and Oxther Usetful Wen sites Data Mining Bibliography quetly, so visit ofent) ACMS]) KInuggets o The Dista Mine Hook References in Data Mining and Knowledge Discowery Generul DatMining Uxama Fayyau. Gregory Pialetsky Padhratc Smyth, and Ramasamny uthu "Advances in Knowledge Discovery or Knwwledge Diacovery". Builetin of Mining AAAI Pressthe MrI J. Ross Quinlan, (4.5: Programs for Machine Leaming", Morgan Kaufmann P gory Michael Berry and TinofT, "Data Mining Suppor), Jotn Wiley Sons. 1997 Knowledge and Data Engineering. 5(6):903-91 Decenber 1993,(a)链接的网页(b)苯分子图2-3不同的图形数据具有图形对象的数据如果对象具有结构即对象包含具有联系的子对象,则这样的对象常常用图形表示。例如,化合物的结构可以用图形表示,其中结点是原子,结点之间的链是化学键。图2-3b给出化合物苯的分子结构示意图,包含碳原子(黑色)和氢原子(灰色)。图形表示可以确定何种子结构频繁地出现在化合物的集合中,并且查明这些子结构中是否有某种子结构与诸如熔点或生成热等特定的化学性质有关。子结构挖掘是数据挖掘中分析这类数据的一个分支,将在7.5节讨论。
4.有序数据对于某些数据类型,属性具有涉及时间或空间序的联系。下面介绍各种类型的有序数据,并显示在图2-4中。时间顾客购买的商品 C1 A,B CGCAGGGCCCGCCCCGCGCCGTC122345 C3 A,C C,D GAGAAGGGCCCGCCTGGCGGGCG t3 A,D GGGGGAGGCGGGGCCGCCCGAGCE C1 CCAACCGAGTCCGACCAGGTGCC A,E CCCTCTGCTCGGCCTAGACCTGA顾客购买时间与购买商品 GCTCATTAGGCGGCAGCGGACAG C1 (: A, B)(t2: C,D)(5:A,E) C2(t3: A, D)(4: E) GCCAAGTAGAACACGCGAAGCGC C3 (t2: A,) TGGGCTGCCTGCTGCGACCAGGG(a)时序事务数据(b)基因组序列数据明尼阿波利斯月平均气温3025出201510590120150180年经度温度(c)温度时间序列(d)空间温度数据图2-4不同的有序数据时序数据时序数据( sequential data)也称时间数据( temporal data),可以看作记录数据的扩充,其中每个记录包含一个与之相关联的时间。考虑存储事务发生时间的零售事务数据。时间信息可以帮助我们发现“万圣节前夕糖果销售达到高峰”之类的模式。时间也可以与每个属性相关联,例如,每个记录可以是一位顾客的购物历史,包含不同时间购买的商品列表。使用这些信息,就有可能发现“购买DVD播放机的人趋向于在其后不久购买DVD”之类的模式。图2-4a展示了一些时序事务数据。有5个不同的时间t1、2、t、t4和t5;3位不同的顾客C1、C2和C3;5种不同的商品A、B、C、D和E。在图上面的表中,每行对应于一位顾客在特定的时间购买的商品。例如,在时间t3,顾客C2购买了商品A和D。下面的表显示相同的信息,但每行对应于一位顾客。每行包含涉及该顾客的所有事务信息,其中每个事务包含一些商品和购买这些商品的时间,例如顾客C3在时间口购买了商品A和C。序列数据序列数据( sequence data)是一个数据集合,它是各个实体的序列,如词或字母的序列。除没有时间戳之外,它与时序数据非常相似,只是有序序列考虑项的位置。例如,动植物的遗传信息可以用称作基因的核苷酸的序列表示。与遗传序列数据有关的许多问题都涉及由核苷酸序列的相似性预测基因结构和功能的相似性。图2-4b显示用4种核苷酸表示的一段人类基
因码。所有DNA都可以用A、T、G和C四种核苷酸构造。时间序列数据时间序列数据(time series data)是一种特殊的时序数据,其中每个记录都是一个时间序列(time series),即一段时间以来的测量序列。例如,金融数据集可能包含各种股票每日价格的时间序列对象。再例如,考虑图2-4c,该图显示明尼阿波利斯从1982年到1994年的月平均气温的时间序列。在分析时间数据时,重要的是要考虑时间自相关( temporal autocorrelation),即如果两个测量的时间很接近,则这些测量的值通常非常相似。空间数据有些对象除了其他类型的属性之外,还具有空间属性,如位置或区域。空间数据的一个例子是从不同的地理位置收集的气象数据(降水量、气温、气压)。空间数据的一个重要特点是空间自相关性(spatial autocorrelation),即物理上靠近的对象趋向于在其他方面也相似。这样,地球上相互靠近的两个点通常具有相近的气温和降水量。空间数据的重要例子是科学和工程数据集其数据取自二维或三维网格上规则或不规则分布的点上的测量或模型输出。例如,地球科学数据集记录在各种分辨率(如每度)下经纬度球面网格点(网格单元)上测量的温度和气压(见图2-4d)。另一个例子,在瓦斯气流模拟中,可以针对模拟中的每个网格点记录流速和方向。5.处理非记录数据大部分数据挖掘算法都是为记录数据或其变体(如事务数据和数据矩阵)设计的。通过从数据对象中提取特征,并使用这些特征创建对应于每个对象的记录,针对记录数据的技术也可以用于非记录数据。考虑前面介绍的化学结构数据。给定一个常见的子结构集合,每个化合物都可以用一个具有二元属性的记录表示,这些二元属性指出化合物是否包含特定的子结构。这样的表示实际上是事务数据集,其中事务是化合物,而项是子结构。在某些情况下,容易用记录形式表示数据但是这类表示并不能捕获数据中的所有信息。考虑这样的时间空间数据,它由空间网格每一点上的时间序列组成。通常,这种数据存放在数据矩阵中,其中每行代表一个位置,而每列代表一个特定的时间点。然而,这种表示并不能明确地表示属性之间存在的时间联系以及对象之间存在的空间联系。但并不是说这种表示不合适,而是说分析时必须考虑这些联系。例如,在使用数据挖掘技术时,假定属性之间在统计上是相互独立的并不是一个好主意。2.2数据质量数据挖掘使用的数据常常是为其他用途收集的,或者在收集时未明确其目的。因此,数据挖掘常常不能“在数据源头控制质量”相比之下,统计学的实验设计或调查往往其数据质量都达到了一定的要求。由于无法避免数据质量问题,因此数据挖掘着眼于两个方面:(1)数据质量问题的检测和纠正,(2)使用可以容忍低质量数据的算法。第一步的检测和纠正,通常称作数据清理(data cleaning)下面几节讨论数据质量。尽管也讨论某些与应用有关的问题,但是关注的焦点是测量和数据收集问题。2.2.1测量和数据收集问题期望数据完美是不现实的。由于人的错误、测量设备的限制或数据收集过程的漏洞都可能导
致问题。数据的值乃至整个数据对象都可能会丢失。在有些情况下,可能有不真实的或重复的对象,即对应于单个“实际”对象出现了多个数据对象。例如,对于一个最近住过两个不同地方的人,可能有两个不同的记录。即使所有的数据都不缺,并且“看上去很好”,也可能存在不一致,如一个人身高2m,但体重只有2kg在下面几段,我们关注数据测量和收集方面的数据质量问题。我们先定义测量误差和数据收集错误,然后考虑涉及测量误差的各种问题:噪声、伪像、偏倚、精度和准确率。最后讨论可能同时涉及测量和数据收集的数据质量问题:离群点、遗漏和不一致的值、重复数据。1.测量误差和数据收集错误术语测量误差(measurement error)是指测量过程中导致的问题。一个常见的问题是:在某种程度上,记录的值与实际值不同。对于连续属性,测量值与实际值的差称为误差(error)。术语数据收集错误(data collection error)是指诸如遗漏数据对象或属性值,或不当地包含了其他数据对象等错误。例如,一种特定种类动物研究可能包含了相关种类的其他动物,它们只是表面上与要研究的种类相似。测量误差和数据收集错误可能是系统的也可能是随机的。我们只考虑一般的错误类型。在特定的领域,总有些类型的错误是常见的,并且常常有很好的技术来检测并纠正这些错误。例如,人工输入数据时键盘录入错误是常见的,因此许多数据输入程序具有检测技术,并且通过人工干预纠正这类错误。2.噪声和伪像噪声是测量误差的随机部分。这可能涉及值被扭曲或加入了谬误对象。图2-5显示被随机噪声干扰前后的时间序列。如果在时间序列上加更多的噪声,形状将会消失。图2-6显示了三组添加一些噪声点(用“+”表示)前后的数据点集。注意,有些噪声点与非噪声点混在一起。(a)时间序列(b)含噪声的时间序列图2-5时间序列中的噪声(a)三组点(b)含噪声点(+)图26空间中的噪声术语“噪声”通常用于包含时间或空间分量的数据。在这些情况下,常常可以使用信号或图像处理技术降低噪声,从而帮助发现可能“淹没在噪声中”的模式(信号)。尽管如此,完全消
除噪声通常是困难的,而许多数据挖掘工作都关注设计鲁棒算法(robust algorithm),即在噪声干扰下也能产生可以接受的结果。数据错误可能是更确定性现象的结果,如一组照片在同一地方出现条纹。数据的这种确定性失真常称作伪像(artifact)3.精度、偏倚和准确率在统计学和实验科学中,测量过程和结果数据的质量用精度和偏倚度量。我们给出标准的定义,随后简略加以讨论。对于下面的定义,我们假定对相同的基本量进行重复测量,并使用测量值集合计算均值(平均值),作为实际值的估计。定义2.3精度(precision)(同一个量的)重复测量值之间的接近程度。定义2.4偏倚(bias)测量值与被测量之间的系统的变差。精度通常用值集合的标准差度量,而偏倚用值集合的均值与测出的已知值之间的差度量。只有那些通过外部手段能够得到测量值的对象偏倚才是可确定的。假定我们有1克质量的标准实验室重量,并且想评估实验室的新天平的精度和偏倚。我们称重5次,得到下列值:{1.015,0.990,1.013,1.001,0.986}。这些值的均值是1.00,因此偏倚是0.001.用标准差度量,精度是0.013。通常使用更一般的术语准确率表示数据测量误差的程度。定义2.5准确率(accuracy)被测量的测量值与实际值之间的接近度。准确率依赖于精度和偏倚,但是由于它是一个一般化的概念,因此没有用这两个量表达准确率的公式。准确率的一个重要方面是有效数字( significant digit)的使用。其目标是仅使用数据精度所能确定的数字位数表示测量或计算结果。例如,对象的长度用最小刻度为毫米的米尺测量,则我们只能记录最接近毫米的长度数据,这种测量的精度为±0.5mm。我们不再详细地讨论有效数字,因为大部分读者应当在先前的课程中接触过,并且在理工科和统计学教材中讨论得相当深入。诸如有效数字、精度、偏倚和准确率问题常常被忽视,但是对于数据挖掘、统计学和自然科学,它们都非常重要。通常,数据集并不包含数据精度信息,用于分析的程序返回的结果也没有这方面的信息。但是,缺乏对数据和结果准确率的理解,分析者将可能出现严重的数据分析错误。4.离群点离群点(outlier)是在某种意义上具有不同于数据集中其他大部分数据对象的特征的数据对象,或是相对于该属性的典型值来说不寻常的属性值。我们也称其为异常(anomalous)对象或异常值。有许多定义离群点的方法,并且统计学和数据挖掘界已经提出了很多不同的定义。此外,区别噪声和离群点这两个概念是非常重要的。离群点可以是合法的数据对象或值。因此,不像噪声,离群点本身有时是人们感兴趣的对象。例如,欺诈和网络攻击检测中,目标就是从大量正常对象或事件中发现不正常的对象和事件。第10章更详细地讨论异常检测。5.遗漏值一个对象遗漏一个或多个属性值的情况并不少见。有时可能会出现信息收集不全的情况,例如有的人拒绝透露年龄或体重。还有些情况下,某些属性并不能用于所有对象,例如表格常常有条件选择部分,仅当填表人以特定的方式回答前面的问题时,条件选择部分才需要填写,但为简
单起见存储了表格的所有字段。无论何种情况,在数据分析时都应当考虑遗漏值。有许多处理遗漏值的策略(和这些策略的变种),每种策略可能适用于特定的情况。这些策略在下面列出,同时我们指出它们的优缺点。删除数据对象或属性一种简单而有效的策略是删除具有遗漏值的数据对象。然而,即使不完整的数据对象也包含一些有用的信息,并且,如果许多对象都有遗漏值,则很难甚至不可能进行可靠的分析。尽管如此,如果某个数据集只有少量的对象具有遗漏值,则忽略它们可能是合算的。一种与之相关的策略是删除具有遗漏值属性。然而,做这件事要小心,因为被删除的属性可能对分析是至关重要的。估计遗漏值有时,遗漏值可以可靠地估计。例如,在考虑以大致平滑的方式变化的、具有少量但分散的遗漏值的时间序列时,遗漏值可以使用其他值来估计(插值)另举一例,考虑一个具有许多相似数据点的数据集,与具有遗漏值的点邻近的点的属性值常常可以用来估计遗漏的值。如果属性是连续的,则可以使用最近邻的平均属性值如果属性是分类的,则可以取最近邻中最常出现的属性值。为了更具体地解释,考虑地面站记录的降水量,对于未设地面站的区域,降水量可以使用邻近地面站的观测值估计。在分析时忽略遗漏值许多数据挖掘方法都可以修改,忽略遗漏值。例如,假定正在对数据对象聚类,需要计算各对数据对象间的相似性。如果某对的一个对象或两个对象都有某些属性有遗漏值,则可以仅使用没有遗漏值的属性来计算相似性。当然,这种相似性只是近似的,但是除非整个属性数目很少,或者遗漏值的数量很大,否则这种误差影响不大。同样地,许多分类方法都可以修改,便于处理遗漏值。6.不一致的值数据可能包含不一致的值。比如地址字段列出了邮政编码和城市名,但是有的邮政编码区域并不包含在对应的城市中。可能是人工输入该信息时录颠倒了两个数字,或许是在手写体扫描时错读了一个数字。无论导致不一致值的原因是什么,重要的是能检测出来,并且如果可能的话,纠正这种错误。有些不一致类型容易检测,例如人的身高不应当是负的。有些情况下,可能需要查阅外部信息源,例如当保险公司处理赔偿要求时,它将对照顾客数据库核对赔偿单上的姓名与地址。检测到不一致后,有时可以对数据进行更正。产品代码可能有“校验”数字,或者可以通过一个备案的已知产品代码列表,复核产品代码;如果发现它不正确但接近一个已知代码,则纠正它。纠正不一致需要额外的或冗余的信息。例2.6不一致的海洋表面温度该例解释实际的时间序列数据中的不一致性。这些数据是在海洋的不同点测量的海洋表面温度(SST)。最早,人们利用船或浮标使用海洋测量方法收集SST数据;而最近,开始使用卫星来收集这些数据。为了创建长期的数据集,需要使用这两种数据源。然而,由于数据来自不同的数据源,两部分数据存在微妙的不同。这种差异显示在图2-7中,该图显示了各年度之间SST值的相关性。如果某两个年度的SST值是正相关的,则对应于这两年的位置为白色,否则为黑色。(季节性的变化从数据中删除,否则所有的年都是高度相关的。)在数据汇集一起的地方(1983年)有一个明显的变化。1958~1982和1983~1999两组,每组内的年相互之间趋向于正相关,但与另一组的年负相关。这并不意味该数据不能用,但是分
析者应当考虑这种差异对数据挖掘分析的潜在影响。口6588905606570758085909年图2-7年对之间SST数据的相关性。白色区域表示正相关,黑色区域表示负相关7.重复数据数据集可能包含重复或几乎重复的数据对象许多人都收到过重复的邮件,因为他们以稍微不相同的名字多次出现在数据库中。为了检测并删除这种重复,必须处理两个主要问题。首先,如果两个对象实际代表同一个对象,则对应的属性值必然不同,必须解决这些不一致的值;其次,需要避免意外地将两个相似但并非重复的数据对象(如两个人具有相同姓名)合并在一起。术语去重复(deduplication)通常用来表示处理这些问题的过程。在某些情况下,两个或多个对象在数据库的属性度量上是相同的,但是仍然代表不同的对象。这种重复是合法的。但是,如果某些算法设计中没有专门考虑这些属性可能相同的对象,就还是可能导致问题。本章习题13就是这么一个例子。2.2.2关于应用的问题数据质量问题也可以从应用角度考虑,表达为“数据是高质量的,如果它适合预期的应用”特别是对工商业界,数据质量的这种提议非常有用。类似的观点也出现在统计学和实验科学,那里强调精心设计实验来收集与特定假设相关的数据。与测量和数据收集一样,许多数据质量问题与特定的应用和领域有关。我们这里仍然只考虑一些一般性问题。时效性有些数据收集后就开始老化。比如说,如果数据提供正在发生的现象或过程的快照,如顾客的购买行为或Web浏览模式,则快照只代表有限时间内的真实情况。如果数据已经过时,则基于它的模型和模式也已经过时。相关性可用的数据必须包含应用所需要的信息。考虑构造一个模型,预测交通事故发生率。如果忽略了驾驶员的年龄和性别信息,那么除非这些信息可以间接地通过其他属性得到,否则模型的精度可能是有限的。确保数据集中的对象相关不太容易。一个常见问题是抽样偏倚(sampling bias),指样本包含
的不同类型的对象与它们在总体中的出现情况不成比例例如调查数据只反映对调查做出响应的那些人的意见。(抽样的其他问题将在2.3.2节进一步讨论。)由于数据分析的结果只反映现有的数据,抽样偏倚通常导致不正确的分析。关于数据的知识理想情况下,数据集附有描述数据的文档。文档的质量好坏决定它是支持还是干扰其后的分析。例如,如果文档标明若干属性是强相关的,则说明这些属性可能提供了高度冗余的信息,我们可以决定只保留一个。(考虑销售税和销售价格)然而,如果文档很糟糕,例如,没有告诉我们某特定字段上的遗漏值用-9999指示,则我们的数据分析就可能出问题。其他应该说明的重要特性是数据精度、特征的类型(标称的、序数的、区间的、比率的)、测量的刻度(如长度用米还是英尺)和数据的来源。2.3数据预处理本节,我们讨论应当采用哪些预处理步骤,让数据更加适合挖掘。数据预处理是一个广泛的领域,包含大量以复杂的方式相关联的不同策略和技术。我们将讨论一些最重要的思想和方法,并试图指出它们之间的相互联系。具体地说,我们将讨论如下主题。·聚集。抽样。维归约。特征子集选择。·特征创建。离散化和二元化。变量变换。粗略地说,这些项目分为两类,即选择分析所需要的数据对象和属性以及创建改变属性。这两种情况的目标都是改善数据挖掘分析工作,减少时间,降低成本和提高质量。细节参见以下几节。术语注记:下面,我们有时将根据习惯用法,使用特征(feature)或变量(variable)指代属性(attribute)2.3.1聚集有时,“少就是多”,而聚集就是如此。聚集(aggregation)将两个或多个对象合并成单个对象。考虑一个由事务(数据对象)组成的数据集,它记录一年中不同日期在各地(明尼阿波利斯、芝加哥、巴黎)商店的商品日销售情况,见表2-4。对该数据集的事务进行聚集的一种方法,是用一个商店事务替换该商店的所有事务。这把每天出现在一个商店的成百上千个事务记录归约成单个日事务,而数据对象的个数减少为商店的个数。表24包含顾客购买信息的数据集事务ID商品商店位置日期价格::101123 Watch Chicago09/06/04$25.99101123 Battery Chicago09/06/04s5.99101124 Shoes Minneapolis 09/06/04 $75.00 PDG
这里显而易见的问题是如何创建聚集事务即在创建代表单个商店或日期的聚集事务时,如何合并所有记录的每个属性的值。定量属性(如价格)通常通过求和或求平均值进行聚集定性属性(如商品)可以忽略或汇总成在一个商店销售的所有商品的集合。表2-4中的数据也可以看作多维数组,其中每个属性是一个维。从这个角度,聚集是删除属性(如商品类型)的过程,或者是压缩特定属性不同值个数的过程,如将日期的可能值从365天压缩到12个月。这种类型的聚集通常用于olapOnline Analytical Processing,联机分析处理)OLAP在第3章进一步讨论。聚集的动机有多种。首先,数据归约导致的较小数据集需要较少的内存和处理时间,因此可以使用开销更大的数据挖掘算法。其次,通过高层而不是低层数据视图,聚集起到了范围或标度转换的作用。在前面的例子中,在商店位置和月份上的聚集给出数据按月、按商店,而不是按天、按商品的视图。最后,对象或属性群的行为通常比单个对象或属性的行为更加稳定。这反映了统计学事实:相对于被聚集的单个对象,诸如平均值、总数等聚集量具有较小的变异性。对于总数,实际变差大于单个对象的(平均)变差,但是变差的百分比较小;而对于均值,实际变差小于单个对象的(平均)变差。聚集的缺点是可能丢失有趣的细节。在商店的例子中,按月的聚集就丢失了星期几具有最高销售额的信息。例2.7澳大利亚降水量该例基于澳大利亚从1982年到1993年的降水量。我们把澳大利亚国土按经纬度0.5°乘0.5°大小分成300个网格。图2-8a的直方图显示这些网格单元上的平均月降水量的标准差,而图2-8b的直方图显示相同位置的平均年降水量的标准差。可见,平均年降水量比平均月降水量的变异性小。所有降水量测量(以及它们的标准差)都以厘米(cm)为单位。180160120100新凶02068101214161823456标准差标准差(a)平均月降水量标准差的直方图(b)平均年降水量标准差的直方图图2-8澳大利亚从1982年到1993年月和年降水量标准差的直方图2.3.2抽样抽样是一种选择数据对象子集进行分析的常用方法在统计学中,抽样长期用于数据的事先调查和最终的数据分析。在数据挖掘中,抽样也非常有用然而,在统计学和数据挖掘中,抽样的动机并不相同。统计学使用抽样是因为得到感兴趣的整个数据集的费用太高、太费时间,而数据挖掘使用抽样是因为处理所有的数据的费用太高、太费时间。在某些情况下,使用抽样的算法
可以压缩数据量,以便可以使用更好但开销较大的数据挖掘算法。有效抽样的主要原理如下:如果样本是有代表性的,则使用样本与使用整个数据集的效果几乎一样。而样本是有代表性的,前提是它近似地具有与原数据集相同的(感兴趣的)性质。如果数据对象的均值(平均值)是感兴趣的性质,而样本具有近似于原数据集的均值,则样本就是有代表性的。由于抽样是一个统计过程,特定样本的代表性是变化的,因此我们所能做的最好的抽样方案就是选择一个确保以很高的概率得到有代表性的样本。如下所述,这涉及选择适当的样本容量和抽样技术。1.抽样方法有许多抽样技术,但是这里只介绍少数最基本的抽样技术和它们的变形。最简单的抽样是简单随机抽样( simple random sampling)。对于这种抽样,选取任何特定项的概率相等随机抽样有两种变形(其他抽样技术也一样):(1)无放回抽样每个选中项立即从构成总体的所有对象集中删除;(2)有放回抽样对象被选中时不从总体中删除。在有放回抽样中,相同的对象可能被多次抽出。当样本与数据集相比相对较小时,两种方法产生的样本差别不大。但是,对于分析,有放回抽样较为简单,因为在抽样过程中,每个对象被选中的概率保持不变。当总体由不同类型的对象组成,每种类型的对象数量差别很大时,简单随机抽样不能充分地代表不太频繁出现的对象类型。当分析需要所有类型的代表时,这可能出现问题。例如,当为稀有类构建分类模型时,样本中适当地提供稀有类是至关重要的,因此需要提供具有不同频率的感兴趣的项的抽样方案。分层抽样( stratified sampling)就是这样的方法,它从预先指定的组开始抽样。在最简单的情况下,尽管每组的大小不同,但是从每组抽取的对象个数相同。另一种变形是从每一组抽取的对象数量正比于该组的大小。例2.8抽样与信息损失一旦选定抽样技术,就需要选择样本容量。较大的样本容量增大了样本具有代表性的概率,但也抵消了抽样带来的许多好处。反过来,使用较小容量的样本,可能丢失模式,或检测出错误的模式。图2-9a显示包含8000个二维点的数据集,而图29b和图2-9c显示从该数据集抽取的容量分别为2000和500的样本。该数据集的大部分结构都出现在2000个点的样本中,但是许多结构在500个点的样本中丢失了。口(a)8000个点(b)2000个点(c)500个点图2-9抽样丢失结构的例子例2.9确定适当的样本容量为了说明确定合适的样本容量需要系统的方法,考虑下面的任务。
给定一个数据集,它包含少量容量大致相等的组。从每组至少找出一个代表点。假定每个组内的对象高度相似,但是不同组中的对象不太相似。还假定组的个数不多(例如,10个组)。图2-10a显示了一个理想簇(组)的集合,这些点可能从中抽取使用抽样可以有效地解决该问题。一种方法是取数据点的一个小样本,逐对计算点之间的相似性,然后形成高度相似的点组。从这些组每组取一个点,则可以得到具有代表性的点的集合。然而,按照该方法,我们需要确定样本的容量,它以很高的概率确保得到期望的结果,即从每个簇至少找出一个代表点。图2-10b显示随着样本容量从10变化到60时,从10个组的每一个得到一个对象的概率。有趣的是,使用容量为20的样本,只有很小的机会(20%)得到包含所有10个簇的样本。即便使用容量为30的样本,得到不包含所有10个簇中对象的样本的几率也很高(几乎40%)。该问题将在第8章习题4讨论聚类中进一步考察。口0.80.60.40.2010203040506070样本大小(a)点的10个组(b)样本包含所有10个组中点的概率图2-10从10个组找出具有代表性的点2.渐进抽样合适的样本容量可能很难确定,因此有时需要使用自适应( adaptive)或渐进抽样(progre- ssive sampling)方法。这些方法从一个小样本开始,然后增加样本容量直至得到足够容量的样本。尽管这种技术不需要在开始就确定正确的样本容量,但是需要评估样本的方法,确定它是否足够大。例如,假定使用渐进抽样来学习一个预测模型。尽管预测模型的准确率随样本容量增加,但是在某一点准确率的增加趋于稳定。我们希望在稳定点停止增加样本容量。通过掌握模型准确率随样本逐渐增大的变化情况,并通过选取接近于当前容量的其他样本,我们可以估计出与稳定点的接近程度,从而停止抽样。2.3.3维归约数据集可能包含大量特征。考虑一个文档的集合,其中每个文档是一个向量,其分量是文档中出现的每个词的频率。在这种情况下通常有成千上万的属性(分量),每个代表词汇表中的一个词。再看一个例子,考虑包含过去30年各种股票日收盘价的时间序列数据集。在这种情况下,属性是特定天的价格,也数以千计。维归约有多方面的好处。关键的好处是,如果维度(数据属性的个数)较低,许多数据挖掘算法的效果就会更好。这一部分是因为维归约可以删除不相关的特征并降低噪声,一部分是因为
维灾难。(维灾难在下面解释。)另一个好处是维归约可以使模型更容易理解,因为模型可能只涉及较少的属性。此外,维归约也可以更容易让数据可视化。即使维归约没有将数据归约到二维或三维,数据也可以通过观察属性对或三元组属性达到可视化,并且这种组合的数目也会大大减少。最后,使用维归约降低了数据挖掘算法的时间和内存需求。术语“维归约”通常用于这样的技术:通过创建新属性,将一些旧属性合并在一起来降低数据集的维度。通过选择旧属性的子集得到新属性,这种维归约称为特征子集选择或特征选择。特征选择将在2.3.4节讨论。下面我们简单介绍两个重要的主题:维灾难和基于线性代数方法(如主成分分析)的维归约技术。1.维灾难维灾难是指这样的现象:随着数据维度的增加,许多数据分析变得非常困难。特别是随着维度增加,数据在它所占据的空间中越来越稀疏。对于分类,这可能意味没有足够的数据对象来创建模型,将所有可能的对象可靠地指派到一个类对于聚类,点之间的密度和距离的定义(对聚类是至关重要的)失去了意义。(在9.1.2节、9.4.5节和9.4.7节进一步讨论)结果是,对于高维数据,许多分类和聚类算法(以及其他数据分析算法)都麻烦缠身分类准确率降低,聚类质量下降。2.维归约的线性代数技术维归约的一些最常用的方法是使用线性代数技术,将数据由高维空间投影到低维空间,特别是对于连续数据。主成分分析(Principal Components Analysis,PCA)是一种用于连续属性的线性代数技术,它找出新的属性(主成分),这些属性是原属性的线性组合,是相互正交的。。(orthogonal),并且捕获了数据的最大变差。例如,前两个主成分是两个正交属性,是原属性的线性组合,尽可能多地捕获了数据的变差奇异值分解(Singular Value Decomposition,SvD)是一种线性代数技术,它与PCA有关,并且也用于维归约。2.3.4特征子集选择降低维度的另一种方法是仅使用特征的一个子集。尽管看起来这种方法可能丢失信息,但是在存在冗余或不相关的特征的时候,情况并非如此。冗余特征重复了包含在一个或多个其他属性中的许多或所有信息。例如,一种产品的购买价格和所支付的销售税额包含许多相同的信息。不相关特征包含对于手头的数据挖掘任务几乎完全没用的信息,例如学生的ID号码对于预测学生的总平均成绩是不相关的冗余和不相关的特征可能降低分类的准确率,影响所发现的聚类的质量。尽管使用常识或领域知识可以立即消除一些不相关的和冗余的属性,但是选择最佳的特征子集通常需要系统的方法。特征选择的理想方法是:将所有可能的特征子集作为感兴趣的数据挖掘算法的输入,然后选取产生最好结果的子集。这种方法的优点是反映了最终使用的数据挖掘算法的目的和偏爱。然而,由于涉及n个属性的子集多达2n个,这种方法在大部分情况下行不通,因此需要其他策略。有三种标准的特征选择方法:嵌入、过滤和包装。嵌入方法(embedded approach)特征选择作为数据挖掘算法的一部分是理所当然的。特别是在数据挖掘算法运行期间,算法本身决定使用哪些属性和忽略哪些属性。构造决策树分类器的算法(在第4章讨论)通常以这种方式运行。
过滤方法(filter approach)使用某种独立于数据挖掘任务的方法,在数据挖掘算法运行前进行特征选择,例如我们可以选择属性的集合,它的属性对之间的相关度尽可能低。包装方法(wrapper approach)这些方法将目标数据挖掘算法作为黑盒,使用类似于前面介绍的理想算法,但通常并不枚举所有可能的子集来找出最佳属性子集。由于嵌入方法与具体的算法有关,这里我们只进一步讨论过滤和包装方法。1.特征子集选择体系结构可以将过滤和包装方法放到一个共同的体系结构中。特征选择过程可以看作由四部分组成:子集评估度量、控制新的特征子集产生的搜索策略、停止搜索判断和验证过程。过滤方法和包装方法的唯一不同是它们使用了不同的特征子集评估方法。对于包装方法,子集评估使用目标数据挖掘算法;对于过滤方法,子集评估技术不同于目标数据挖掘算法。下面的讨论提供了该方法的一些细节,汇总在图2-11中。选择的属性满足停止判断评估不满足验证过程属性搜索策略属性子集图2-11特征子集选择过程流程图从概念上讲,特征子集选择是搜索所有可能的特征子集的过程。可以使用许多不同类型的搜索策略,但是搜索策略的计算花费应当较低,并且应当找到最优或近似最优的特征子集。通常不可能同时满足这两个要求,因此需要折中权衡。搜索的一个不可缺少的组成部分是评估步骤,根据已经考虑的子集评价当前的特征子集。这需要一种评估度量,针对诸如分类或聚类等数据挖掘任务,确定属性特征子集的质量。对于过滤方法,这种度量试图预测实际的数据挖掘算法在给定的属性集上执行的效果如何;对于包装方法,评估包括实际运行目标数据挖掘应用,子集评估函数就是通常用于度量数据挖掘结果的评判标准。因为子集的数量可能很大,考察所有的子集可能不现实,因此需要某种停止搜索判断。其策略通常基于如下一个或多个条件:迭代次数,子集评估的度量值是否最优或超过给定的阈值,一个特定大小的子集是否已经得到,大小和评估标准是否同时达到,使用搜索策略得到的选择是否可以实现改进。最后,一旦选定特征子集,就要验证目标数据挖掘算法在选定子集上的结果。一种直截了当的评估方法是用全部特征的集合运行算法并将全部结果与使用该特征子集得到的结果进行比较。如果顺利的话,特征子集产生的结果将比使用所有特征产生的结果更好,或者至少几乎一样
好。另一个验证方法是使用一些不同的特征选择算法得到特征子集,然后比较数据挖掘算法在每个子集上的运行结果。2.特征加权特征加权是另一种保留或删除特征的办法。特征越重要,所赋予的权值越大,而不太重要的特征赋予较小的权值。有时,这些权值可以根据特征的相对重要性的领域知识确定,也可以自动确定。例如,有些分类方法,如支持向量机(第5章),产生分类模型,其中每个特征都赋予一个权值。具有较大权值的特征在模型中所起的作用更加重要。在计算余弦相似度时进行的对象规范化(2.4.5节)也可以看作一类特征加权。2.3.5特征创建常常可以由原来的属性创建新的属性集,更有效地捕获数据集中的重要信息。此外,新属性的数目可能比原属性少,使得我们可以获得前面介绍的维归约带来的所有好处。下面介绍三种创建新属性的相关方法:特征提取、映射数据到新的空间和特征构造。1.特征提取由原始数据创建新的特征集称作特征提取( extraction)考虑照片的集合,按照照片是否包含人脸分类。原始数据是像素的集合,因此对于许多分类算法都不适合。然而,如果对数据进行处理,提供一些较高层次的特征,诸如与人脸高度相关的某些类型的边和区域等,则会有更多的分类技术可以用于该问题。可是,最常使用的特征提取技术都是高度针对具体领域的。对于特定的领域,如图像处理,在过去一段时间已经开发了各种特征和提取特征的技术,但是这些技术在其他领域的应用却是有限的。因而,一旦数据挖掘用于一个相对较新的领域,一个关键任务就是开发新的特征和特征提取方法。2.映射数据到新的空间使用一种完全不同的视角挖掘数据可能揭示出重要和有趣的特征例如,考虑时间序列数据,它们常常包含周期模式。如果只有单个周期模式,并且噪声不多,则容易检测到该模式;另一方面,如果有大量周期模式,并且存在大量噪声,则很难检测这些模式。尽管如此,通过对该时间序列实施傅里叶变换(Fourier transform),将它转换成频率信息明显的表示,就能检测到这些模式。在下面的例子中,不必知道傅里叶变换的细节,只需要知道对于时间序列,傅里叶变换产生其属性与频率有关的新数据对象就足够了。例2.10傅里叶分析图2-12b中的时间序列是其他三个时间序列的和,其中两个显示在图2-12a中,其频率分别是每秒7个和17个周期,第三个时间序列是随机噪声。图2-12c显示功率频谱。在对原时间序列施加傅里叶变换后,可以计算功率频谱。(非正式地看,功率频谱正比于每个频率属性的平方)尽管有噪声,图中有两个尖峰,对应于两个原来的、无噪声的时间序列的周期。再说一遍,本例的要点是:更好的特征可以揭示数据的重要性质。口也可以采用许多其他类型的变换。除傅里叶变换外,对于时间序列和其他类型的数据,经证实小波变换(wavelet transform)也是非常有用的。 PDG
10.2040608506070时间(秒)时间(秒)频率(a)两个时间序列b)噪声时间序列(c)功率频谱图2-12傅里叶变换应用:识别时间序列数据中的基本频率3.特征构造有时,原始数据集的特征具有必要的信息,但其形式不适合数据挖掘算法。在这种情况下,一个或多个由原特征构造的新特征可能比原特征更有用。例2.11密度为了解释这一点,考虑一个包含人工制品信息的历史数据集。该数据集包含每个人工制品的体积和质量,以及其他信息。为简单起见,假定这些人工制品使用少量材料(木材、陶土、铜、黄金)制造,并且我们希望根据制造材料对它们分类。在此情况下,由质量和体积特征构造的密度特征(即密度=质量/体积)可以很直接地产生准确的分类。尽管有一些人试图通过考察已有特征的简单的数学组合来自动地进行特征构造,但是最常见的方法还是使用专家的意见构造特征。2.3.6离散化和二元化有些数据挖掘算法,特别是某些分类算法,要求数据是分类属性形式。发现关联模式的算法要求数据是二元属性形式。这样,常常需要将连续属性变换成分类属性(离散化, discretization),并且连续和离散属性可能都需要变换成一个或多个二元属性(二元化, binarization)此外,如果一个分类属性具有大量不同值(类别),或者某些值出现不频繁,则对于某些数据挖掘任务,通过合并某些值减少类别的数目可能是有益的。与特征选择一样,最佳的离散化和二元化方法是“对于用来分析数据的数据挖掘算法,产生最好结果”的方法。直接使用这种判别标准通常是不实际的。因此,离散化和二元化一般要满足这样一种判别标准,它与所考虑的数据挖掘任务的性能好坏直接相关。1.二元化一种分类属性二元化的简单技术如下:如果有m个分类值,则将每个原始值唯一地赋予区间[0,m-1]中的一个整数。如果属性是有序的,则赋值必须保持序关系。(注意,即使属性原来就用整数表示,但如果这些整数不在区间[0,m-1]中,则该过程也是必需的。)然后,将这m个整数的每一个都变换成一个二进制数。由于需要n=ogm个二进位表示这些整数,因此要使用n个二元属性表示这些二进制数。例如,一个具有5个值{awful,poor,ok,good, great}的分类变量需要三个二元变量x1、x2、x3转换见表2-5
表25一个分类属性到三个二元属性的变换分类值整数值 awful0 poor OKx0000 good01234001101010 great这样的变换可能导致复杂化,如无意之中建立了转换后的属性之间的联系。例如,在表2-5中,属性x2和x3是相关的,因为good值使用这两个属性表示。此外,关联分析需要非对称的二元属性,其中只有属性的出现(值为1)才是重要的。因此,对于关联问题,需要为每一个分类值引入一个二元属性,如表2-6所示。如果结果属性的个数太多,则可以在二元化之前使用下面介绍的技术减少分类值的个数。表2-6一个分类属性到五个非对称二元属性的转换分类值整数值 X1X2 X3X4xs awful10000 poor01000 OK012300100 good00010 great00001同样,对于关联问题,可能需要用两个非对称的二元属性替换单个二元属性。考虑记录人的性别(男、女)的二元属性,对于传统的关联规则算法,该信息需要转换成两个非对称的二元属性,其中一个仅当是男性时为1,而另一个仅当是女性时为1。(对于非对称的二元属性,由于其提供一个二进制位信息需要占用存储器的两个二进制位,因而在信息的表示上不太有效。)2.连续属性离散化通常,离散化应用于在分类或关联分析中使用到的属性上。一般来说,离散化的效果取决于所使用的算法,以及用到的其他属性。然而,属性离散化通常单独考虑。连续属性变换成分类属性涉及两个子任务决定需要多少个分类值,以及确定如何将连续属性值映射到这些分类值。在第一步中,将连续属性值排序后,通过指定n-1个分割点(split point)把它们分成n个区间。在颇为平凡的第二步中,将一个区间中的所有值映射到相同的分类值。因此,离散化问题就是决定选择多少个分割点和确定分割点位置的问题。结果可以用区间集合{(x0,x,(x1,x2,,(xn-1,xn)}表示,其中x和xn可以分别为∞或+∞,或者用一系列不等式x<x≤x1,,xn-1<x<x表示。非监督离散化用于分类的离散化方法之间的根本区别在于使用类信息(监督, supervised)还是不使用类信息(非监督, unsupervised)如果不使用类信息,则常使用一些相对简单的方法。例如,等宽(equal width)方法将属性的值域划分成具有相同宽度的区间,而区间的个数由用户指定。这种方法可能受离群点的影响而性能不佳,因此等频率(equal frequncy)或等深(equal depth)方法通常更为可取。等频率方法试图将相同数量的对象放进每个区间。作为非监督离散化的另一个例子,可以使用诸如K均值(见第8章)等类方法。最后,目测检查数据有时也可能是种有效的方法。
例2.12离散化技术本例解释如何对实际数据集使用这些技术。图2-13a显示了属于四个不同组的数据点,以及两个离群点位于两边的大点。可以使用上述技术将这些数据点的x值离散化成四个分类值。(数据集中的点具有随机的y分量,可以更容易地看出每组有多少个点。)尽管目测检查该数据的方法效果很好,但不是自动的,因此我们主要讨论其他三种方法。使用等宽、等频率和K均值技术产生的分割点分别如图2-13b、图2-13c和图2-13d所示,图中分割点用虚线表示。如果我们用不同组的不同对象被指派到相同分类值的程度来度量离散化技术的性能,则K均值性能最好,其次是等频率,最后是等宽。0101520020(a)原始数据(b)等宽离散化101520020等频率离散化(d)K均值离散化图2-13不同的离散化技术监督离散化上面介绍的离散化方法通常比不离散化好,但是记住最终目的并使用附加的信息(类标号)常常能够产生更好的结果。这并不奇怪,因为未使用类标号知识所构造的区间常常包含混合的类标号。一种概念上的简单方法是以极大化区间纯度的方式确定分割点。然而,实践中这种方法可能需要人为确定区间的纯度和最小的区间大小。为了解决这一问题,一些基于统计学的方法用每个属性值来分隔区间,并通过合并类似于根据统计检验得出的相邻区间来创建较大的区间。基于熵的方法是最有前途的离散化方法之一,我们将给出一种简单的基于熵的方法。首先,需要定义熵(entropy)设k是不同的类标号数,m是某划分的第i个区间中值的个数,而m是区间i中类j的值的个数。第i个区间的熵e由如下等式给出=-2P; log 2P PDG其中,py=mm是第i个区间中类j的概率(值的比例)该划分的总熵e是每个区间的熵的加
权平均,即 e=其中,m是值的个数,W1=mm是第i个区间的值的比例,而n是区间个数。直观上,区间的熵是区间纯度的度量。如果一个区间只包含一个类的值(该区间非常纯),则其熵为0并且不影响总熵。如果一个区间中的值类出现的频率相等(该区间尽可能不纯),则其熵最大一种划分连续属性的简单方法是:开始,将初始值切分成两部分,让两个结果区间产生最小熵。该技术只需要把每个值看作可能的分割点即可,因为假定区间包含有序值的集合。然后,取一个区间,通常选取具有最大熵的区间,重复此分割过程,直到区间的个数达到用户指定的个数,或者满足终止条件。例2.13两个属性离散化该方法用来独立地离散化图2-14所示的二维数据的属性x和y在图2-14a所示的第一个离散化中,属性x和被划分成三个区间(虚线指示分割点)在图2-14b所示的第二个离散化中,属性x和y被划分成五个区间。口25808808(a)三个区间(b)五个区间图2-14离散化四个点组(类)的属性x和y这个简单的例子解释了离散化的两个特点首先,在二维中,点类是很好分开的,但在一维中,情况并非如此。一般而言,分别离散化每个属性通常只保证次最优的结果。其次,五个区间比三个好,但是,至少从熵的角度看,六个区间对离散化的改善不大。(没有给出六个区间的熵值和结果。)因而需要有一个终止标准,自动地发现划分的正确个数。3.具有过多值的分类属性分类属性有时可能具有过多的值。如果分类属性是序数属性,则可以使用类似于处理连续属性的技术,以减少分类值的个数。然而,如果分类属性是标称的,就需要使用其他方法。考虑一所大学,它有许多系,因而系名属性可能具有数十个不同的值。在这种情况下,我们可以使用系之间联系的知识,将系合并成较大的组,如工程学、社会科学或生物科学。如果领域知识不能提供有用的指导,或者这样的方法会导致很差的分类性能,则需要使用更为经验性的方法,如仅当分组结果能提高分类准确率或达到某种其他数据挖掘目标时,才将值聚集到一起。
2.3.7变量变换变量变换(variable transformation)是指用于变量的所有值的变换。(尽管我们也偶尔用属性变换这个术语,但是遵循习惯用法,我们使用变量指代属性)换言之,对于每个对象,变换都作用于该对象的变量值。例如,如果只考虑变量的量级,则可以通过取绝对值对变量进行变换。接下来的部分,我们讨论两种重要的变量变换类型:简单函数变换和规范化。1.简单函数对于这种类型的变量变换,一个简单数学函数分别作用于每一个值。如果x是变量,这种变换的例子包括x,logx,ex,√x,1/x,sinx和x|在统计学中,变量变换(特别是平方根、对数和倒数变换)常用来将不具有高斯(正态分布的数据变换成具有高斯(正态)分布的数据。尽管这可能很重要,但是在数据挖掘中,其他理由可能更重要。假定感兴趣的变量是一次会话中的数据字节数,并且字节数的值域范围为1到10亿。这是一个很大的值域,使用常用对数变换将其进行压缩可能是有益的。这样的话,传输10和10字节的会话比传输10字节和1000字节的会话更为相似(9-8=1对3-1=2)。对于某些应用,如网络入侵检测,可能需要如此,因为前两个会话多半表示传输两个大文件,而后两个会话可能是两个完全不同的类型。使用变量变换时需要小心,因为它们改变了数据的特性。尽管有时需要这样做,但是如果变换的特性没有深入理解,则可能出现问题。例如,变换1/x虽然压缩了大于1的值,但是却放大了0和1之间的值,举例来说,{1,2,3}变换成{1,1/2,1/3},但是(1,1/2,1/3}变换成{1,2,3},这样,对于所有的值集,变换1逆转了序。为了帮助弄清楚一个变换的效果,重要的是要问如下问题:需要保序吗?变换作用于所有的值,特别是负值和0吗?变换对0和1之间的值有何特别影响?本章习题17考察了变量变换的其他方面。2.规范化或标准化另一种常见的变量变换类型是变量的标准化(standardization)或规范化(normalization)在数据挖掘界,这两个术语常常可互换,然而,在统计学中,术语规范化可能与使变量正态(高斯)的变换相混淆。标准化或规范化的目标是使整个值的集合具有特定的性质。一个传统的例子是统计学中的“对变量标准化”。如果是属性值的均值(平均值),而S是它们的标准差,则变换x=(x-x)s创建一个新的变量,它具有均值0和标准差1。如果要以某种方法组合不同的变量,则为了避免具有较大值域的变量左右计算结果,这种变换常常是必要的。例如,考虑使用年龄和收入两个变量对人进行比较。对于任意两个人,收入之差的绝对值(数百或数千元)多半比年龄之差的绝对值(小于150)大很多。如果没有考虑到年龄和收入值域的差别,则对人的比较将被收入之差所左右。例如,如果两个人之间的相似性或相异性使用本章后面的相似性或相异性度量来计算,则在很多情况下(如欧几里得距离)收入值将左右计算结果。均值和标准差受离群点的影响很大,因此通常需要修改上述变换。首先,用中位数(median)(即中间值)取代均值。其次,用绝对标准差( absolute standard deviation)取代标准差。例如,如果x是变量,则x的绝对标准差为OA=x-μ其中x是变量x的第i个值,m是对象的个数,而μ是均值或中位数。存在离群点时,计算值集的位置(中心)和发散估计的其他方法分别在3.2.3节和3.2.4节介绍。这些度量也可以用来定义标准化变换。 PDG2.4相似性和相异性的度量相似性和相异性是重要的概念,因为它们被许多数据挖掘技术所使用,如聚类、最近邻分类
和异常检测等。在许多情况下,一旦计算出相似性或相异性,就不再需要原始数据了。这种方法可以看作将数据变换到相似性(相异性)空间,然后进行分析。首先,我们讨论基本要素—相似性和相异性的高层定义,并讨论它们之间的联系。为方便起见,我们使用术语邻近度(proximity)表示相似性或相异性。由于两个对象之间的邻近度是两个对象对应属性之间的邻近度的函数,因此我们首先介绍如何度量仅包含一个简单属性的对象之间的邻近度,然后考虑具有多个属性的对象的邻近度度量。这包括相关和欧几里得距离度量,以及 Jaccard和余弦相似性度量。前二者适用于时间序列这样的稠密数据或二维点,后二者适用于像文档这样的稀疏数据。接下来,我们考虑与邻近度度量相关的若干重要问题。本节最后简略讨论如何选择正确的邻近度度量。2.4.1基础1.定义两个对象之间的相似度( similarity)的非正式定义是这两个对象相似程度的数值度量因而,两个对象越相似,它们的相似度就越高。通常相似度是非负的,并常常在0(不相似)和1(完全相似)之间取值。两个对象之间的相异度(dissimilarity)是这两个对象差异程度的数值度量。对象越类似,它们的相异度就越低。通常,术语距离(distance)用作相异度的同义词,正如我们将介绍的,距离常常用来表示特定类型的相异度。有时,相异度在区间[0,1]中取值,但是相异度在0和∞之间取值也很常见。2.变换通常使用变换把相似度转换成相异度或相反,或者把邻近度变换到一个特定区间,如[0,1]例如,我们可能有相似度,其值域从1到1,但是我们打算使用的特定算法或软件包只能处理相异度,或只能处理[0,1区间的相似度。之所以在这里讨论这些问题,是因为在稍后讨论邻近度时,我们将使用这种变换。此外,这些问题相对独立于特定的邻近度度量。通常,邻近度度量(特别是相似度)被定义为或变换到区间[0,1]中的值。这样做的动机是使用一种适当的尺度,由邻近度的值表明两个对象之间的相似(或相异)程度。这种变换通常是比较直截了当的。例如,如果对象之间的相似度在1(一点也不相似)和10(完全相似)之间变化,则我们可以使用如下变换将它变换到[0,1]区间:s=(s-1)/9,其中s和s分别是相似度的原值和新值。一般来说,相似度到[0,1]区间的变换由如下表达式给出:s'=(s-min_s)/(max_smins),其中maxs和min_s分别是相似度的最大值和最小值。类似地,具有有限值域的相异度也能用d=(d-min_d)/(max_d-min_d)映射到[0,1]区间。然而,将邻近度映射到[0,1区间可能非常复杂。例如,如果邻近度度量原来在区间[0,∞取值,则需要使用非线性变换,并且在新的尺度上,值之间不再具有相同的联系。对于从0变化到∞的相异度度量,考虑变换d=d/(1+d)相异度0、0.5、2、10、100和1000分别被变换到0、0.33、0.67、0.90、0.99和0.999在原来相异性尺度上较大的值被压缩到1附近,但是否希望如此则取决于应用。另一个问题是邻近度度量的含义可能会被改变。例如,相关性(稍后讨论是一种相似性度量,在区间[-1,1]上取值,通过取绝对值将这些值映射到[0,1区间丢失了符号信息,而对于某些应用,符号信息可能是重要的(见本章习题22)
将相似度变换成相异度或相反也是比较直截了当的,尽管我们可能再次面临保持度量的含义问题和将线性尺度改变成非线性尺度的问题。如果相似度(相异度)落在[0,1]区间,则相异度(相似度)可以定义为d=1-s(或s=1-d)另一种简单的方法是定义相似度为负的相异度(或相反)。例如,相异度0,1,10和100可以分别变换成相似度0,-1,-10和-100负变换产生的相似度结果不必局限于[0,1区间,但是,如果希望的话,则可以使用变换s=1/(d+1),s=e-或s=1- d-min_d对于变换s=1/(d+1),相异度0,1,10,100分 max_d-min_d别被变换到1,0.5,0.09,0.01;对于s=e-d,它们分别被变换到1.00,0.37,0.00,0.00;对于s= 1--d-min_d max_d-min_d,它们分别被变换到1.00,0.99,0.00,0.00。在这里的讨论中,我们关注将相异度变换到相似度。相反方向的转换参见本章习题23。一般来说,任何单调减函数都可以用来将相异度转换到相似度(或相反)当然,在将相似度变换到相异度(或相反),或者在将邻近度的值变换到新的尺度时,也必须考虑一些其他因素。我们提到过一些问题,涉及保持意义、扰乱标度和数据分析工具的需要,但是肯定还有其他问题。2.4.2简单属性之间的相似度和相异度通常,具有若干属性的对象之间的邻近度用单个属性的邻近度的组合来定义,因此我们首先讨论具有单个属性的对象之间的邻近度考虑由一个标称属性描述的对象,对于两个这样的对象,相似意味什么呢?由于标称属性只携带了对象的相异性信息,因此我们只能说两个对象有相同的值,或者没有。因而在这种情况下,如果属性值匹配,则相似度定义为1,否则为0;相异度用相反的方法定义:如果属性值匹配,相异度为0,否则为1对于具有单个序数属性的对象,情况更为复杂,因为必须考虑序信息。考虑一个在标度{poor,fair,Ok,good, wonderful}上测量产品(例如,糖块)质量的属性。一个评定为wonderful的产品P1与一个评定为good的产品P2应当比它与一个评定为OK的产品P3更接近。为了量化这种观察,序数属性的值常常映射到从0或1开始的相继整数,例如,{poor=0,fair=1,ok=2,good=3, wonderful=4}.于是,P1与P2之间的相异度d(P1,P2)=3-2=1,或者,如果我们希望相异度在0和1之间取值,d(P1,P2)=(3-2)/=0.25;序数属性的相似度可以定义为s=1-d。序数属性相似度(相异度)的这种定义可能使读者感到有点担心,因为这里我们定义了相等的区间,而事实并非如此。如果根据实际情况,我们应该计算出区间或比率属性。值fair与good的差真和OK与 wonderful的差相同吗?可能不相同,但是在实践中,我们的选择是有限的,并且在缺乏更多信息的情况下,这是定义序数属性之间邻近度的标准方法。对于区间或比率属性,两个对象之间的相异性的自然度量是它们的值之差的绝对值。例如,我们可能将现在的体重与一年前的体重相比较,说“我重了10磅。”在这类情况下,相异度通常在0和∞之间,而不是在0和1之间取值。如前所述,区间或比率属性的相似度通常转换成相异度。表2-7总结了这些讨论。在该表中,x和y是两个对象,它们具有一个指明类型的属性,d(x,y和s(x,y)分别是x和y之间的相异度和相似度(分别用d和s表示)其他方法也是可能的,但是表中的这些是最常用的。
表2-7简单属性的相似度和相异度属性类型相异度相似度标称的d=如果x=y1如果x=y1如果x≠y510如果x≠y d=lx-yV(n-1)序数的(值映射到整数0到n-1,其中ns=1-d是值的个数)区间或1 d-min_d比率的 d=lx-yl下面两节介绍更复杂的涉及多个属性的对象之间的邻近性度量:(1)数据对象之间的相异度;(2)数据对象之间的相似度。这样分节可以更自然地展示使用各种邻近度度量的基本动机。然而,我们要强调的是使用上述技术,相似度可以变换成相异度,反之亦然。2.4.3数据对象之间的相异度本节,我们讨论各种不同类型的相异度。我们从讨论距离(距离是具有特定性质的相异度)开始,然后给出一些更一般的相异度类型的例子。距离我们首先给出一些例子,然后使用距离的常见性质更正式地介绍距离。一维、二维、三维或高维空间中两个点x和y之间的欧几里得距离(Euclidean distance)d由如下熟悉的公式定义:d(x,y)=(x-y)2(2-1)其中,n是维数,而x和y分别是x和y的第k个属性值(分量)我们用图2-15、表2-8和表2-9解释该公式,它们展示了这个点集、这些点的x和y坐标以及包含这些点之间距离的距离矩阵(distance matrix)2p1 p3 p4 p201◆2●3x图2-15四个二维点表2-8四个点的x和y坐标表2-9表2-8的欧几里得距离矩阵点x坐标y坐标 pl p2 p3p4 pl p2 p3023520pl0.02.83.25.1p22.80.01.43.233.21.40.02.0 p45.13.22.00.0
公式(2-1)给出的欧几里得距离可以用公式(22)的闵可夫斯基距离(Minkowski distance来推广: dax, ) (Ix-y)(2-2)其中r是参数。下面是闵可夫斯基距离的三个最常见的例子。r=1,城市街区(也称曼哈顿、出租车L1范数)距离。一个常见的例子是汉明距离(Hamming distance),它是两个具有二元属性的对象(即两个二元向量)之间不同的二进制位个数。r=2,欧几里得距离(L2范数)r=∞,上确界(Lmax或L范数)距离。这是对象属性之间的最大距离。更正式地,L距离由公式(2-3)定义:(2-3)注意不要将参数r与维数(属性数)n混淆。欧几里得距离、曼哈顿距离和上确界距离是对n的所有值(1,2,3,…)定义的,并且指定了将每个维(属性)上的差的组合成总距离的不同方法。表2-10和表2-11分别给出表2-8数据的L1离和距离的邻近度矩阵。注意,所有的距离矩阵都是对称的,即第个表目与第j个表目相同,例如,在表2-9中,第4行第1列和第1行第4列都包含值5.1表2-10表2-8的L1距离矩阵表2-11表2-8的L距离矩阵Lp pl p2 p3p4LP pl p2 p3 p40.04.04.06.010.02.03.05.0p24.00.02.04.0p22.00.01.03.0p34.02.00.02.0p33.01.00.02.0p46.04.02.00.0p45.03.02.00.0距离(如欧几里得距离)具有一些众所周知的性质。如果d(x,y)是两个点x和y之间的距离,则如下性质成立。(1)非负性。(a)对于所有x和y,d(x,y)≥0,(b)仅当x=y时d(x,y)=0(2)对称性。对于所有x和y,d(x,y)=d(y,x)(3)三角不等式。对于所有x,y和z,d(,z)≤d(x,y)+d(y,z满足以上三个性质的测度称为度量( metric)。有些人只对满足这三个性质的相异性度量使用术语距离,但在实践中常常违反这一约定。这里介绍的三个性质是有用的,数学上也是令人满意的。此外,如果三角不等式成立,则该性质可以用来提高依赖于距离的技术(包括聚类)的效率(见本章习题25)。尽管如此,许多相异度都不满足一个或多个度量性质。下面我们给出两个这种测度的例子。例2.14非度量的相异度:集合差基于集合论中定义的两个集合差的概念举例。设有两个
集合A和B,A-B是不在B中的A中元素的集合。例如,如果A={1,2,3,4},而B={2,3,4},则A-B={1},而B-A=,即空集。我们可以将两个集合A和B之间的距离定义为d(A,B)=size(-),其中size是一个函数,它返回集合元素的个数。该距离测度是大于或等于零的整数值,但不满足非负性的第二部分,也不满足对称性,同时还不满足三角不等式。然而,如果将相异度修改为d(,B)=size(-)+size(B-A),则这些性质都可以成立(见本章习题21)口例2.15非度量的相异度:时间这里给出一个更常见的例子,其中相异性测度并非度量,但依然是有用的。定义时间之间的距离测度如下:d(t)=124+(t2-)如果t≥t2(2-4)例如,(1PM,2PM)=1小时,而d(2PM,1P)=23小时这种定义是有意义的,例如,在回答如下问题时就体现了这种定义的意义:“如果一个事件在每天下午1点发生,现在是下午2点,那么我们还需要等待多长时间才能等到该事件再度发生?”口2.4.4数据对象之间的相似度对于相似度,三角不等式(或类似的性质通常不成立,但是对称性和非负性通常成立。更明确地说,如果s(x,y)是数据点x和y之间的相似度,则相似度具有如下典型性质。(1)仅当x=y时s(x,y)=1(0≤s≤1)(2)对于所有x和y,s(x,y)=s(y,x)。(对称性)对于相似度,没有与三角不等式对应的一般性质。然而,有时可以将相似度简单地变换成一种度量距离。稍后讨论的余弦相似性度量和 Jaccard相似性度量就是两个例子。此外,对于特定的相似性度量,还可能在两个对象相似性上导出本质上与三角不等式类似的数学约束。例2.16非对称相似性度量考虑一个实验,实验中要求人们对屏幕上快速闪过的一小组字符进行分类。该实验的混淆矩阵(confusion matrix)记录每个字符被分类为自己的次数和被分类为另一个字符的次数。例如,假定“0”出现了200次,它被分类为“0”160次,而被分类为“o”40次。类似地,“o”出现200次并且分类为“”170次,但是分类为“0”只有30次。如果取这些计数作为两个字符之间相似性的度量则得到一种相似性度量,但这种相似性度量不是对称的。在这种情况下,通过选取s(x,y)=s(y,x)(s(x,y)+s(y,x)2,相似性度量可以转换成对称的,其中s是新的相似性度量。2.4.5邻近性度量的例子本节给出一些相似性和相异性度量的具体例子。1.二元数据的相似性度量两个仅包含二元属性的对象之间的相似性度量也称为相似系数( similarity coefficient),并且通常在0和1之间取值,值为1表明两个对象完全相似而值为0表明对象一点也不相似有许多理由表明在特定情形下,一种系数为何比另一种好。设x和y是两个对象,都由n个二元属性组成。这样的两个对象(即两个二元向量)的比较可生成如下四个量(频率)
foo=x取0并且y取0的属性个数fo1=x取0并且y取1的属性个数fio=x取1并且y取0的属性个数fi=x取1并且y取1的属性个数简单匹配系数(Simple Matching Coefficient,MC)一种常用的相似性系数是简单匹配系数,定义如下:SMC值匹配的属性个数一f++f+fo+ foo(2-5)属性个数该度量对出现和不出现都进行计数。因此,SMC可以在一个仅包含是非题的测验中用来发现回答问题相似的学生。 Jaccard系数(Jaccard Coefficient)假定x和y是两个数据对象,代表一个事务矩阵(见2.1.2节)的两行(两个事务)。如果每个非对称的二元属性对应于商店的一种商品,则1表示该商品被购买,而0表示该商品未被购买。由于未被顾客购买的商品数远大于被其购买的商品数,因而像SMC这样的相似性度量将会判定所有的事务都是类似的。这样,常常使用 Jaccard系数来处理仅包含非对称的二元属性的对象。 Jaccard系数通常用符号J表示,由如下等式定义:匹配的个数f不涉及0-0匹配的属性个数fo1+f1+fu(2-6)例2.17smc和 Jaccard相似性系数为了解释这两种相似性度量之间的差别,我们对如下二元向量计算SMC和J=(1,0,0,0,0,0,0,0,0,0y=(0,0,0,0,0,0,1,0,0,1)fo1=2x取0并且y取1的属性个数fi=1x取1并且y取0的属性个数fo=7x取0并且y取0的属性个数fi=0x取1并且y取1的属性个数 SMC=+foo0+7fo+fi+fi+f02+1+0+70.7 J=f=0=0 for+ fio++1+02.余弦相似度通常,文档用向量表示,向量的每个属性代表一个特定的词(术语)在文档中出现的频率。当然,实际情况要复杂得多,因为需要忽略常用词,并使用各种技术处理同一个词的不同形式、不同的文档长度以及不同的词频。尽管文档具有数以百千计或数以万计的属性(词),但是每个文档向量都是稀疏的,因为它具有相对较少的非零属性值。(文档规范化并不对零词目创建非零词目,即文档规范化保持稀疏性)这样,与事务数据一样,相似性不能依赖共享0的个数,因为任意两个文档多半都不会包
含许多相同的词,从而如果统计0-0匹配,则大多数文档都与其他大部分文档非常类似。因此,文档的相似性度量不仅应当像 Jaccard度量一样需要忽略00匹配,而且还必须能够处理非二元向量。下面定义的余弦相似度(cosine similarity就是文档相似性最常用的度量之一。如果x和y是两个文档向量,则(2-7)Ⅲ其中,“”表示向量点积,x.y=x,是量x的长度,1==√x。例2.18两个文档向量的余弦相似度该例计算下面两个数据对象的余弦相似度,这些数据对象可能代表文档向量:x=(3,2,0,5,0,0,0,2,0,0)y=(1,0,0,0,0,0,0,1,0,2)xy=3×1+2×0+00+5x0+0x0+0x+00+2×1+00+02=5=33+2x2+00+55+0x0+0x0+0x0+2x2+0×0+0x0=6.48y√1+00+00+0x0+00+0x0+0x0+1×1+00+2x2=2.45cos(x,y)=0.31口如图2-16所示,余弦相似度实际上是x和y之间夹角(余弦)的度量。这样,如果余弦相似度为1,则x和y之间夹角为0°,并且除大小(长度)之外,x和y是相同的;如果余弦相似度为0,则x和y之间夹角为90°,并且它们不包含任何相同的词(术语)图2-16余弦度量的几何解释公式(2-7)可以写成公式(2-8)的形式: cos(,)==.y(2-8)其中,x=x/x,而y=y川x和y被它们的长度除,将它们规范化成具有长度1。这意味在计算相似度时,余弦相似度不考虑两个数据对象的量值。(当量值是重要的时,欧几里得距离可能是一种更好的选择。)对于长度为1的向量,余弦度量可以通过简单地取点积计算。从而,在需要计算大量对象之间的余弦相似度时,将对象规范化,使之具有单位长度可以减少计算时间。3.广义 Jaccard系数广义 Jaccard系数可以用于文档数据,并在二元属性情况下归约为 Jaccard系数。广义 Jaccard系数又称 TatoTanimoto系数。(然而,还有一种系数也称系数)该系数用EJ表示,由下式定义:
 X.y E(,)+ -x.y(2-9)4.相关性两个具有二元变量或连续变量的数据对象之间的相关性是对象属性之间线性联系的度量。(更一般属性之间的相关性计算可以类似地定义。)更准确地,两个数据对象x和y之间的皮尔森相关(Pearson's' correlation)系数由下式定义: corr(x,)= covariance(x,y)=(2-10) standard_deviation(x) x standard_deviation(y),sy这里我们使用标准的统计学记号和定义: covariance(x,)=(x-x)(y-))(2-11) standard_deviation(x)= Sx= (x2-x) standard_deviation(y)=sy= (y.-y)2x=1它x是x的均值ny=y是y的均值例2.19完全相关相关度总是在-1到1之间取值。相关度为1(-1)意味x和y具有完全正(负)线性关系,即x=ayk+b,其中a和b是常数。下面两个x和y的值集分别给出相关度为-1和+1的情况。为简单起见,第一组中取x和y的均值为0x=(-3,6,0,3,-6y=(1,-2,0,-1,2)x=(3,6,0,3,6y=(1,2,0,1,2)例2.20非线性关系如果相关度为0,则两个数据对象的属性之间不存在线性关系。然而,仍然可能存在非线性关系。在下面的例子中,数据对象的属性之间存在非线性关系yk=x2,但是它们的相关度为0x=(-3,-2,-1,0,1,2,3)y=(9,4,1,0,1,4,9)口例2.21相关性可视化通过绘制对应属性值对可以很容易地判定两个数据对象x和y之间的相关性。图2-17给出了一些这种图x和y具有30个属性,这些属性的值随机地产生(服从正态分布),使得x和y的相关度从-1到1。图中每个小圆圈代表30个属性中的一个,其x坐标是x的一个属性的值,而其y坐标是y的相同属性的值。
1.000.900.80-0.700.600.500.40-0.30-0.200.100.000.100.200.300.400.500.600.700.800.901.00图2-17解释相关度从-1到1的散布图如果通过减去均值,然后规范化使其长度为1来变换x和y,则它们的相关度可以通过求点积来计算注意,这与其他情况下使用的标准化不同在其他情况下,我们使用变换x=(xk-x)x和y=(yk-y)sy Bregman散度*本节,我们简略介绍 Bregman散度(Bregman divergence),它是一族具有共同性质的邻近函数。这样,可以构造使用 Bregman发散函数的一般数据挖掘算法,如聚类算法,具体的例子是K均值聚类算法(8.2节。注意,本节需要向量计算方面的知识。 Bregman散度是损失或失真函数。为了理解损失函数,考虑如下情况:设x和y是两个点,其中y是原来的点,而x是它的某个失真或近似,例如,可能是由于添加了一些随机噪声到y上而产生的。损失函数的目的是度量用x近似导致的失真或损失。当然,x和y越类似,失真或损失就越小,因而 Bregman散度可以用作相异性函数。有如下正式定义。定义2.6 Bregman散度给定一个严格凸函数中(连同一些通常满足的适度限制),由该函数生成的 Bregman散度(损失函数)D(x,y)通过下面的公式给出:D(x,y)=(x)-(y)-o(y),(x-y)(2-12)其中,V(y)是在y上计算的的梯度,xy是x与y的向量差,而((y),(x-y)是V()和(x-y)的内积对于欧几里得空间中的点,内积就是点积。D(x,y)可以写成D(x,y)=中(x)-L(x),其中L(x)=(y)+V(y)(x-y)代表在y上正切于函数中的平面方程。使用微积分学的术语,L(x)是函数在y点附近的线性部分,而 Bregman散度是一个函数与该函数的线性近似之间的差。选取不同的p,可以得到不同的 Bregman散度。例2.22我们使用平方欧几里得距离给出 Bregman散度的一个具体例子。为了简化数学计
算,我们仅限于一维。设x和y是实数,而t)是实数值函数,(t=t2在此情况下,梯度归结为导数,而点积归结为乘积。例如,公式(2-12)变成公式(2-13)D(x,y)=x2-y2-2y(x-y)=(x-y)(2-13)该例的图形在图2-18中给出,其中y=1在x=2和x=3上给出了 Bregman散度。口10 \p(x)=x2D(213图2-18图示 Bregman散度2.4.6邻近度计算问题本节讨论与邻近性度量有关的一些重要问题:(1)当属性具有不同的尺度(scale)或相关时如何处理;(2)当对象包含不同类型的属性(例如,定量属性和定性属性)时如何计算对象之间的邻近度;(3)当属性具有不同的权重(即并非所有的属性都对对象的邻近度具有相等的贡献)时,如何处理邻近度计算。1.距离度量的标准化和相关性距离度量的一个重要问题是当属性具有不同的值域时如何处理。(这种情况通常称作“变量具有不同的尺度。”)前面,使用欧几里得距离,基于年龄和收入两个属性来度量人之间的距离。除非这两个属性是标准化的,否则两个人之间的距离将被收入所左右。一个相关的问题是,除值域不同外,当某些属性之间还相关时,如何计算距离。当属性相关、具有不同的值域(不同的方差)并且数据分布近似于高斯(正态)分布时,欧几里得距离的拓广, Mahalanobis距离是有用的。具体地说,两个对象(向量)x和y之间的 Mahalanobis距离定义为: mahalanobis(x, y)=(x-y)(x-y)?(2-14)其中1是数据协方差矩阵的逆。注意,协方差矩阵是这样的矩阵,它的第个元素是第i个和第j个属性的协方差,由公式(2-11)定义。例2.23在图2-19中有1000个点,其x属性和y属性的相关度为0.6在椭圆长轴两端的距离的费用昂贵,但是对于其属性相关的对象来说是值得的。如果属性相对来说不相关,只是具PDG两个大点之间的欧几里得距离为14.7,但 Mahalanobis距离仅为6。实践中,计算 Mahalanobis有不同的值域,则只需要对变量进行标准化就足够了。
 5r.........................3203图2-19二维点的集合。两个大点代表的点之间的 Mahalanobis距离为6,它们的欧几里得距离为14.72.组合异种属性的相似度前面的相似度定义所基于的方法都假定所有属性具有相同类型。当属性具有不同类型时,就需要更一般的方法。直截了当的方法是使用表2-7分别计算出每个属性之间的相似度,然后使用一种导致0和1之间相似度的方法组合这些相似度。总相似度一般定义为所有属性相似度的平均值。不幸的是,如果某些属性是非对称属性,这种方法效果不好例如,如果所有的属性都是非对称的二元属性,则相似性度量先归结为简单匹配系数一种对于二元非对称属性并不合适的度量。处理该问题的最简单方法是:如果两个对象在非对称属性上的值都是0,则在计算对象相似度时忽略它们。类似的方法也能很好地处理遗漏值概括地说,算法2.1可以有效地计算具有不同类型属性的两个对象x和y之间的相似度。修改该过程可以很轻松地处理相异度。算法2.1异种对象的相似度1:对于第k个属性,计算相似度s(x,y),在区间[0,1]中2:对于第k个属性,定义一个指示变量,如下:=0,如果第k个属性是非对称属性,并且两个对象在该属性上的值都是0,或者如果一个对象的第k个属性具有遗漏值=1,否则3:使用如下公式计算两个对象之间的总相似度: (x,y) similarity(x,)=(2-15)3.使用权值在前面的大部分讨论中,所有的属性在计算邻近度时都会被同等对待。但是,当某些属性对邻近度的定义比其他属性更重要时,我们并不希望这种同等对待的方式。为了处理这种情况,可以通过对每个属性的贡献加权来修改邻近度公式。如果权wk的和为1,则公式(2-15)变成
 similarity(x, y)=, (x,y)(2-16)闵可夫斯基距离的定义也可以修改为:r(2-17)2.4.7选取正确的邻近性度量下面是一些一般观察,可能会对你有所帮助。首先,邻近性度量的类型应当与数据类型相适应。对于许多稠密的、连续的数据,通常使用距离度量,如欧几里得距离等。连续属性之间的邻近度通常用属性值的差来表示,并且距离度量提供了一种将这些差组合到总邻近性度量的良好方法。尽管属性可能有不同的取值范围和不同的重要性,但这些问题通常都可以用前面介绍的方法处理。对于稀疏数据,常常包含非对称的属性,通常使用忽略00匹配的相似性度量。从概念上讲,这反映了如下事实:对于一对复杂对象,相似度依赖于它们共同具有的性质数目,而不是依赖于它们都缺失的性质数目。在特殊的情况下对于稀疏的、非对称的数据,大部分对象都只具有少量被属性描述的性质,因此如果考虑它们都不具有的性质的话,它们都高度相似。余弦、 Jaccard和广义 Jaccard度量对于这类数据是合适的。数据向量还有一些其他特征需要考虑。例如,假定对于比较时间序列感兴趣。如果时间序列的量值是重要的(例如,每个时间序列表示同一单位不同年份的总销售),则可以使用欧几里得距离。如果时间序列代表不同的量(例如,血压和氧消耗量),通常需要确定时间序列是否具有相同的形状,而不是相同的量值,那么相关度可能更可取(使用考虑量和级的差异的内置规范化)在某些情况下,为了得到合适的相似性度量,数据的变换或规范化是重要的,因为这种变换并非总能在邻近性度量中提供,例如,时间序列数据可能具有显著影响相似性的趋势或周期模式。此外,正确地计算相似度还需要考虑时间延迟。最后,两个时间序列可能只在特定的时间周期上相似,例如,气温与天然气的用量之间存在很强的联系,但是这种联系仅出现在取暖季节。实践考虑也是重要的。有时,一种或多种邻近性度量已经在某个特定领域使用,因此,其他人已经回答了应当使用何种邻近性度量的问题;另外,所使用的软件包或聚类算法可能完全限制了选择;如果关心效率,则我们可能希望选择具有某些性质的邻近性度量,这些性质(如三角不等式)可以用来降低邻近度计算量(见本章习题25)然而,如果通常的实践或实践限制并未规定某种选择,则正确地选择邻近性度量可能是一项耗时的任务,需要仔细地考虑领域知识和度量使用的目的。可能需要评估许多不同的相似性度量,以确定哪些结果最有意义。文献注释理解待分析的数据至关重要,并且在基本层面,这是测量理论的主题比如说,定义属性类型的初始动机是精确地指出哪些统计操作对何种数据是合法的。我们给出了测量理论的概述,这些源于s.s. Stevens的经典文章[79]。(表2-2和表23取 Stevens自[80])尽管这是最普遍的观点并且相当容易理解和使用,但是测量理论远不止这些。权威的讨论可以在测量理论基础的三卷系
列[63,69,81中找到。同样值得关注的是Hand[55]的文章,文中广泛地讨论了测量理论和统计学,并且附有该领域其他研究者的评论。最后,有许多书籍和文章都介绍了科学与工程学的特定领域中的测量问题。数据质量是一个范围广泛的主题,涉及使用数据的每个学科。精度、偏倚、准确率的讨论和一些重要的图可以在许多科学、工程学和统计学的导论性教材中找到。数据质量“适合使用”的观点在 Redman的书[76]中有更详细的解释。对数据质量感兴趣的人一定也会对MT的总体数据质量管理计划[70,84]感兴趣然而,处理特定领域的数据质量问题所需要的知识最好是通过考察该领域的研究者的数据质量实践而得到。与其他预处理任务相比,聚集是一个不够成形的主题。然而,聚集是数据库联机分析处理(OLAP)领域使用的主要技术之一,这将在第3章讨论。聚集在符号数据分析领域也起到了一些作用(Bock和 Diday[47)该领域的一个目标是用符号数据对象汇总传统的记录数据,而符号数据对象的属性比传统属性更复杂。例如这些属性的值可能是值的集合(类别)、区间、具有权重的值的集合(直方图)。符号数据分析的另一个目标是能够在由符号数据对象组成的数据上进行聚类、分类和其他类型的数据分析。抽样是一个已经在统计学及其相关领域中透彻研究的主题。许多统计学导论性书籍(如 Lindgren的书[65])都有关于抽样的讨论,并且还有通篇讨论该主题的书,如 Cochran的经典教科书[49]Gu和Liu[54]提供了关于数据挖掘抽样综述,而 Olken和 Rotem[72]提供了关于数据库抽样的综述。还有许多涉及数据挖掘和数据库抽样的文献也值得关注,包括 Palmer和 Faloutsos[74]、 Provost等[75]、 Toivonen[2、Zaki等[85]的文章。在统计学,已经用于维归约的传统技术是多维定标(MDS)(Borg和 Groenen[48], Kruskal和 Uslaner[64])和主成分分析(Ca)( Jolliffe5[58]),主成分分析类似于奇异值分解(sD) (Demmel[50]).离散化是一个已经在数据挖掘领域广泛讨论的主题。有些分类算法只能使用分类属性,并且关联分析需要二元数据,这样就有了重要的动机,去考察如何最好地对连续属性进行二元化或离散化。对于关联分析,建议读者阅读 Srikant Agrawal的文章[78],而分类领域离散化的一些有用的参考文献包括 Dougherty等[51]、 Elomaa和Rusu[52]、 Fayyad和rani[53]以及 Hussain等[56]特征选择是另一个在数据挖掘领域被彻底研究的主题 Molina等的综述[71]和Liu和 Motada的两本书[66,67]提供了涵盖该主题的广泛材料。其他有用的文章包括Blum和 Langley[46]Kohavi和John[62]和Liu等[68]很难提供特征变换主题的参考文献,因为不同学科的实践差异很大。许多统计学书籍都讨论了变换,但是讨论通常都限于特定的目的,如确保变量的规范性,或者确保变量具有相等的方差。我们提供两种参考文献: Osborne[73]和 Tukey[83]尽管我们已经讨论了一些最常用的距离和相似性度量,但是还有数以百计的这样的度量,并且更多的度量还正在提出。与本章的其他许多主题一样,许多度量都局限于特定的领域,例如,在时间序列领域,见 Kalpakis等[59]、Keoh和 Pazzani[61]的文章。聚类方面的书提供了最好的一般讨论,特别是如下书籍: Anderberg[45]、Jain和 Dubes[57]、 Kaufman和 Rousseeuw[60]以及 Sneath和 Sokal[77] PDG
参考文献 [45] M. R. Anderberg. Cluster Analysis for Applications. Academic Press, New York, December 1973 [46] A. Blum and. Langley. Selection of Relevant Features and Examples in Machine Learning. Artificial Intelligence, 97(1-2): 245-271, 1997. [47] H. H. Bock and E. Diday. Analysis of Symbolic Data: Exploratory Methods for Extracting Statistical Information from Complex Data (Studies in Classification, Data Analysis, and Knowledge Organization) Springer-Verlag Telos, January 2000. [48] I. Borg and P. Groenen. Modern Multidimensional Scaling-Theory and Applications. Springer- Verlag, February 199749]w.g. . Cochran. Sampling Techniques. John Wiley&sons,3 3rd edition,july1977.[50]j.w. Demmel. Applied Numerical Linear Algebra. Society for Industrial& Applied Mathematics, September 1997.[51]j. Dougherty,r. Kohavi,andm. Sahami. Supervised and Unsupervised Discretization of Continuous Features. In Proc. of the 12th Intl. Conf. on Machine Learning, pages 194-202, 1995 [52] T. Elomaa and J. Rousu. General and Efficient Multisplitting of Numerical Attributes. Machine Learning,36(3):201-244,1999 [53] U. M. Fayyad and K. B. Irani. Multi-interval discretization of continuousvalued attributes for classification learning. In Proc. 13th Int. Joint Conf. on Artificial Intelligence, pages 1022-1027. Morgan Kaufman, 1993. [54] F. H. Gaohua Gu and H. Liu. Sampling and Its Application in Data Mining: A Survey.Technical Report TRA6/00, National University of Singapore, Singapore, 2000. [55] D. J. Hand. Statistics and the Theory of Measurement. Journal of the Royal Statistical Society:Series A(Statistics in Society), 159(3):445-492, 1996. [56] F. Hussain, H. Liu, C. L. Tan, and M. Dash. TRC6/99: Discretization: an enabling technique. Technical report, National University of Singapore, Singapore, 1999. (57] A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice Hall Advanced Reference Series. Prentice Hall, March 1988. Book available online at http://www.cse.msu.edu/-jain/ Clustering Jain Dubes.pdf. [58] I. T. Jolliffe. Principal Component Analysis. Springer Verlag, 2nd edition, October 2002 [59] K. Kalpakis, D. Gada, and V. Puttagunta Distance Measures for Effective Clustering of ARIMA Time-Series. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining, pages 273-280. IEEE Computer Society, 2001. [60] L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: An Introduction to Cluster Analysis. Wiley Series in Probability and Statistics. John Wiley and Sons, New York, November 1990 [61] E. J. Keogh and M. J. Pazzani. Scaling up dynamic time warping for datamining applications. In KDD, pages285-289,2000. [62] R. Kohavi and G. H. John. Wrappers for Feature Subset Selection. Artificial Intelligence, 97(1-2):273-324,1997. [63] D. Krantz, R. D. Luce, P. Suppes, and A. Tversky Foundations of Measurements: Volume I: Additive and polynomial representations. Academic Press, New York, 1971. [64] J. B. Kruskal and E.. Uslaner. Multidimensional Scaling. Sage Publications, August 1978 [65] B. W. Lindgren. Statistical Theory. CRC Press, January 1993 [66] H. Liu and H. Motoda, editors. Feature Extraction, Construction and Selection: A Data Mining Perspective. Kluwer International Series in Engineering and Computer Science, 453. Kluwer Academic Publishers, July 1998. [67] H. Liu and H. Motoda. Feature Selection for Knowledge Discovery and Data Mining. Kluwer International Series in Engineering and Computer Science, 454. Kluwer Academic Publishers, July61998 [68] H. Liu, H. Motoda, and L. Yu. Feature Extraction, Selection, and Construction. In N. Ye, editor, The
 Handbook of Data Mining, pages 22-41. Lawrence Erlbaum Associates, Inc., Mahwah, NJ, 2003. [69] R. D. Luce,. Krantz, P. Suppes, and A. Tversky. Foundations of Measurements: Volume 3: Representation, Axiomatization, and Invariance. Academic Press, New York, 1990. [70] MIT Total Data Quality Management Program. web.mit.edu/tdqm/www/index.shtml, 2003 [71] L. C. Molina, L. Belanche, and A. Nebot. Feature Selection Algorithms: Survey and Experimental Evaluation. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, 2002. [72] F. Olken and D. Rotem. Random Sampling from Databases A Survey. Statistics Computing,5(1):25-42, March1995[73]J. Osborne. Notes on the Use of Data Transformations. Practical Assessment, Research& Evaluation,28(6),2002 [74] C.. Palmer and C. Faloutsos. Density biased sampling: An improved method for data mining and clustering. ACM SIGMOD Record, 29(2): 82 -92, 2000. [75] F. J. Provost, D. Jensen, and T. Oates. Efficient Progressive Sampling. In Proc. of the 5th Inrl. Conf. on Knowledge Discovery and Data Mining, pages 23-32,1999. [76] T. C. Redman Data Quality: The Field Guide. Digital Press, January 2001. [77] P. H. A. Sneath and R. R. Sokal. Numerical Taxonomy. Freeman, San Francisco, 1971. [78] R. Srikant and R. Agrawal. Mining Quantitative Association Rules in Large Relational Tables. In Proc. of 1996 ACM-SIGMOD Intl. Conf. on Management of Data, pages 1-12, Montreal, Quebec, Canada, August 1996. [79] S.S. Stevens. On the Theory of Scales of Measurement. Science, 103(2684): 677-680, June 1946 [80] S. S. Stevens. Measurement. In G. M. Maranell, editor, Scaling: A Sourcebook for Behavioral Scientists, pages 22-41. Aldine Publishing Co., Chicago, 1974. [81] P. Suppes, D. Krantz, R. D. Luce, and A. Tversky. Foundations of Measurements Volume 2: Geometrical, Threshold, and Probabilistic Representations. Academic Press, New York,1989. [82]. Toivonen. Sampling Large Databases for Association Rules. In VLDB96, pages 134-145. Morgan Kaufman, September 1996 [83] J. W. Tukey. On the Comparative Anatomy of Transformations. Annals of Mathematical Statistics,28(3):602-632, September1957 [84] R. Y. Wang, M. Ziad, Y. W. Lee, and Y R. Wang. Data Quality. The Kluwer International Series on Advances in Database Systems, Volume 23. Kluwer Academic Publishers, January 2001. [85]. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara. Evaluation of Sampling for Data Mining of Association Rules. Technical Report TR617 Rensselaer Polytechnic Institute, 1996习题1.在第2章的第一个例子中,统计人员说:“是的,字段2和3也有不少问题。”从所显示的三行样本数据,你能解释她为什么这样说吗?2.将下列属性分类成二元的、离散的或连续的,并将它们分类成定性的(标称的或序数的)或定量的(区间的或比率的)。某些情况下可能有多种解释,因此如果你认为存在二义性,简略给出你的理由。例子:年龄。回答:离散的、定量的、比率的。(a)用AM和PM表示的时间。(b)根据曝光表测出的亮度。所(c)根据人的判断测出的亮度。(d)按度测出的0和360之间的角度。(e)奥运会上授予的铜牌、银牌和金牌。 PDG)海拔高度。
(g)医院中的病人数。(h)书的ISBN号(查找网上的格式)(i)用如下值表示的透光能力:不透明、半透明、透明。)军衔。(k)到校园中心的距离。(用每立方厘米克表示的物质密度。(m)外套寄存号码。(出席一个活动时,你通常会将外套交给服务生,然后他给你一个号码,你可以在离开时用它来领取你的外套。)3.某个地方公司的销售主管与你联系,他相信他已经设计出了一种评估顾客满意度的完美方法。他这样解释他的方案:“这太简单了我简直不敢相信,以前竟然没有人想到,我只是记录顾客对每种产品的抱怨次数,我在数据挖掘书中读到计数具有比率属性,因此,我的产品满意度度量必定具有比率属性。但是,当我根据顾客满意度度量评估产品并拿给老板看时,他说我忽略了显而易见的东西,说我的度量毫无价值。我想,他简直是疯了,没发现我们的畅销产品满意度最差,因为对它的抱怨最多。你能帮助我摆平他吗?”(a)谁是对的,销售主管还是他的老板?如果你的回答是他的老板,你需要做些什么来修正满意度度量?(b)对于原来的产品满意度度量的属性类型,你的想法是什么?4.几个月之后,习题3中提到的那个销售主管又同你联系。这次,他设计了一个更好的方法,用以评估顾客喜爱一种产品超过喜爱其他类似产品的程度。他解释说:“在开发一种新产品时,我们通常创建一些变种并评估顾客更喜欢哪一种。我们的标准做法是同时散发所有的产品变种并要求他们根据喜爱程度对产品变种划分等级。然而,我们的评测题目很不明确,当有两个以上产品时尤其如此,这让测试占用了很长的时间。我建议对产品逐对比较,然后使用这些比较来划分等级,这样,如果我们有3个产品变种,我们就让顾客比较变种1和2,然后2和3,最后3和1。使用我的方法,评测时间是原来的三分之一,但是进行评测的雇员抱怨说,他们不能从评测结果得到一致的等级评定。昨天,我的老板想要知道最新的产品评估。另外我还得告诉你,老的产品评估方法就是他提出的。你能帮助我吗?”(a)销售主管是否陷入困境?他的方法能够根据顾客的喜好产生产品变种的有序等级吗?解释你的观点。(b)是否有办法修正销售主管的方法?对于基于逐对比较创建序数度量,你作何评价?(c)对于原来的产品评估方案,每个产品变种的总等级通过计算所有评测题目上的平均值得到,你是否认为这是一种合理的方法?你会采取哪种方法?5.你能想象一种情况,标识号对于预测是有用的吗?6.一位教育心理学家想使用关联分析来分析测试结果。测试包含100个问题,每个问题有4个可能答案。(a)如何将该数据转换成适合关联分析的形式? PDG(b)能得到何种属性类型以及有多少个属性?7.下面哪种量更可能具有时间自相关性:日降水量,日气温?为什么?
8.讨论:为什么文档-词矩阵是具有非对称的离散特征或非对称的连续特征的数据集的例子?9.许多科学领域依赖于观测而不是(或不仅是)设计的实验,比较涉及观测科学与实验科学和数据挖掘的数据质量问题。10.讨论测量精度与术语单精度和双精度之间的差别。单精度和双精度用在计算机科学,通常分别表示32位和64位浮点数。11.对于处理存放在文本文件而不是二进制格式中的数据,给出至少两个优点。12.区别噪声和离群点。确保考虑以下问题。(a)噪声曾令人感兴趣或使人期望吗?离群点呢?(b)噪声对象可能是离群点吗?(c)噪声对象总是离群点吗?(d)离群点总是噪声对象吗?(e)噪声能将典型值变成例外值吗?反之呢?13.考虑发现数据对象的K个最近邻问题。某个程序员为该任务设计了算法2.2算法2.2发现K个最近邻的算法1:fori=1到数据对象个数do2:找出第i个对象到其他所有对象的距离。3:按递减序对这些距离排序。(维持对象与距离的关联。)4: return与排序表中前K个距离相关联的对象。 5: end for(a)如果数据集中存在重复对象,讨论该算法可能存在的问题。假定对于相同的对象,距离函数只返回距离0。(b)如何解决该问题?14.对亚洲象群的成员测量如下属性:重量、高度、象牙长度、象鼻长度和耳朵面积。基于这些测量,可以使用2.4节的哪种相似性度量来对这些大象进行比较或分组?论证你的答案并说明特殊情况。15.给定m个对象的集合,这些对象划分成K组,其中第组的大小为m如果目标是得到容量为n<m的样本,下面两种抽样方案有什么区别?(假定使用有放回抽样。)(a)从每组随机地选择nmm个元素。(b)从数据集中随机地选择n个元素,而不管对象属于哪个组。16.考虑一个文档-词矩阵,其中是第i个词(术语)出现在第j个文档中的频率,而m是文档数。考虑由下式定义的变量变换:=.logm(2-18) df其中,是出现第i个词的文档数,称作词的文档频率(document frequency)该变换称作逆文档频率(inverse document frequency)变换。(a)如果词出现在一个文档中,该变换的结果是什么?如果术语出现在每个文档中呢?b)该变换的目的可能是什么?
17.假定我们对比率属性x使用平方根变换,得到一个新属性x。作为分析的一部分,你识别出区间(a,b),在该区间内,x与另一个属性y具有线性关系。(a)换算成x,(a,b)的对应区间是什么?(b)给出y关联x的方程。18.本习题比较和对比某些相似性和距离度量。(a)对于二元数据,L1距离对应于汉明距离,即两个二元向量不同的二进位数。Jaccard相似度是两个二元向量之间相似性的度量计算如下两个二元向量之间的汉明距离和 Jaccard相似度x=0101010001y=0100011000(b)Jaccard相似度与汉明距离哪种方法更类似于简单匹配系数,哪种方法更类似于余弦度量?解释你的结论。(注意:汉明度量是距离,而其他三种度量是相似性,但是不要被这一点所迷惑。)(c)假定你正在根据包含共同基因的个数比较两个不同物种的有机体的相似性。你认为哪种度量更适合用来比较构成两个有机体的遗传基因,是汉明还是 Jaccard?解释你的结论。(假定每种动物用一个二元向量表示,其中如果一个基因出现在有机体中,则对应的属性取值1,否则取值0)(d)如果你想比较构成相同物种的两个有机体的遗传基因(例如,两个人),你会使用汉明距离, Jaccard系数,还是一种不同的相似性或距离度量?解释原因。(注意,两个人的相同基因超过99.9%。)19.对于下面的向量x和y,计算指定的相似性或距离度量。(a)x=(1,1,1,1),y=(2,2,2,2)余弦、相关、欧几里得(b)x=(0,1,0,1),y=(1,0,1,0)余弦、相关、欧几里得、 Jaccard(c)x=(0,-1,0,1),y=(1,0,-1,0)余弦、相关、欧几里得。(d)x=(1,1,0,1,0,1),y=(1,1,1,0,0,1)余弦、相关、Jaccard(e)x=(2,-1,0,2,0,-3),y=(-1,1,-1,0,0,-1)余弦、相关20.这里,进一步考察余弦度量和相关性度量。(a)对于余弦度量,可能的值域是什么?(b)如果两个对象的余弦度量为1,它们相等吗?解释原因。c)如果余弦度量与相关性度量有关系的话,有何关系?(提示:在余弦和相关性相同或不同情况下,考虑诸如均值、标准差等统计量。)(d)图2-20a显示100000个随机生成的点的余弦度量与欧几里得距离之间的关系,这些点已经规范化,L2长度为1。当向量的L2长度为1时,关于欧几里得距离与余弦相似性之间的关系,你能得出什么样的一般观测结论?(e)图2-20b显示100000个随机生成的点的相关性度量与欧几里得距离之间的关系,这些点已经标准化,具有均值0和标准差1当向量已经标准化,具有均值0和标准差1时,关于欧几里得距离与相关性之间的关系,你能得出什么样的一般观测⊙结论?()当每个数据对象的L2长度为1时,推导余弦相似度与欧几里得距离之间的数学关
系。(g)当每个数据点通过减去均值并除以其标准差标准化时,推导相似度与欧几里得距离之间的数学关系。1.41.41.21.211080.80.6〓0.60.40.40.20.20.20.40.60.80.20.40.60.8余弦相似性相关性(a)欧几里得距离与余弦度量之间的关系(b)欧几里得距离与相关性之间的关系图2-20习题20的图形21.证明下式给出的集合差度量满足2.4.3节的度量公理:(2-19)其中,A和B是集合,A-B是集合差。22.讨论如何将相关值从区间[-1,1]映射到区间[0,1]。注意,你所使用的变换类型可能取决于你的应用。因此,考虑两种应用:对时间序列聚类,给定一个时间序列预测另一个的性质。23.给定一个在区间[0,1]取值的相似性度量,描述两种将该相似度变换成区间[0,∞中的相异度的方法。24.通常,邻近度定义在一对对象之间。(a)阐述两种定义一组对象之间邻近度的方法。(b)如何定义欧几里得空间中两个点集之间的距离?(c)如何定义两个数据对象集之间的邻近度?(除邻近度定义在任意一对对象之间外,对数据对象不做任何假定。)25.给定欧几里得空间中一个点集S,以及S中每个点到点x的距离。(x是否属于S并不重要。)(a)如果目标是发现点y(y≠x)指定距离内的所有点,解释如何利用三角不等式和已经计算的到x的距离,来减少必需的距离计算数量。提示:三角不等式d(x,z)≤d(x,y)+d(y,x)可以写成d(x,y)≥d(x,z)-d(y,z)(b)x和y之间的距离对距离计算的数量有何影响?c)假定你可以从原来的数据点的集合中发现一个较小的子集S,使得数据集中的每个点都至少到S中一个点的距离不超过指定的E,并且你还得到了中每对点之间的距离矩阵。描述一种技术,使用这些信息以最少的距离计算量,从数据集中计算到一个指定点距离不超过B的所有点的集合。
26.证明1减 Jaccard相似度是两个数据对象x和y之间的一种距离度量,该度量满足2.4.3节的度量公理。具体地,d(x,y)=1-J(x,y)27.证明定义为两个数据向量x和y之间夹角的距离度量满足2.4.3节的度量公理具体地, d(x, y)= arccos(cos(x, y)).28.解释为什么计算两个属性之间的邻近度通常比计算两个对象之间的相似度简单。
第章探索数据第2章讨论了知识发现过程中重要的高层数据问题。本章是数据探索导论,对数据进行初步研究,以便更好地理解它的特殊性质。数据探索有助于选择合适的数据预处理和数据分析技术。它甚至可以处理一些通常由数据挖掘解决的问题,例如,有时可以通过对数据进行直观检查来发现模式。此外,数据探索中使用的某些技术(如可视化)可以用于理解和解释数据挖掘结果。本章包括三个主题:汇总统计、可视化和联机分析处理(OLAP)汇总统计(如值集合的均值和标准差)和可视化技术(如直方图和散布图)是广泛用于数据探索的标准方法。OLAP是一种新近开发的包含一系列考察多维数组数据的技术。OLAP的分析功能集中在从多维数据数组中创建汇总表的各种方法。OLAP技术包括在不同的维上或不同的属性值上聚集数据,例如,如果给定基于产品、位置和日期记录的销售信息,则可以使用OLAP技术创建按月和按产品类别描述特定地点的销售活动汇总。本章涵盖的主题与探测性数据分析(Exploratory Data Analysis,eda有许多重叠,eDA是卓越的统计学家 John Tukey于20世纪70年代创建的。像EDA一样,本章特别强调可视化,而与EDA不同的是,本章并不包含诸如聚类分析和异常检测等主题,其原因有二:首先,数据挖掘将描述性数据分析技术本身看作目的,而统计学(EDA由此发源)趋向于将基于假设的检验作为最终目标;其次,聚类分析和异常检测都是很大的领域,需要用整章进行深入讨论。因此,聚类分析将在第8章和第9章给出,而异常检测则在第10章讨论。3.1鸢尾花数据集在下面的讨论中,我们经常提到鸢尾花(Iis)数据集,该数据集可以从加州大学欧文分校(UCI)的机器学习库中得到。鸢尾花数据集包含150种鸢尾花的信息,每50种取自三个鸢尾花种之一:Setosa、 Versicolour和 Virginica。每个花的特征用下面5种属性描述。(1)萼片长度(厘米)(2)萼片宽度(厘米)(3)花瓣长度(厘米)(4)花瓣宽度(厘米) (5)(Setosa, Versicolour, Virginica )花的萼片是花的外部结构,保护花的更脆弱的部分(如花瓣)。在许多花中,萼片是绿的,只有花瓣是鲜艳多彩的,然而,对于鸢尾花,尊片也是鲜艳多彩的。图3-1给出了一种 Virginica鸢尾花的图片,鸢尾花的萼片比花瓣大并且下垂,而花瓣向上。
图3-1鸢 Virginica尾花的图片. Robert. Mohlenbrock@usda -NRCS- PLANTS Database/usdaNRC.1995.东北湿地植物志:野外办公室植物物种指南。东北国家技术中心,切斯特,宾夕法尼亚州(删除了背景)3.2汇总统计汇总统计(summary statistics)是量化的(如均值和标准差),用单个数或数的小集合捕获可能很大的值集的各种特征。汇总统计的日常例子有家庭平均收入、四年内完成本科学位的学生比例。的确,对于许多人,汇总统计是最常见的统计形式。我们将集中讨论对单个属性值的汇总统计,但是也将简略介绍某些多变元汇总统计。本节只考虑汇总统计的描述性质。然而,统计学将数据视为源于被各种参数刻画的基本统计过程,而这里讨论的某些汇总统计可以看作是产生数据的基本分布的统计参数的估计。3.2.1频率和众数给定一个无序的、分类的值的集合,为了进一步刻画值的性质,除计算特定数据集中每个值出现的频率外没有多少的事情可做。给定一个在v1,,i,v}上取值的分类属性x和m个对象的集合,值v的频率定义为: frequency()具有属性值v的对象数(3-1)m分类属性的众数(mode)是具有最高频率的值。例3.1考虑学生的集合。学生具有一个属性年级,可以从集合{一年级,二年级,三年级,四年级}中取值。表3-1显示年级属性每个值的学生人数,年级属性的众数是大学一年级,其频率为0.33,这或许暗示因退学导致的减员或扩招。
表3-1一所假想大学中各年级学生人数年级人数频率一年级2000.33二年级1600.27三年级1300.22四年级1100.18口分类属性常常(但并非总是)具有少量值,因此这些值的众数和频率可能是令人感兴趣的和有用的。注意,尽管如此,对于鸢尾花数据集和类属性,由于三种类型的花具有相同的频率,因而众数的概念并无意义。对于连续数据,按照目前的定义,众数通常没有用,因为单个值的出现可能不超过一次,然而,在某些情况下,众数可能提供关于值的性质或关于出现遗漏值的重要信息。例如,以毫米为单位测量,20个人的身高通常不会重复,但是如果以分米为单位测量,则某些人可能具有相同的身高。此外,如果使用唯一的值表示遗漏值,则该值常常表现为众数。3.2.2百分位数对于有序数据,考虑值集的百分位数(percentile)更有意义。具体地说,给定一个有序的或连续的属性x和0与100之间的数p,第p个百分位数xp是一个x值,使得x的p%的观测值小于xp。例如,第50个百分位数是值x50%,使得的所有值的50%小于x50%表3-2显示鸢尾花数据集的四个定量属性的百分位数。表3-2萼片长度、萼片宽度、花瓣长度和花瓣宽度的百分位数(所有的值都以厘米为单位)百分位数片长度萼片宽度花瓣长度花瓣宽度04.32.01.00.1104.82.51.40.2205.02.71.50.2305.22.81.70.4405.63.03.91.2505.83.04.41.3606.13.14.61.5706.33.25.01.8806.63.45.41.9906.93.65.82.21007.94.46.92.5例3.2从1到10的整数的百分位数x0%,X10%90%,10依次为:1.0,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.0按照惯例,min(x)=x0%,而max(x)=x100%口3.2.3位置度量:均值和中位数对于连续数据,两个使用最广泛的汇总统计是均值(mean)和中位数(median),它们是值具体的例子中,这些值是m个儿童的身高),设(xa)xm代表非递减序排序后的x值,这样,PDG集位置的度量。考虑m个对象的集合和属性x,设{x1,,xm}是这m个对象的x属性值(在这个x(1)=min(x),而xm)=max(x),于是均值和中位数定义如下:
(3-2)mx(r+1)如果m是奇数,即m=2r+1 median(x)={(3-3)2(x+x)如果m偶数,即m=2r概括地说,如果有奇数个值,则中位数是中间值;如果有偶数个值,则中位数是中间两个值的平均值。这样,对于7个值,中位数是x(4,而对于10个值,中位数是(x1+x)尽管有时将均值解释为值集的中间,但是仅当值以对称方式分布时,才是对的。如果值的分布是倾斜的,则中位数是中间的一个更好的指示符。此外,均值对于离群值很敏感;对于包含离群值的数据,中位数可以再次更稳健地提供值集中间的估计。为了克服传统均值定义的问题,有时使用截断均值(trimmed mean)概念。指定0和100之间的百分位数p,丢弃高端和低端p2)%的数据,然后用常规的方法计算均值,所得的结果即是截断均值。中位数是p=100%时的截断均值而标准均值是对应于p=0%的截断均值。例3.3考虑值集{1,2,3,4,5,90}。这些值的均值是17.5,而中位数是3.5,p=40%时的截断均值也是3.5口例3.4鸢尾花数据集的四个定量属性的均值、中位数和截断均值(p=20%)在表3-3给出。除属性花瓣长度外,其余三个属性的三个位置度量具有相似的值。口表3-3萼片长度、萼片宽度、花瓣长度和花瓣宽度的均值、中位数和截断均值所有值都以厘米为单位)度量尊片长度尊片宽度花瓣长度花瓣宽度均值5.843.053.761.20中位数5803.004.351.30截断均值(20%)5.793.023.721.123.2.4散布度量:极差和方差连续数据的另一组常用的汇总统计是值集的弥散或散布度量这种度量表明属性值是否散布很宽,或者是否相对集中在单个点(如均值)附近。最简单的散布度量是极差(range)。给定属性x,它具有m个值{x1,xm},x的极差定义为: range(x)= max(x)-min(x) =(m)-X()(3-4)尽管极差标识最大散布,但是如果大部分值都集中在一个较窄的范围内,并且更极端的值的个数相对较少,则可能会引起误解。因此,作为散布的度量,方差( variance)更可取。通常,属性x的(观测)值的方差记作s2,并在下定义。标准差(standard deviation)是方差的平方根,记作x,它与x具有相同的单位。1 variance(x)=s2=-1-x)2∑均值可能被离群值扭曲,并且由于方差用均值计算,因此它也对离群值敏感。确实,方差对PDG(3-5)离群值特别敏感,因为它使用均值与其他值的差的平方。这样常常需要使用比值集散布更稳健的
估计。下面是三种这样的度量的定义:绝对平均偏差(absolute average deviation,aad)中位数绝对偏差( median absolute deviation,MAD)和四分位数极差(interquartile range,IR)表3-4显示鸢尾花数据集的这些度量。 AAD()=(3-6) MAD(x) =median((x1-,,.xm-x))(3-7) interquartile range() =X75%-X25%(3-8)表3-4萼片长度、萼片宽度、花瓣长度和花瓣宽度的极差、标准差(std)、绝对平均偏差(AAD)、中位绝对偏差(MAD)和中间四分位数极差(QR)(所有值都以厘米为单位)度量萼片长度萼片宽度花瓣长度花瓣宽度极差3.62.45.92.4 std0.80.41.80.8 AAD0.70.31.60.6 MAD0.70.31.20.7 IQR1.30.53.5153.2.5多元汇总统计包含多个属性的数据(多元数据)的位置度量可以通过分别计算每个属性的均值或中位数得到。这样,给定一个数据集,数据对象的均值由x=(x1…,x)(3-9)给出,其中x是第i个属性x的均值。对于多元数据,每个属性的散布可以独立于其他属性,使用3.2.4节介绍的方法计算。然而,对于具有连续变量的数据,数据的散布更多地用协方差矩阵( covariance matrix)S表示,其中,S的第个元素s是数据的第i个和第j个属性的协方差。这样,如果x和x分别是第i个和第j个属性,则=covariance(,)(3-10)而 covariance(xi,x)由 covariance(,)(x8-x)(x-x) m-1k=(3-11)给出,其中x和xy分别是第k个对象的第i和第j个属性的值。注意, covariance(xix)= variancex)这样,协方差矩阵的对角线上是属性的方差。两个属性的协方差是两个属性一起变化并依赖于变量大小的度量协方差的值接近于0表明两个变量不具有(线性)关系,但是不能仅靠观察协方差的值来确定两个变量之间的关联程度。因为两个属性的相关性直接指出两个属性(线性)相关的程度,对于数据探索,相关性比协方差更可取。(另见2.4.5节关于相关的讨论。)相关矩阵( correlation matrix)R的第个元素是数据的第i个和第j个属性之间的相关性。如果x1和分别是第i个和第j个属性,则 PDG
 ry =correlation covariance(, x(3-12)其中,s和s分别是x和x的方差。R的对角线上的元素是 correlation,x)=1,而其他元素在-1和1之间。考虑包含每对对象而不是每对属性之间相关性的相关矩阵也是有用的。3.2.6汇总数据的其他方法当然,还有其他类型的汇总统计,例如,值集的倾斜度(skewness)度量值对称地分布在均值附近的程度。另外还有一些其他数据特征,很难定量地度量,例如,值的分布是否是多模态的(multimodal),即数据具有多个“肿块”,大部分值集中在那里。然而,在许多情况下,理解关于属性值如何分布的更复杂、更微妙的方面,最有效的方法是通过直方图观察这些值。(直方图在下一节讨论。)3.3可视化数据可视化是指以图形或表格的形式显示信息成功的可视化需要将数据(信息)转换成可视的形式,以便能够借此分析或报告数据的特征和数据项或属性之间的关系。可视化的目标是形成可视化信息的人工解释和信息的意境模型。在日常生活中,可视化技术(如图和表等)常常是优先选择的方法,用来解释气象、经济和政治选举的结果。同样,尽管在大多数技术学科(包括数据挖掘)中通常强调算法或数学方法,但是可视化技术也能在数据分析方面起关键性作用。事实上,有时将可视化技术在数据挖掘方面的应用称作可视化数据挖掘( visual data mining)3.3.1可视化的动机使用可视化技术的首要动机是人们能够快速吸取大量可视化信息,并发现其中的模式。考虑图3-2,它以摄氏度为单位显示1982年7月的海洋表面温度(SST)这张图汇总大约250000个数据,并且一目了然。例如,在这张图上可以很容易地看出,海洋温度在赤道最高,而在两极最低。9306253020151090180-150-120-90-60-300306090120150180温度经度 PDG图3-21982年7月的海洋表面温度(SST)
可视化的另一个动机是利用“锁在人脑袋中”的领域知识。尽管使用领域知识是数据挖掘的一项重要任务,但是在统计学或算法工具中通常无法充分地利用这种知识,或者是不可能利用的。在某些情况下,可以使用非可视化工具进行分析,然后用可视化的方式提供结果,由领域专家进行评估。在其他情况下,让领域专家检查可视化数据可能是发现有意义的模式的最佳方法,因为利用领域知识,通常可以快速排除许多无意义的模式,并且直接聚焦到重要的模式上。3.3.2般概念本节考察一些与可视化有关的一般概念,特别是考察将数据和它的属性可视化的一般方法。本节将简略提及一些可视化技术,并在其后讨论具体的方法时更详细地介绍。我们假定读者熟悉线图、条形图和散布图。1.表示:将数据映射到图形元素可视化的第一步是将信息映射成可视形式,即将信息中的对象、属性和联系映射成可视的对象、属性和联系。也就是说,数据对象、它们的属性,以及数据对象之间的联系要转换成诸如点、线、形状和颜色等图形元素。对象通常用三种方法表示。首先,如果只考虑对象的单个分类属性,则通常根据该属性的值将对象聚成类,并且把这些类作为表的项或屏幕的区域显示。(本章后面给出的例子是交叉表和条形统计图表。)其次,如果对象具有多个属性,则可以将对象显示为表的一行(或列),或显示为图的一条线。最后,对象常常解释为二维或三维空间中的点,其中点可能用几何图形表示,如圆圈、十字叉或方框。对于属性,其表示取决于属性的类型,即取决于属性是标称的、序数的还是连续的(区间的或比率的)。序数的和连续的属性可以映射成连续的、有序的图形特征,如在x,y或乙轴上的位置,亮度,颜色,或尺寸(直径、宽度或高度等)。对于分类属性,每个类别可以映射到不同的位置、颜色、形状、方位、修饰物或表的列。然而,对于标称属性,由于它的值是无序的,因此在使用具有与其值相关的固有序的图形特征(如颜色、位置等)时,就需要特别小心。换言之,用来表示序数值的图形元素通常有序,但是标称值没有序。通过图形元素表示的关系或者是显式的,或者是隐式的。对于图形数据,通常使用标准的图形表示点和点间的连线。如果点(数据对象)或连线(关系)具有自己的属性或特性,则这些属性也可以图示。例如,如果点是城市,连线是公路,则点的直径可以表示人口,而连线的宽度可以表示交通流量。通常将对象和属性映射到图形元素,隐含地将数据中的联系映射到图形对象之间的联系。例如,如果数据对象代表具有位置的物理对象(如城市),则对应于数据对象的图形对象的相对位置趋向于自然地保持对象的实际相对位置。同样,如果两个或三个连续属性取作点的坐标值,则结果图通常呈现属性和数据点的联系,因为看上去靠近的数据点具有相似的属性值。一般地,很难确保将对象和属性的映射表示成图形元素之间易于观察的联系。的确,这是可视化的最主要难点之一。在任意给定的数据集中,有许多蕴涵的联系,因此可视化的主要难点是选择一种技术,让关注的联系易于观察。2.安排如前所述,对于好的可视化来说,正确选择对象和属性的可视化表示是基本的要求。在可视化显示中,项的安排也是至关重要的。我们用两个例子解释这一点。
例3.5本例解释重新安排表中数据的重要性。在表3-5中,显示具有6个二元属性的9个数据对象,对象和属性之间没有明显的联系,至少乍一看如此。然而,重新排列该表的行和列后,如表36所示,则可以清楚地看出表中只有两类对象一类的前三个属性取1,而另一类的后三个属性取1。口表3-5具有6个二元属性(列)的表3-6具有6个二元属性(列)的9个对象(行)9个对象(行)的表的表,排列后使得行和列的联系明朗123456613254101011041110002101001211100030101106111000410100181110005010110500011161010013000111701011090001118101001100011190101107000111例3.6考虑图3-3a,该图显示一个图的可视化。如果将连通子图分开,如图3-3b所示,则结点和图之间的联系就变得更加简单易懂。口(a)图的原视图(b)图的连通子图分开后的视图图3-3图的两种可视化3.选择可视化的另一个关键概念是选择( selection),即删除或不突出某些对象和属性。具体说来,尽管只具有少数维的数据对象通常可以使用直截了当的方法映射成二维或三维图形表示,但是还没有令人完全满意和一般的方法表示具有许多属性的数据。同样,如果有很多数据对象,则可视化所有对象可能导致显示过于拥挤。如果有许多属性和许多对象,则情况会更加复杂。处理很多属性的最常用方法是使用属性子集(通常是两个属性)。如果维度不太高,则可以构造双变量(双属性)图矩阵用于联合观察。(图3-16显示鸢尾花数据集属性对的散布图矩阵。)或者说,可视化程序可以自动地显示一系列二维图,其中次序由用户或根据某种预定义的策略控制,让可视化二维图的集族提供数据的更完全的视图。选择一对(或少数)属性的技术是一类维归约,并且有许多更复杂的维归约可以使用,如主成分分析(PCA)当数据点的个数很多(例如超过数百个)或者数据的极差很大时,充分显示每个对象的信息是困难的,有些数据点可能遮掩其他数据点或者数据对象可能占据不了足够多的像素来清楚地
显示其特征。例如,如果只有一个像素可用于显示,则对象的形状不能用于对象特性编码。在这些情况下,或者通过放大数据的特定区域,或者通过选取数据点样本,能够删除某些对象是有用的。3.3.3技术可视化技术对于分析的数据类型通常是专用性的。的确,新的可视化技术和方法,以及已有方法的变形,正在不断地创建,以应对新的数据类型和可视化任务。尽管可视化具有专门性和特殊性,但是仍然有一般性方法可对可视化技术进行分类。一种分类是基于所涉及的属性个数(1、2、3或多),或者基于数据是否具有某种特殊的性质(如层次结构或图结构)。可视化方法也可以根据所涉及的属性类型分类。另一种分类是根据应用类型:科学的、统计学的或信息学的可视化。下面的讨论将使用三种类型:少量属性的可视化,具有时间和/或空间属性的数据可视化,以及具有大量属性的数据可视化。这里讨论的大部分可视化技术都可以在一些数学和统计学软件包中找到,其中一些是免费的。万维网上还有大量数据集免费提供。建议读者在阅读下面各段时,试试这些可视化技术。1.少量属性的可视化本段考察用于具有少量属性的可视化数据的技术。有些技术(如直方图)可以显示单个属性观测值分布,其他技术(如散布图)旨在显示两个属性值之间的关系。茎叶图茎叶图(stem and leaf plot)可以用来观测一维整型或连续数据的分布。(开始,我们假定数据是整型的,然后解释如何将茎叶图用于连续数据。)对于最简单的一类茎叶图,将值分组,其中每组包含的值除最后一位数字外相同。每个组成为茎,而组中的最后一位数字成为叶。因此,如果值是两位整数(例如,35、36、42和51),则茎是高位数字(例如,3、4和5),而叶是低位数字(如1、2、5和6)。通过垂直绘制茎,水平绘制叶,可以提供数据分布的可视表示。例3.7图3-4给出的数据取自鸢尾花数据集是以厘米为单位的萼片长度(乘以10,取整数值)。为方便起见,值已经排序。这个数据的茎叶图显示在图3-5中。图3-4中的每个数先根据它的十位数字放到一个垂直的组4,5,6或7中,然后将它的最后一位数字放到冒号右边。通常,特别是当数据量很大时,需要将茎分裂。例如,不是将十位数字为4的所有值都放在一个“桶”中,而是将茎4重复两次:40-44的所有值放在对应于第一个茎的桶中而45~49的所有值放在对应于第二个茎的桶中。这种方法显示在图3-6的茎叶图中。其他变形也是可能的。口4344444445464646464747484848484849494949494950505050505050505050515151515151515151525252525354545454545455555555555555656565656565757575757575757585858585858585959596060606060606161616161616262626263636363636363636364646464646464656565656566667676767676767676868686969696970717272727374767777777779图3-4鸢尾花数据集中的片长度数据
4:368899995:00001111245556677788896:00112334455577788997:0122236777图3-5鸢尾花数据集中的片长度的茎叶图4:34444:566667788888999995:0000001111224445:6677788896:00112334446:5677778897:01222347:677779图3-6鸢尾花数据集中对应于数字的桶分裂后的片长度的茎叶图直方图茎叶图是一种类型的直方图(histogram)该图通过将可能的值分散到箱中,并显示落入每个箱中的对象数,显示属性值的分布。对于分类属性,每个值在一个箱中。如果值过多,则使用某种方法将值合并。对于连续属性,将值域划分成箱(通常是等宽的,但不必是等宽的)并对每个箱中的值计数。一旦有了每个箱的计数,就可以构造条形图(bar plot),即每个箱用一个条形表示,并且每个条形的面积正比于落在对应区间的值(对象)的个数。如果所有的区间都是等宽的,则所有的条形的宽度相同,并且条形的高度正比于落在对应箱中值的个数。例3.8图3-7显示片长度、萼片宽度、花瓣长度和花瓣宽度的直方图(10个箱)由于直方图的形状依赖于箱的个数,同一数据但具有20个箱的直方图显示在图3-8中。2053本30本(a)萼片长度(b)萼片宽度(c)花瓣长度(d)花瓣宽度图3-7四个鸢尾花属性的直方图(10个箱)期20新15新2010(a)萼片长度(b)萼片宽度(c)花瓣长度(d)花瓣宽度图3-8四个鸢尾花属性的直方图(20个箱)直方图有一些变形。相对频率直方图(relative frequency histogram)用相对频率取代计数,
然而,这只是一种y轴尺度的变化,直方图的形状并不改变。另一种常见的变形是 Pareto直方图(Pareto histogram),它专门针对无序的分类数据, Pareto直方图与普通直方图一样,只是分类按计数排序,让计数从左到右递减。二维直方图二维直方图(two--dimensional histogram)也是一种类型的直方图。它将每个属性划分成区间,而两个区间集定义值的二维长方体。例3.9图3-9显示花瓣长度和花瓣宽度的二维直方图。由于每个属性划分成3个箱,因此有9个矩形二维箱。每个长方体条的高度指示落入箱中的对象(在此情况下是花)的个数。大部分花落入3个沿着对角线的箱中。查看一维分布不可能看到这种情况。口本花瓣宽度花瓣长度图3-9鸢尾花数据集花瓣长度和花瓣宽度的二维直方图尽管二维直方图可以用来观察关于两个属性的值如何同时出现的有趣问题,但是观察它们比较困难。例如,不难想象这样一种情况,某些柱体被其他柱体遮掩。盒状图盒状图(box plot)是另一种显示一维数值属性值分布的方法。图3-10显示片长度的加标记的盒状图。盒的下端和上端分别指示第25和第75个百分位数,而盒中的线指示第50个百分位数的值,底部和顶部的尾线分别指示第10和第90个百分位数,离群值用“+”显示。盒状图相对紧凑,因此可以将许多盒状图放在一个图中。还可以使用占据较少空间的盒状图的简化版本。例3.10鸢尾花数据集前4个属性的盒状图显示在图3-11中也可以使用盒状图来比较不同对象类之间的属性如何变化,如图3-12所示。离群值←第90个百分位数←第75个百分位数第50个百分位数←第25个百分位数宁←第10个百分位数片长度片宽度花瓣长度,花瓣宽度图3-10尊片长度盒状图描述图3-11鸢尾花属性的盒状图
宁宁片长度尊片宽度花瓣长度花瓣宽度片长度片宽度花瓣长度花瓣宽度片长度片宽度花瓣长度花瓣宽度 (a)Setosa (b)Versicolour (c)Virginica图3-12鸢尾花种类的盒状图饼图饼图(pie chart)类似于直方图,但通常用于具有相对较少的值的分类属性。饼图使用圆的相对面积显示不同值的相对频率,而不是像直方图那样使用条形的面积或高度。尽管饼图在通俗文章中很常见,但是它们在技术性出版物中并不常用,因为相对面积的大小很难确定。在技术方面,直方图更可取。例3.11图3-13给出了一个饼图,显示鸢尾花数据集的鸢尾花种类的分布。在该例中,三种类型的花都具有相同的频率。口 Setosa Virginica Versicolour图3-13鸢尾花的类型分布百分位数图和经验累计分布函数一种更定量地显示数据分布的图是经验累计分布函数图。尽管这种类型的图听上去可能很复杂,但是概念相当简单。对于统计分布的每个值,一个累计分布函数(cumulative distribution function,CDF)显示点小于该值的概率。对于每个观测值,一个经验累计分布函数(empirical cumulative distribution function,ECDF)显示小于该值的点的百分比。由于点的个数是有限的,经验累计分布函数是一个阶梯函数。例3.12图3-14显示了鸢尾花属性的ECDF.属性的百分位数提供了类似的信息,图3-15显示了表3-2中鸢尾花数据集的4个连续属性百分位数图(percentile plot)。读者应当将这些图与图3-7和图3-8的直方图进行比较。散布图大部分人都在某种程度上熟悉散布图,本书也在.4.5节使用过散布图来解释线性相关。散布图使用数据对象两个属性的值作为x和y坐标值,每个数据对象都作为平面上的一个点绘制(假定属性值是整数或实数)
0.90.8。。0.79870.60.60.50500030.24321020.10.1(a)萼片长度b)萼片宽度0.9080.8070706060.55004302020.10.10525(c)花瓣长度(d)花瓣宽度图3-144个鸢尾花属性的经验CDF尊片长度尊片宽度化长度宽度3百分位数图3-15尊片长度、尊片宽度、花瓣长度和花瓣宽度的百分位数图例3.13图3-16显示了鸢尾花数据集的每对属性的散布图。不同的鸢尾花种类使用不同的标记表示。属性对的散布图安排在一种称作散布图矩阵(scatter plot matrix)的表格形式中,提供了一种有组织的方式,以同时考察许多散布图。口散布图有两个主要用途。其一,它们图形化地显示两个属性之间的关系。在2.4.5节,我们看到如何使用散布图判定线性相关程度(见图2-17)。直接使用散布图,或者使用变换后属性的散布图,也可以判定非线性关系。其二,当类标号给出时,可以使用散布图考察两个属性将类分开的程度。如果可以画一条直线(或一条更复杂的曲线)将两个属性定义的平面分成区域,每个区域包含一个类的大部分对象,则可能基于这对指定的属性构造精确的分类器;否则的话,就需要更多的属性或更复杂的方法建立分类器。在图3-16中,许多属性对(如,花瓣宽度和花瓣长度)都提供了适度的鸢尾花种类分隔。
赵蒙士输∞38己业+尊片长度尊片宽度花瓣长度花瓣宽度图3-16鸢尾花数据集的散布图矩阵例3.14使用散布图显示数据集的三个属性有两种不同的方法。第一种,根据三个,而不是两个属性的值来显示每个对象。图3-1显示了鸢尾花数据集的三个属性的三维散布图。第二种,将其中一个属性与标记的某种特性(如大小、颜色或形状)相关联。图3-18显示了鸢尾花数据集的三个属性的散布图,其中属性萼片宽度映射到标记的大小。 +SetosaVersicolour1业都0.55萼片宽度32432765花瓣宽度图3-17萼片宽度、萼片长度和花瓣宽度的三维散布图
2.5 .Setosa Vorsicolour▲ Virginica1.50.53456花瓣长度图3-18花瓣长度对花瓣宽度的散布图,标记的大小指萼片宽度扩展的二维和三维图如图3-18所示,可以扩展成二维或三维图,以便提供一些附加的属性。例如,使用颜色或阴影、大小、形状,散布图可以显示三个附加信息,可以表达五个或六个维。然而,需要小心,随着数据可视表达的复杂性增加,对于解释信息的人就变得更加困难。将六个维的信息放进二维或三维图中没有多少好处,如果做的话也不可能理解。2.可视化时间空间数据数据常常有空间或时间属性,例如,数据可能是在某空间栅格上的观测值的集合,如地球表面上的压力,或物体模拟在各个栅格点上的模拟温度,这些观测值也可以在不同的时间点得到。此外,数据也可能只有一个时间分量,如反映每日股票价格的时间序列数据。等高线图对于某些三维数据,两个属性指定平面上的位置,而第三个属性具有连续值,如温度或海拔高度。对于这样的数据,一种有用的可视化工具是等高线图(contour plot)等高线图将平面划分成一些区域,区域中的第三个属性(温度或海拔高度)的值粗略地相等。等高线图的常见例子是显示地面位置海拔高度的等高线图。例3.15图3-19显示1998年12月份平均海洋表面温度(SST)的等高线图,地面温度被随意地设定为0℃。在许多等高线地图(如图3-19中的等高线图)中,将两个区域分开的等高线(contour line)用分开区域的值标记。为简明起见,删除了其中一些标记。口曲面图与等高线图一样,曲面图(《surfcepIot《)使用两个属性表示x和y坐标,曲面图的第三个属性用来指示高出前两个属性定义的平面的高度。尽管这种图可能是有用的,但是这要求至少某个范围内,对于前两个属性值的所有组合,第三个属性的值都有定义。此外,如果曲面不太规则,除非交互地观察,否则很难看到所有信息。因而,曲面图通常用来描述数学函数,或变化相对光滑的物理曲面。例3.16图3-20显示12个点的集合周围密度的曲面图。这个例子将在9.3.3节进一步讨论。口矢量场图在某些数据中,一个特性可能同时具有值和方向。例如,考虑物质流或随位置改变的密度。在这些情况下,同时显示方向和量的图可能是有用的。这种类型的图称作矢量图(vector plot).
图3-191998年12月份SST的等高线图◆◆◆◆◆(a)12个点的集合(b)总密度函数—曲面图图3-2012个点的集合的密度例3.17图3-21显示图3-20b中两个较小密度尖峰的密度等高线图,并附以密度梯度向量。图3-21图3-20中下端两个密度尖峰的密度梯度(变化)矢量图 PDG
低维切片考虑时间空间数据集,它记录不同地点和时间上的某种量,如温度或气压。这样的数据有四个维,不容易用迄今所介绍的图来显示。然而,通过显示一组图,每月一个,可以显示数据的各个“切片”。通过考察特定区域的逐月改变,就可能注意到所出现的变化,包括可能因为季节原因而导致的变化。例3.18该例的基本数据集是从1982年到1999年、在2.5°乘2.5°的经纬度网格上的月平均海平面气压(SLP)一年12个月的气压图显示在图3-22中,在这个例子中,我们对1982年特定月份的切片感兴趣。更一般地,我们可以考虑沿任意维的数据切片。口一月二月三月四月五月六月七月八月九月十月十一月十二月图3-221982年海平面气压月报图动画无论是否涉及时间,处理数据切片的另一种方法是使用动画,其基本思想是显示数据的相继二维切片。人的视觉系统很适合检测视觉变化,并且常常能够注意到可能很难用其他方式检测到的变化。尽管动画具有视觉吸引力但是一组静止的图(如图3-22中的那些)可能更有用,因为这种类型的可视化使得我们可以按任意次序、使用任意多时间来研究这些信息。3.3.4可视化高维数据本节介绍可以显示更多维的可视化技术,使用这些技术所能观察的维数比使用刚刚讨论过的技术观察的更多。然而,即便这些技术也有一些局限性:它们只能显示数据的某些侧面。矩阵图像可以看作像素的矩形阵列,其中每个像素用它的颜色和亮度刻画,数据矩阵是值
的矩形阵列,那么,将数据矩阵的每个元素与图像中的一个像素相关联,就可以把数据矩阵看作图像,像素的亮度和颜色由矩阵对应元素的值决定。在对数据矩阵可视化时,有一些重要的实用性考虑:如果类标号已知,则重新排列数据矩阵的次序,使得某个类的所有对象聚在一起这是很有用的方法,例如,这可以很容易地检查某个类的所有对象是否在某些属性上具有相似的属性值:如果不同的属性具有不同的值域,则可以对属性标准化,使其均值为0,标准差为1,这防止具有最大量值的属性在视觉上左右图形。例3.19图3-23显示鸢尾花数据集的标准化数据矩阵,前50行代表 Setosa种类的鸢尾花,接下来的50行代表 Versicolour,最后50行代表 Virginica Setosa花的花瓣宽度和长度远低于平均值,而 Versicolour花的花瓣宽度和长度在平均值附近, Virginica花的花瓣宽度和长度高于平均值口寻找数据对象集的邻近矩阵图中的结构也是很有用的。当类标号已知时,最好对相似矩阵的行列排序,以便将某个类的所有对象聚在一起。这样可以目视评估每个类的内聚性,与其他类的分离性。例3.20图3-24显示鸢尾花数据集的相关矩阵,该矩阵的行和列也已经重新组织,使得特定种类的花在一起。每组内的花相互之间最为相似,但是 Versicolour和 Virginica与它们和 Setosa相比更为类似。0.908070.60504萼片长度萼片宽度花瓣长度花瓣宽度标准差 Virginica相关度图3-23鸢尾花数据矩阵图,其中列已经标准图3-24鸢尾花数据集的相关矩阵化,均值为0,标准差为1如果类标号未知,则多种技术(矩阵重新定序和顺序排列)都可以用来重新安排相似矩阵的行和列,以便一组组高度相似的对象和属性放在一起并可以通过视觉识别。实际上,这是一种简单聚类。关于如何使用邻近矩阵考察数据的聚类结构见8.5.3节。平行坐标系平行坐标系(parallel coordinates)每个属性一个坐标轴,但是与传统的坐标系不同,平行坐标系不同的坐标轴是平行的,而不是正交的。此外,对象用线而不是用点表示,具体地说,对象每个属性的值映射到与该属性相关联的坐标轴上的点,然后将这些点连接起来形成代表该对象的线。你可能担心这将产生混乱,然而,在许多情况下,对象趋向于分成少数几组,其中每个组内的点具有类似的属性值,如果这样的话,并且数据对象的数量不太多,则结果平行坐标图可以揭示有意义的模式。
例3.21图3-25显示鸢尾花数据集4个数值属性的平行坐标图。代表不同类的对象的线由其浓淡和类型来区分,这里使用三种不同类型的线实线、点线和虚线。该平行坐标图表明,三个类关于花瓣宽度和花瓣长度分开得相当好,但关于片长度和萼片宽度分开得不太好。图3-26是相同数据的另一个平行坐标图,与前一图相比只是坐标轴的次序不同。8 -Setosa -Virginica Versicolour5420片长度萼片宽度花瓣长度花瓣宽度图3-254个鸢尾花属性的平行坐标图8765 -Setosa -Virginica ..Versicolour320片宽度萼片长度花瓣长度花瓣宽度图3-264个鸢尾花属性的平行坐标图,属性重新定序,以突出组的相似性和相异性平行坐标图的缺点之一是,在这种图中模式的检测可能取决于坐标轴的序。例如,如果线交叉太多,则图形就变得模糊不清,因此,需要安排坐标轴,以得到具有较少交叉的坐标轴序列。比较图3-26和图3-25,在图3-26中,片宽度(最混杂的属性)在图的最左边;而在图3-25中,该属性在中间。星形坐标和 Chernoff脸显示多维数据的另一种方法是用非文字传达信息的符号图示符(glyph)或图标(icon)对对象编码。更明确地说,对象的每个属性映射到图示符的一个特征,使得属性的值决定特征的
准确性质。这样,只需要扫一眼我们就可以辨别两个对象的差异。星形坐标(star coordinates)是该方法的一个例子。该技术对每个属性使用一个坐标轴,这些坐标轴从一个中心点向四周辐射,就像车轮的辐条,均匀地散开。通常,所有的属性值都映射到0,1区间。使用如下过程将对象映射到星形坐标系:将对象的每个属性值转换成一个分数,代表它在该属性的最大和最小值之间的距离,把这个分数映射到对应于该属性的坐标上的点,再将每个点用线段连接到相邻坐标轴上的点,形成一个多边形,多边形的大小和形状提供了对象属性值的视觉描述。为了便于解释,每个对象都使用单独的坐标系,换句话说,每个对象映射成一个多边形。星形坐标图的一个例子是鸢尾花150号花的星形坐标图,如图3-27a所示。还可以将特征值映射到更为熟悉的对象,如脸。该技术以其创建者 Herman Chernoff的名字命名为 Chernoff脸(Chernoff face)在这种技术中,每个属性与脸部的一个特征相关联,而属性的值确定脸部特征的表达方式。这样,随着对应的数据特征值的增加,脸的形状可能拉长。 Chernoff脸的一个例子是鸢尾花150号花的 Chernoff脸,如图3-27b所示。用于将脸映射到4个特征的方案在下面列出。脸部的其他特征,如眼间宽度和口的长度,是给定的省缺值。数据特征面部特征萼片长度脸部大小萼片宽度前额颚相对弧长花瓣长度前额形状花瓣宽度颚的形状赵士都花瓣长度片长度赵靠a)鸢尾花150号花的星形图(b)鸢尾花150号花的Cheroff脸图3-27鸢尾花数据集鸢尾花150号花的星形坐标图和 Cheroff脸例3.22图3-28和图3-29提供了用这两种方法观察多维数据的更多图示,这两个图分别显示取自鸢尾花数据集的15种花的星形图和脸状图前5种花属于Setosa种类,中间5种属于 Versicolour种类,而最后5种属于 Virginica种类。口尽管这些图有很好的视觉效果,但是它们不能很好地伸缩,因此对于许多数据挖掘问题,其应用受到限制。尽管如此,它们仍然可以作为快速比较用其他技术选择的少量对象集的一种手段
123555152515253545510210310510110210310410图3-28使用星形坐标的15种鸢尾花的图形图3-29使用 Cheroff脸的15种鸢尾花的图形3.3.5注意事项下面给出可视化注意事项的简短列表,以结束本节关于可视化的讨论。尽管这些指南颇具智慧,但也不能盲目遵循,指南永远不能取代对手头问题的深思熟虑。 ACCENT原则。下面是D.A.burn提出(经Michael Friendly改编)的有效图形显示的 ACCENT原则。理解( Apprehension)。正确察觉变量之间关系的能力。图形能够最大化对变量之间关系的理解吗?·清晰性(Clarity)。以目视识别图形中所有元素的能力。最重要的元素或关系在视觉上最突出吗?·一致性(Consistency)根据与以前的图形的相似性解释图形的能力。元素、符号形状和颜色与以前图形使用的一致吗?·有效性(Efficiency)。用尽可能简单的方法描绘复杂关系的能力。图形元素的使用经济吗?图形容易解释吗?·必要性( Necessity)对图形和图形元素的需要。与其他替代方法(表、文本)相比,图形是提供数据的更有用的形式吗?为表示关系,所有的图形元素都是必要的吗?·真实性(Truthfulness)通过图形元素相对于隐式或显式尺度的大小,确定图形元素所代表的真实值的能力。图形元素可以准确地定位和定标吗? Tufte指南。 Edward. Tufte列举了如下图形的优点(graphical excellence)原则图形的优点是感兴趣的(物质的、统计的和设计的)数据的良好设计的表示。图形的优点包括与清晰性、精确性和有效性相关的复杂思想。图形的优点是它在最小的空间内、以最少的笔墨、在最短的时间内为观察者提供最多的信息。图形的优点几乎总是多元的。图形的优点需要表述数据的真实性。3.4OLAP和多维数据分析本节考察来自将数据集看作多维数组的技术和见解.大量数据库系统支持这种观点,特别是OLAP(联机分析处理)系统事实上,OLAP系统的一些术语和能力已经使它进入被数百万人
使用的电子数据表程序。OLAP系统还非常关注交互式数据分析,并提供可视化数据和产生汇总统计的广泛能力。由于这些原因,我们的多维数据分析方法将基于OLAP系统常见的术语和概念。3.4.1用多维数组表示鸢尾花数据大部分数据集都可以用表来表示,其中每一行是一个对象,每一列是一个属性。在许多情况下,也可以将数据看作多维数组。我们通过将鸢尾花数据集表示成多维数组来解释这种方法。表3-7是通过如下方法创建的:离散化花瓣长度和花瓣宽度属性,使它们取值低、中和高,然后统计鸢尾花数据集中具有特定的花瓣宽度、花瓣长度和种类的花的数量。(对于花瓣宽度,类别低、中和高分别对应于区间[0,0.75),[0.75,.75)和[1.75,∞);对于花瓣长度,类别低、中和高分别对应于区间[0,2.5),[2.5,5)和[5,∞))表中没有显示空组合不包含任何一种花的组合。表3-7具有花瓣宽度、花瓣长度和种类特定组合的花的数量花瓣长度花瓣宽度种类计数 Setosa46 Setosa低低中中中中高高高高低中低中高高中中高高 Setosa Versicolour Versicolour Virginica Versicolour Virginica22333232 Versicolour Virginica44该数据可以组织成多维数组,如图3-30所示,其中,三个维分别对应于花瓣宽度、花瓣长度和种类。为清晰起见,显示了该数组的三个二维表切片,每个对应于一个种类见表3-8、表3-9和表3-10.表3-7和图3-30包含的信息是相同的,只是,在图3-30(以及表3-8、表3-9和表3-10)显示的多维表示中,属性花瓣宽度、花瓣长度和种类的值是数组下标。重要的是从多维的观点观察数据可以获得深入透彻的了解。表3-8、表3-9和表3-10显示,每个鸢尾花种类由花瓣宽度和花瓣长度值的不同组合来刻画, Setosa花具有较低的宽度和长度, Versicolour花具有中等的宽度和长度,而 Virginica花具有较高的宽度和长度。花瓣宽度 Virginica Versicolour Setosa高000中002低246花瓣宽度高中低 POG图3-30鸢尾花数据集的多维数组表示
表3-8依照 Setosa种类的花的花瓣表3-9依照 Versicolour种类的花的花长度和宽度的交叉表瓣长度和宽度的交叉表宽度宽度中2长度中低2000高000长度中低000中042高032表3-10依照 Virginica种类的花的花瓣长度和宽度的交叉表宽度长度中低000中003高0343.4.2多维数据:一般情况前一节给出了一个具体的例子,使用多维方法表示和分析一个熟悉的数据集。这里,详细介绍一般的方法。开始通常使用表的形式表示数据(如表37),这种表称作事实表(fact table)。用多维数组表示数据需要两个步骤:维的识别和分析所关注的属性的识别。维是分类属性,或者如前面的例子所示,是转换成分类属性的连续属性。属性值充当对应于该属性的维的数组下标,而属性值的个数是维的大小。在前面的例子中,每个属性有三个可能的值,因此每个维的大小都是3,并且可以通过3个值索引。这产生了3×3×3的多维数组。属性值的每个组合(每个不同的属性一个值)定义了多维数组的一个单元。使用前面的例子解释,如果花瓣长度=低,花瓣宽度=中,而种类=Setosa,则标识了一个值为2的特定单元。即,数据集中只有两种花具有指定的属性值。注意,表3-7中数据集的每一行(对象)对应于多维数组的一个单元。每个单元的内容代表一个我们在分析时感兴趣的目标量(target quantity)(目标变量或属性)的值。在鸢尾花例子中,目标量是其花瓣宽度和长度落入特定范围内的花的个数。目标属性是定量的,因为多维数据分析的关键目标是观察聚集量,如总和或平均值。下面总结用表形式表示的数据集创建多维数据表示的过程:首先确定用作维的分类属性以及用作分析目标的定量属性,然后将表的每一行(对象)映射到多维数组的一个单元,单元的下标由被选作维的属性的值指定,而单元的值是目标属性的值,假定没有被数据定义的单元的值为0例3.23为了进一步解释刚刚讨论的概念,我们给出一个更传统的涉及销售的例子。这个例子的事实表由表3-11给出,多维表示的维是产品ID、地点和日期属性,而目标属性是收入。图3-31显示了该数据集的多维表示,这个较大、更复杂的数据集将用来解释多维数据分析的其他概念。口
表3-11不同地点和时间的产品销售收入(单位:美元)产品ID地点日期收入 Minneapolis Oct. 182004$250 Chicagooct.182004$79 Parisct.18200430127 Minneapolis Oct. 182004$232127 Chicago Oct.182004$3278:27 ParisOct.182004$1325::日期$$$产品D图3-31销售数据的多维表示3.4.3分析多维数据本节介绍不同的多维分析技术。重点讨论数据立方体的创建和相关操作,如切片、切块、维归约、上卷和下钻。1.数据立方体:计算聚集量从多维角度看待数据的主要动机就是需要以多种方式聚集数据。在产品销售的例子中,我们可能希望找出特定年份、特定产品的总销售收入,或者希望得到每一地点所有产品的年销售收入。计算聚集总和涉及固定某些属性(维)的值在其余属性(维)的所有可能的值上求和。还有其他感兴趣的聚集量,但是为了简单起见,我们只讨论求和。表3-12显示对于日期和产品的各种组合,在所有地点上求和的结果。为简单起见,假定所有的日期在一年之内。如果一年有365天,并且有1000种产品,则表3-12有365000个表项(总和),每个产品一日期对一个。也可以指定商店位置和日期,在产品上求和,或者指定地点和产品,在所有的日期上求和。表3-13显示表3-12的边缘总和( marginal total)。这些总和是进一步在日期或产品上求和的结果。在表3-13中,产品1的总销售收入是30000,过在第一行上(在所有日期上)求和得到。2004年1月1日的总销售收入是$52762,通过在第一列上(在所有产品上)求和得到。总销售收入是$227352127,通过在所有行和所有的列(所有的时间和产品)上求和得到。所有这
些总和都针对所有地点来的,因为表3-13的表项包括所有的地点。表3-12对于固定的时间和产品,在所有地点上求和产生的总和日期2004.1.12004.1.22004.12.311:$1001$987$891产品27 ID$10265$10225$9325:表3-13包括边缘总和的表3-12日期2004.1.12004.1.22004.1231总和$1001$987$891$370000产品 ID27$10265$10225$9325$3800020总和$527362$532953$631221$227352127该例的要点是,从多维数组可以计算大量不同的总和(聚集),取决于我们在多少个属性上求和。假定有n个维,第i个维(属性)有s个可能的值,只在单个属性上求和有n种不同的方式,如果我们在维j上求和,则可以得到S1*-1*S+1*sn个总和,n-1个其他属性(维)的属性值的每种可能组合一个,在一个属性上求和得到的总和形成一个n-1维数组,并且有n个这样的总和数组。在产品销售例子中,在一个属性上求和导致三组总和,每组总和可以用一个二维表显示。如果我们在两个维上求和(或许从在一个维上求和得到的总和数组之一开始),则我们将得到具有n-2个维的总和多维数组,总共有C个这样的总和数组。对于产品销售例子,有C3=3个总和数组,分别是在地点和产品、地点和时间、产品和时间上求和的结果。一般地,在k维上求和产生个总和数组,每个具有n-k维。数据的多维表示,连同所有可能的总和(聚集)称作数据立方体(data cube)。尽管叫立方体,每个维的大小(属性值的个数)却不必相等。此外,数据立方体可能多于或少于三个维。更重要的是,数据立方体是称为交叉表(coss- tabulation)的统计学技术的推广,如果加上边缘总和,则表3-8、表3-9和表3-10就成了交叉表的典型例子。2.维归约和转轴前面介绍的聚集可以看作一种形式的维归约,具体说来,通过在第j维上求和,删除第j维,概念上讲,这将第j维上的每个单元“列”单缩成一个单元。对于销售和鸢尾花例子,在一个维上聚集将数据的维度从3归约2。如果s是第j维上的可能值个数,则单元数约减了一个因子s本章习题17要求读者考察这种类型的维归约和PCA之间的区别。转轴(pivoting)是指在除两个维之外的所有维上聚集。结果是一个二维交叉表,只有两个指定的维作为留下的维。表3-13是一个在日期和产品上转轴的例子。
3.切片和切块这两个生动的名字涉及相当直截了当的操作切片( slicing)是通过对一个或多个维指定特定的值,从整个多维数组中选择一组单元。表3-8、表3-9和表3-10是通过为种类维指定三个不同的值得到的鸢尾花数据集的三个切片。切块(dicing)涉及通过指定属性值区间选择单元子集这等价于由整个数组定义子数组。在实践中,两个操作都可以通过在某些维上聚集来实现。4.上卷和下钻在第2章,属性值在某种意义上被看作是原子”的,然而,实际情况并非总是如此。例如,每个日期有一些与之相关联的性质,如年、月和星期;数据也可以被认为属于一个特定的商业季度,或者,如果应用与教育有关,看作学校的季或学期;地点也有各种性质:洲、国、州(省)和城市;产品也可以划分成各种类别,如服装、电子产品和家具。通常,这些类别可以组织成树或格。例如年由月或星期组成,而它们都由日组成;地点也可以划分成国家,国家包含州(或其他地方政府单位),而州又包含城市;类似地,产品类可以进一步划分,例如,产品类别家具可以划分成子类别:椅子、桌子、沙发,等等。层次结构促使上卷和下钻操作的出现。为了解释这一点,考虑最初的销售数据,它是多维数组,记录每天的销售。我们可以按月聚集(上卷, roll up)销售数据。反过来,给定时间为划分成月份的数据表示,我们可能希望将月销售总和分解(下钻, drill down)成日销售总和,当然,这要求基本销售数据的时间粒度是按天的。这样,上卷和下钻操作与聚集相关。然而,它们不同于迄今为止所讨论的聚集操作,它们在一个维内聚集单元,而不是在整个维上聚集。3.4.4关于多维数据分析的最后评述就OLAP和相关系统所蕴涵的意义而言,多维数据分析将数据看作多维数组,并聚集数据,以便更好地分析数据的结构。对于鸢尾花数据,这种分析清楚地展现了花瓣长度和宽度的不同。对商务数据(如销售数据)的分析也能揭示许多有意义的模式,如有利润(或无利润)的商店或产品。如前所述,有多种类型的数据库系统支持多维数据分析。其中某些系统基于关系数据库,称作 ROLAP系统。业已设计了专门使用多维数据表示作为其基本数据模型的、更专业的数据库系统,这种系统称为MOLAP系统。除了这些类型的系统之外,还开发了统计数据库(SDB),来存储和分析各种统计数据,如政府和其他大型机构收集的人口普查和公共卫生数据。关于OLAP和SDB的参考文献在文献注释中提供。文献注释汇总统计在大部分统计学导论中都被详细讨论如[92]探测式数据分析的参考文献有 Tukey的经典文献[104]、 Velleman和 Hoaglin的书[105]作为大部分电子制表软件(Microsoft EXCEL[95])、统计程序(S99]、SPSS[102]、R[96]可视化技术可以很容易地使用,本章的大部分图都是使用 MATLAB制做的,作为R项目的开放PDG和S-plus[98])以及数学软件(MATLAB94]和 Mathematica[93])的必不可少的一部分,基本的源码软件包,统计软件包R是免费的。关于可视化的文献极多,涵盖了许多领域,跨越了数十年。该领域的经典著作之一是Tufte
的书[103]。 Spence的书[101]对本章的可视化部分影响很大,在原理和技术两方面都是信息可视化的有用文献,该书还提供了许多动态可视化技术的详尽讨论,而本章并未包含这些内容。另外两本关于可视化的可能也值得关注的书是card等[87]和 Fayyad等[89的书最后,万维网上有关于可视化的大量信息。由于eb站点来去频繁最好的办法是使用“信息可视化”、“数据可视化”或“统计图形”进行搜索。然而,我们想特意提到 Friendly的《数据可视化图库》(The Gallery of Data Visualization)[90].本章介绍的有效图形显示的 ACCENT原则可以在这里找到,或在Burn最初提出该原则的文章[86]中找到。有多种图形技术,可以用来考察数据是否是高斯分布的,或者是某种其他分布。此外,还有一些图,用以显示观测值在某种意义上是否是统计显著的。我们没有涵盖这些技术,建议读者查阅前面提到的统计和数学软件包。多维数据分析已经以各种形式存在多年。最早的文章之一是关系数据库之父Codd的白皮书[88]。数据立方体是Gray等[91]提出的,他们介绍了在关系数据库框架下,创建和操纵数据立方体的各种操作。统计数据库与OLAP的比较由 Shoshani[100]给出。关于OLAP的特殊信息可以在数据库销售商的文档资料和许多畅销书中找到。许多数据库教材也有OLAP的一般性讨论,通常是在数据仓库上下文中,例如, Ramakrishnan和 Gehrke的书[97]参考文献 [86] D. A. Burn. Designing Effective Statistical Graphs. In C. R. Rao, editor, Handbook of Statistics 9. Elsevier/North-Holland, Amsterdam, The Netherlands, September 1993. [87] S. K. Card, J. D. MacKinlay, and B. Shneiderman, editors. Readings in Information Visualization: Using Vision to Think. Morgan Kaufmann Publishers, San Francisco, CA, January 1999. [88] E. F. Codd, S. B. Codd, and C. T. Smalley Providing OLAP (On-line Analytical Processing)to User- Analysts: An ITMandate. White Paper, E.F. Codd and Associates, 1993. [89] U. M. Fayyad, G. G. Grinstein, and A Wierse, editors. Information Visualization in Data Mining and Knowledge Discovery. Morgan Kaufmann Publishers, San Francisco, CA, September 2001. [90] m. friendly. gallery of data visualization. http://www.math.yorku.ca/SCS/Gallery/, 2005. [91] J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venkatrao, F. Pellow, and H. Pirahesh. Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals. Journal Data Mining and Knowledge Discovery, 1(1): 29-53, 1997.[92]B.w. Lindgren. Statistical Theory. CRC Press, January1993 [93] mathematica 5.1. wolfram research, inc. http://www.wolfr [94] matlab 7.0. the mathworks, inc. http://www.mathwo 2005. [95]  microsoft excel 2003. microsoft,   inc. http: //www. microsoft. com/, 2003. [96] R: A language and environment for statistical computing and graphics. The R Project for Statistical computing. http: //www.r-project.org/, 2005 [97] R. Ramakrishnan and J. Gehrke. Database Management Systems. McGraw-Hill 3rd edition, August2002. [98] s-plus. insightful corporation. http: //www.insightful.com, 2005. [99] SAS: Statistical Analysis System. SAS institute inc. http://www.sas.com/,2005 [100] A. Shoshani. OLAP and statistical databases similarities and differences. In Proc. of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symp. on Principles of Database Systems, pages 185-196.ACM Press,1997. [101] R. Spence. Information Visualization ACM Press, New York, December 2000. [102] SPSS: Statistical Package for the social sciences.  spss,  inc.  http: //www.spss.com/, 2005.
 [103] E. R. Tufte. The Visual Display of Quantitative Information. Graphics Press, Cheshire, CT, March1986. [104] J. W. Tukey. Exploratory data analysis. Addison-Wesley, 1977. [105] P. Velleman and. Hoaglin. The ABC's of EDA: Applications, Basics, and Computing of Exploratory Data Analysis. Duxbury, 1981.习题1.从UCI机器学习库取得一个数据集,并尽可能多地使用本章介绍的不同的可视化技术。文献注释和本书网站提供了可视化软件的线索。2.至少指出使用颜色可视地提供信息的两个优点和两个缺点。3.关于三维图形,安排问题是什么?4.讨论使用抽样减少需要显示的数据对象个数的优缺点。简单随机抽样(无放回)是一种好的抽样方法吗?为什么是,为什么不是?5.如何创建可视表示来显示描述如下系统的信息。(a)计算机网络。确保包括网络的静态性质(如连接性)和动态性质(如通信量)(b)在特定时间,特定的植物和动物种类在全世界的分布。(c)对于一组基准数据库程序,计算机资源(如处理机时间、内存和磁盘)的使用情况。(d)过去30年内,一个特定国家的工人职业的变化。假定提供了每个人每年的信息,还提供了性别和文化程度。确保处理了以下问题。表示。如何将对象、属性和联系映射到可视化元素?安排。关于如何显示可视化元素,是否有需要考虑的特殊问题?具体的例子可能是视点的选择、透明度的使用、或将特定的对象组分开。选择。如何处理大量属性和数据对象?6.相对于标准直方图,指出茎叶图的一个优点和一个缺点。7.如何处理直方图依赖于箱的个数和位置的问题?8.描述盒状图如何提供属性值是否对称分布的信息。关于图3-11显示的属性的分布对称性,你有何种结论?9.使用图3-12,比较萼片长度、萼片宽度、花瓣长度和花瓣宽度。10.评论使用盒状图考察具有如下4个属性的数据集:年龄、体重、身高和收入。11.对于为什么花瓣长度和宽度的大部分值都落在图3-9沿对角线的桶中,给出一个可能的解释。12.使用图3-14和图3-15,识别花瓣宽度和花瓣长度属性共同的特性。13.简单线图(如图2-12显示两个时间序列的图)可以用来有效地显示高维数据。例如,在图2-12中,容易看出两个时间序列的频率是不同的。时间序列的什么特性使得高维数据可以有效地可视化?14.描述产生稀疏和稠密数据立方体情况类型。使用本书之外的例子加以解释。15.如何扩展多维数据分析概念,使得目标变量可以是定性变量?换言之,何种汇总统计或数据可视化是令人感兴趣的?
16.由表3-14构造数据立方体。这是稠密还是稀疏数据立方体?如果是稀疏的,识别出空单元。表3-14习题16的事实表产品I地点I销售量1011221312652217.讨论基于聚集的维归约与基于PCA和SVD等技术的维归约的区别。
 POG
第章分类:基本概念、决策树与模型评估分类任务就是确定对象属于哪个预定义的目标类。分类问题是一个普遍存在的问题,有许多不同的应用。例如:根据电子邮件的标题和内容检查出垃圾邮件,根据核磁共振扫描的结果区分肿瘤是恶性的还是良性的,根据星系的形状对它们进行分类(见图4-1)(a)螺旋状的星系(b)椭圆状的星系图4-1星系的分类图片来源于NASA网站本章介绍分类的基本概念,讨论诸如模型的过分拟合等关键问题,并提供评估和比较分类技术性能的方法。尽管本章主要关注一种称作决策树归纳的技术,但是本章讨论的大部分内容也适用于其他的分类技术,其中很多技术将在第5章介绍。4.1预备知识分类任务的输入数据是记录的集合。每条记录也称实例或样例,用元组(x,y)表示,其中x是属性的集合,而y是一个特殊的属性,指出样例的类标号(也称为分类属性或目标属性)表4-1列出一个样本数据集,用来将脊椎动物分为以下几类:哺乳类、鸟类、鱼类、爬行类和两栖类。属性集指明脊椎动物的性质,如体温、表皮覆盖、繁殖后代的方式、飞行的能力和在水中生存的能力等。尽管表4-1中的属性主要是离散的,但是属性集也可以包含连续特征。另一方面,类标号却必须是离散属性,这正是区别分类与回归(regression)的关键特征。回归是一种预测建模任务,其中目标属性y是连续的。定义4.1分类( classification)分类任务就是通过学习得到一个目标函数(target function)f,把每个属性集x映射到一个预先定义的类标号y目标函数也称分类模型(classification model分类模型可以用于以下目的。描述性建模分类模型可以作为解释性的工具,用于区分不同类中的对象。例如,对于生物
表41脊椎动物的数据集名字体温表皮覆盖胎生水生动物飞行动物有腿冬眠类标号人类恒温毛发哺乳类蟒蛇冷血鳞片爬行类鲑鱼鱼类鲸冷血鳞片恒温毛发青蛙冷血无巨蜥冷血鳞片蝙蝠恒温毛发鸽子恒温羽毛猫恒温软毛豹纹鲨冷血鳞片海龟冷血鳞片企鹅恒温羽毛是否否是否否是否是是否否是否否否否是是半否否否否是半半否是半否否否否否否是是否否否否否否否是否否否是是是是是否是是是否是否是否否是否是否否否否否是否是哺乳类两栖类爬行类哺乳类鸟类哺乳类鱼类爬行类鸟类豪猪恒温刚毛是哺乳类鳗冷血鳞片鱼类蝾螈冷血无两栖类学家或者其他人,一个描述性模型有助于概括表4-1中的数据,并说明哪些特征决定一种脊椎动物是哺乳类、爬行类、鸟类、鱼类或者两栖类。预测性建模分类模型还可以用于预测未知记录的类标号。如图4-2所示,分类模型可以看作是一个黑箱,当给定未知记录的属性集上的值时,它自动地赋予未知样本类标号。例如,假设有一种叫作毒蜥(gila monster)的生物,其特征如下:名字体温表皮覆盖胎生水生动物飞行动物有腿冬眠类标号毒蜥冷血鳞片否否否是是?可以使用根据表4-1中的数据集建立的分类模型来确定该生物所属的类。输入输出分类模型属性集(x)类标号(y)图4-2分类器的任务是根据输入属性集x确定类标号y分类技术非常适合预测或描述二元或标称类型的数据集,对于序数分类(例如,把人分类为高收入、中等收入或低收入组),分类技术不太有效,因为分类技术不考虑隐含在目标类中的序关系。其他形式的联系,如子类与超类的关系(例如,人类和猿都是灵长类动物,而灵长类是哺乳类的子类)也被忽略。本章余下的部分只考虑二元的或标称类型的类标号。4.2解决分类问题的一般方法分类技术(或分类法)是一种根据输入数据集建立分类模型的系统方法。分类法的例子包括决策树分类法、基于规则的分类法、神经网络、支持向量机和朴素贝叶斯分类法。这些技术都使用一种学习算法(learning algorithm)确定分类模型,该模型能够很好地拟合输入数据中类标号和属性集之间的联系。学习算法得到的模型不仅要很好地拟合输入数据,还要能够正确地预测未知样本的类标号。因此,训练算法的主要目标就是建立具有很好的泛化能力模型,即建立能够准确地预测未知样本类标号的模型。
图4-3展示解决分类问题的一般方法。首先,需要一个训练集( training set),它由类标号已知的记录组成。使用训练集建立分类模型,该模型随后将运用于检验集(test set),检验集由类标号未知的记录组成。训练集Td属性1属性2属性3类 1 Yes Large 125K No学习算法 2 No Medium 100K No 3 No Small 70K No 4 Yes Medium 120K No 5 No Large 95K Yes归纳 6 No Medium 60K No 7 Yes Large 220K No 8 No Small 85K Yes学习模型 9 No Medium 75K No 10 No Small 90KYes模型检验集id属性1属性2属性3类应用模型 11 No Small 55K 12 Yes Medium 80K推论 13Yes Large 110K? 14No Small 95K 15 No Large 67K图4-3建立分类模型的一般方法分类模型的性能根据模型正确和错误预测的检验记录计数进行评估,这些计数存放在称作混淆矩阵(confusion matrix)的表格中。表4-2述二元分类问题的混淆矩阵。表中每个表项f表示实际类标号为i但被预测为类j的记录数,例如,fo1代表原本属于类0但被误分为类1的记录数。按照混淆矩阵中的表项,被分类模型正确预测的样本总数是fi+fo),而被错误预测的样本总数是(fio+fo)表4-2二类问题的混淆矩阵预测的类类=1类=0实际的类类=1 fio类=0f虽然混淆矩阵提供衡量分类模型性能的信息,但是用一个数汇总这些信息更便于比较不同模型的性能。为实现这一目的,可以使用性能度量(performance metric),如准确率accuracy)其定义如下:准确率正确预测数f+f预测总数fu+fio+fo+fo(4-1)同样,分类模型的性能可以用错误率(error rate)来表示,其定义如下: PDG错误率=错误预测数 fio fo预测总数f+fio+fo+f(4-2)
大多数分类算法都在寻求这样一些模型,当把它们应用于检验集时具有最高的准确率,或者等价地,具有最低的错误率。我们将在4.5节再次讨论模型的评估问题。4.3决策树归纳本节介绍决策树( decision tree)分类法,这是一种简单但却广泛使用的分类技术。4.31决策树的工作原理为了解释决策树分类的工作原理,考虑上一节中介绍的脊椎动物分类问题的简化版本这里,我们不把脊椎动物分为五个不同的物种,而只考虑两个类别:哺乳类动物和非哺乳类动物。假设科学家发现了一个新的物种,怎么判断它是哺乳动物还是非哺乳动物呢?一种方法是针对物种的特征提出一系列问题。第一个问题可能是,该物种是冷血动物还是恒温动物。如果它是冷血的,则该物种肯定不是哺乳动物;否则它或者是某种鸟,或者是某种哺乳动物。如果它是恒温的,需要接着问:该物种是由雌性产崽进行繁殖的吗?如果是,则它肯定为哺乳动物,否则它有可能是非哺乳动物(鸭嘴兽和针鼹这些产蛋的哺乳动物除外)上面的例子表明,通过提出一系列精心构思的关于检验记录属性的问题,可以解决分类问题。每当一个问题得到答案,后续的问题将随之而来,直到我们得到记录的类标号。这一系列的问题和这些问题的可能回答可以组织成决策树的形式,决策树是一种由结点和有向边组成的层次结构。图4-4显示哺乳类动物分类问题的决策树,树中包含三种结点。根结点(root node),它没有入边,但有零条或多条出边内部结点(internal node),恰有一条入边和两条或多条出边。叶结点(leaf node)或终结点(terminal node),恰有一条入边,但没有出边。在决策树中,每个叶结点都赋予一个类标号非终结点(non--terminal node)(包括根结点和内部结点)包含属性测试条件,用以分开具有不同特性的记录。例如,在图4-4中,在根结点处,使用体温这个属性把冷血脊椎动物和恒温脊椎动物区别开来。因为所有的冷血脊椎动物都是非哺乳动物,所以用一个类称号为非哺乳动物的叶结点作为根结点的右子女。如果脊椎动物的体温是恒温的,则接下来用胎生这个属性来区分哺乳动物与其他恒温动物(主要是鸟类)体温根结点内部结点恒温冷血胎生非哺乳动物是否叶结点哺乳动物非哺乳动物图4-4哺乳动物分类问题的决策树
一旦构造了决策树,对检验记录进行分类就相当容易了。从树的根结点开始,将测试条件用于检验记录,根据测试结果选择适当的分支沿着该分支或者到达另一个内部结点,使用新的测试条件,或者到达一个叶结点。到达叶结点之后,叶结点的类称号就被赋值给该检验记录。例如,图4-5显示应用决策树预测火烈鸟的类标号所经过的路径,路径终止于类称号为非哺乳动物的叶结点。未标记名字体温胎生类称号的数据火烈鸟恒温否体温非哺乳动物恒温冷血胎生非哺乳动物是哺乳非哺乳动物动物图4-5对一种未标记的脊椎动物分类。虚线表示在未标记的脊椎动物上使用各种属性测试条件的结果。该脊椎动物最终被指派到非哺乳动物类4.3.2如何建立决策树原则上讲,对于给定的属性集,可以构造的决策树的数目达指数级。尽管某些决策树比其他决策树更准确,但是由于搜索空间是指数规模的,找出最佳决策树在计算上是不可行的。尽管如此,人们还是开发了一些有效的算法,能够在合理的时间内构造出具有一定准确率的次最优决策树。这些算法通常都采用贪心策略,在选择划分数据的属性时,采取一系列局部最优决策来构造决策树,Hunt算法就是一种这样的算法Hunt算法是许多决策树算法的基础,包括ID3、C4.5和CART。本节讨论Hunt算法并解释它的一些设计问题。1.hunt算法在Hunt算法中,通过将训练记录相继划分成较纯的子集,以递归方式建立决策树。设D1是与结点t相关联的训练记录集,而y={y1,y2y}是类标号,Hunt算法的递归定义如下(1)如果D1中所有记录都属于同一个类y,则t是叶结点,用y标记(2)如果D中包含属于多个类的记录,则选择一个属性测试条件(attribute test condition),将记录划分成较小的子集。对于测试条件的每个输出,创建一个子女结点,并根据测试结果将D1中的记录分布到子女结点中。然后,对于每个子女结点,递归地调用该算法。为了解释该算法如何执行,考虑如下问题:预测贷款申请者是会按时归还贷款,还是会拖欠贷款。对于这个问题,训练数据集可以通过考察以前贷款者的贷款记录来构造。在图46所示的例子中,每条记录都包含贷款者的个人信息,以及贷款者是否拖欠贷款的类标号。p
二元的分类的连续的类tid有房者婚姻状况年收入拖贷款者是否否单身125K2否已婚100K3单身70K4是已婚120K5否离异95K6否已婚60K7是离异220K8否单身85K否否否否是否否是否是9否已婚75K10否单身90K图4-6训练数据集:预测拖欠银行贷款的贷款者该分类问题的初始决策树只有一个结点,类标号为“拖欠贷款者=否”(见图4-7a),意味大多数贷款者都按时归还贷款。然而,该树需要进一步的细化,因为根结点包含两个类的记录。根据“有房者”测试条件,这些记录被划分为较小的子集,如图4-7b所示。选取属性测试条件的理由稍后讨论,目前,我们假定此处这样选是划分数据的最优标准。接下来,对根结点的每个子女递归地调用Hunt算法从图46给出的训练数据集可以看出,有房的贷款者都按时偿还了贷款,因此,根结点的左子女为叶结点,标记为“拖欠贷款者=否”(见图4-7b)。对于右子女,我们需要继续递归调用Hunt算法,直到所有的记录都属于同一个类为止。每次递归调用所形成的决策树显示在图4-7c和图4-7d中。有房者是否拖欠贷款者=否拖欠贷款者=否拖欠贷款者=否(a)(b)有房者是否有房者拖欠贷款者=否婚姻状况是否单身已婚离异拖欠贷款者=否婚姻状况年收入拖欠贷款者=否单身已婚<80K≥80K离异拖欠贷款者=是拖欠贷款者=否拖欠贷款者=否拖欠贷款者=是(c)(d)图4-7Hunt算法构造决策树
如果属性值的每种组合都在训练数据中出现并且每种组合都具有唯一的类标号,则Hunt算法是有效的。但是对于大多数实际情况,这些假设太苛刻了,因此,需要附加的条件来处理以下的情况。(1)算法的第二步所创建的子女结点可能为空,即不存在与这些结点相关联的记录。如果没有一个训练记录包含与这样的结点相关联的属性值组合,这种情形就可能发生。这时,该结点成为叶结点,类标号为其父结点上训练记录中的多数类。(2)在第二步,如果与D相关联的所有记录都具有相同的属性值(目标属性除外),则不可能进一步划分这些记录。在这种情况下,该结点为叶结点,其标号为与该结点相关联的训练记录中的多数类。2.决策树归纳的设计问题决策树归纳的学习算法必须解决下面两个问题。(1)如何分裂训练记录?树增长过程的每个递归步都必须选择一个属性测试条件,将记录划分成较小的子集。为了实现这个步骤,算法必须提供为不同类型的属性指定测试条件的方法,并且提供评估每种测试条件的客观度量。(2)如何停止分裂过程?需要有结束条件,以终止决策树的生长过程。一个可能的策略是分裂结点,直到所有的记录都属于同一个类,或者所有的记录都具有相同的属性值。尽管两个结束条件对于结束决策树归纳算法都是充分的,但是还可以使用其他的标准提前终止树的生长过程。提前终止的优点将在4.4.5节讨论。4.3.3表示属性测试条件的方法决策树归纳算法必须为不同类型的属性提供表示属性测试条件和其对应输出的方法。二元属性二元属性的测试条件产生两个可能的输出,如图4-8所示。体温恒温冷血图4-8二元属性的测试条件标称属性由于标称属性有多个属性值,它的测试条件可以用两种方法表示,如图49所示。对于多路划分(图4-9a),其输出数取决于该属性不同属性值的个数。例如,如果属性婚姻状况有三个不同的属性值单身、已婚、离异,则它的测试条件就会产生一个三路划分。另一方面,某些决策树算法(如CART)只产生二元划分,它们考虑创建k个属性值的二元划分的所有2k-1-1种方法。图49b显示了把婚姻状况的属性值划分为两个子集的三种不同的分组方法。序数属性序数属性也可以产生二元或多路划分,只要不违背序数属性值的有序性,就可以对属性值进行分组。图4-10显示了按照属性衬衣尺码划分训练记录的不同的方法。图4-10a和图4-10b中的分组保持了属性值间的序关系,而图4-10c所示的分组则违反了这一性质,因为它把小号和大号分为一组,把中号和加大号放在另一组。
婚姻状况单身离异已婚(a)多路划分婚姻状况婚姻状况婚姻状况或或{已婚}单身,离异}(单身}{已婚,离异}{单身,已婚}【离异(b)二元划分(通过属性值分组)图4-9标称属性的测试条件衬衣尺码衬衣尺码衬衣尺码{小号,中号}(大号,加大号}【小号}【中号,加大号小号,大号【中号,加大号(a) (b)(c)图4-10序数属性值分组的不同方式连续属性对于连续属性来说,测试条件可以是具有二元输出的比较测试(A<v)或(A≥v)也可以是具有形如v≤A<vi+1(i=1,,k)输出的范围查询,图4-11显示了这些方法的差别。对于二元划分,决策树算法必须考虑所有可能的划分点v,并从中选择产生最佳划分的点。对于多路划分,算法必须考虑所有可能的连续值区间。可以采用2.3.6节介绍的离散化的策略,离散化之后,每个离散化区间赋予一个新的序数值,只要保持有序性,相邻的值还可以聚集成较宽的区间。年收入>80K年收入<10K>80K是否10K,25K(25K,50K}50K,80K}(a)(b)图4-11连续属性的测试条件4.3.4选择最佳划分的度量有很多度量可以用来确定划分记录的最佳方法,这些度量用划分前和划分后记录的类分布定义。
设p(it)表示给定结点t中属于类i的记录所占的比例,有时,我们省略结点t,直接用p表示该比例。在两类问题中,任意结点的类分布都可以记作(po,pi),其中p1=1一po例如,考虑图4-12中的测试条件,划分前的类分布是(0.5,05),因为来自每个类的记录数相等。如果使用性别属性来划分数据,则子女结点的类分布分别为.6,0.4)和(0.4,0.6),虽然划分后两个类的分布不再平衡,但是子女结点仍然包含两个类的记录按照第二个属性车型进行划分,将得到纯度更高的划分。性别车型顾客I男女家用豪华V20运动v10/v11: 6 C0::: CO: 1CO:..: 1:0:0C1:3c1:0C1:71:0c1:0c1:11:1(a)(b)(c)图4-12多路划分与二元划分选择最佳划分的度量通常是根据划分后子女结点不纯性的程度。不纯的程度越低,类分布就越倾斜。例如,类分布为(0,1)的结点具有零不纯性而均衡分布(0.5,0.5)的结点具有最高的不纯性。不纯性度量的例子包括: Entropy(t)=- p(ilt)log2p(it)(4-3) Gini()= 1-[p(]?(4-4) Classification error(t) 1-max( t)](4-5)其中c是类的个数,并且在计算熵时,Olog20=0图4-13显示了二元分类问题不纯性度量值的比较,p表示属于其中一个类的记录所占的比例。从图中可以看出,三种方法都在类分布均衡时(即当p=0.5时)达到最大值,而当所有记录都属于同一个类时(p等于1或0)达到最小值。下面我们给出三种不纯性度量方法的计算实例。结点N计数Gini=1-(0/6)2-(6/6)2=0类=00 Entropy=-0/6)log2(6)-6)log266)=0类=16 Error=i-max[0/6,6/6]=0结点N2计数Gini=1-(1/6)-(5/6)2=0.278类=01 Entropy=-(1/6)log2(1/6)-(/6)log2(5/6)=0.650类=15 Error=-max1/6,5/6]=0.167结点N3计数Gini=1-(3/6)2-(36)2=0.5类=03 Entropy=-(36)log2(3/6)-(3/6)log2(3/6)=1类=13 Error=1-max[3/6,3/6]=0.5
10.90.8熵0.70.60.5 Gini0.40.30.2分类误差0.10.10.20.30.40.50.60.70.80.9p图4-13二元分类问题不纯性度量之间的比较从上面的例子及图4-13可以看出,不同的不纯性度量是一致的。根据计算,结点N具有最低的不纯性度量值,接下来依次是N2,N3。虽然结果是一致的,但是作为测试条件的属性选择仍然因不纯性度量的选择而异,如本章习题3所示。为了确定测试条件的效果,我们需要比较父结点(划分前)的不纯程度和子女结点(划分后)的不纯程度,它们的差越大,测试条件的效果就越好。增益△是一种可以用来确定划分效果的标准:△=(parent)-2(v)(4-6)其中,I()是给定结点的不纯性度量,N是父结点上的记录总数,k是属性值的个数,N是与子女结点v相关联的记录个数。决策树归纳算法通常选择最大化增益的测试条件,因为对所有的测试条件来说,I(parent)是一个不变的值,所以最大化增益等价于最小化子女结点的不纯性度量的加权平均值。最后,当选择熵(entropy作为公式(4-6)的不纯性度量时,熵的差就是所谓信息增益(information gain)info1.二元属性的划分考虑图4-14中的图表,假设有两种方法将数据划分成较小的子集。划分前,Gini指标等于0.5,因为属于两个类的记录个数相等如果选择属性A划分数据,结点N1的Gini指标等于0.4898,而N2的Gini指标等于0.480,派生结点的gini指标的加权平均为(7/12)×0.4898+(5/12)×0.480=0.486。类似的,我们可以计算属性B的Gin指标加权平均是0.371.因为属性B具有更小的Gini指标,它比属性A更可取。2.标称属性的划分正如前面提到的,标称属性可以产生二元划分或多路划分,如图4-15所示。二元划分的Gini指标的计算与二元属性类似。对于车型属性第一种二元分组,{运动,豪华}的Gini指标是0.4922,而{家用}的Gini指标是0.3750该分组的gini指标加权平均是:16/20×0.4922+4/20×0.3750=0.468
父结点6 C16gin=0.500B是否是否结点N结点N2结点N结点N2 N N2C042C015C133C142gin=0.486gini=0.371图4-14划分二元属性类似的,对第二种二元分组{运动}和{家用,豪华},Gini指标加权平均是0.167。第二种分组的Gini指标值相对较低,因为其对应的子集的纯度高得多。对于多路划分,需要计算每个属性值Gini指标。Gini(家用})=0.375,ini({运动})=0,Gini({豪华)=0.219,多路划分的总Gini指标等于:4/20×0.375+8/20×0+8/20×0.219=0.163多路划分的Gini指标比两个二元划分都小。这一结果并不奇怪,因为二元划分实际上合并了多路划分的某些输出,自然降低了子集的纯度。车型车型车型{运动,豪华{家用}{家用,豪华{运动}家用豪华运动车型车型车型运动{家用}运动)家用家用(运动}(豪华豪华}豪华 co9082C17131010 C1138017 Gini0.468 Gini0.167 Gini0.163(a)二元划分(b)多路划分图4-15划分标称属性3.连续属性的划分考虑图4-16所示的例子,其中测试条件“年收入≤v”用来划分拖欠贷款分类问题的训练记录。用穷举方法确定v的值,将N个记录中所有的属性值都作为候选划分点。对每个候选v,都要扫描一次数据集,统计年收入大于和小于v的记录数,然后计算每个候选的Gini指标,并从中选择具有最小值的候选划分点。这种方法的计算代价是昂贵的,因为对每个候选划分点计算Gini指标需要O(M)次操作,由于有N个候选,总的计算复杂度为O(N2)。为了降低计算复杂度,按照年收入将训练记录排序,所需要的时间为omog,从两个相邻的排过序的属性值中选择
中间值作为候选划分点,得到候选划分点55,65,72等。无论如何,与穷举方法不同,在计算候选划分点的Gini指标时,不需考察所有N个记录。类 No Yes Yes No No No No年收入排序后的值60707585095100120125220划分点→55657280879297110122172230yes3303312213303330N0716253434343443526170Gini0.4200.400.3750.3430.4170.400.3000.3430.3750.4000.420图4-16连续属性的划分对第一个候选v=55,没有年收入小于$55K的记录,所以年收入<$55K的派生结点的Gini指标是0;另一方面,年收入大于或等于55K的样本记录数目分别为3(类Yes)和7(类No)这样,该结点的Gini指标是0.420。该候选划分的总Gini指标等于0×0+1×0.420=0.420对第二个候选v=65,通过更新上一个候选的类分布,就可以得到该候选的类分布。更具体地说,新的分布通过考察具有最低年收入(即$60K)的记录的类标号得到。因为该记录的类标号是No,所以类No的计数从0增加到1(对于年收入$65K),和从7降到6(对于年收入>$65K),类Yes的分布保持不变。新的候选划分点的加权平均Gini指标为0.400重复这样的计算,直到算出所有候选的Gini指标值,如图4-16所示。最佳的划分点对应于产生最小Gini指标值的点,即v=97。该过程代价相对较低,因为更新每个候选划分点的类分布所需的时间是一个常数。该过程还可以进一步优化:仅考虑位于具有不同类标号的两个相邻记录之间的候选划分点。例如,因为前三个排序后的记录(分别具有年收入$60K、$70K和$75K)具有相同的类标号,所以最佳划分点肯定不会在$60K和$75K之间,因此,候选划分点v=$55K、$65K、$72K、$87K、$92K、$110K、$12K、$172K和$230K都将被忽略,因为它们都位于具有相同类标号的相邻记录之间。该方法使得候选划分点的个数从11个降到2个。4.增益率熵和Gini指标等不纯性度量趋向有利于具有大量不同值的属性。图412显示了三种可供选择的测试条件,划分本章习题2中的数据集。第一个测试条件性别与第二个测试条件车型相比,容易看出车型似乎提供了更好的划分数据的方法,因为它产生更纯的派生结点。然而,如果将这两个条件与顾客相比,后者看来产生更纯的划分,但顾客却不是一个有预测性的属性,因为每个样本在该属性上的值都是唯一的。即使在不太极端情形下,也不会希望产生大量输出的测试条件,因为与每个划分相关联的记录太少,以致不能作出可靠的预测。解决该问题的策略有两种。第一种策略是限制测试条件只能是二元划分,CART这样的决策树算法采用的就是这种策略;另一种策略是修改评估划分的标准,把属性测试条件产生的输出数也考虑进去,例如,决策树算法C4.5采用称作增益率(gain ratio)的划分标准来评估划分。增益率定义如下: Gain ratio= iato(4-7) PDG Split Info
其中,划分信息 Split Info-(v)log2P(v2,而k是划分的总数。例如,如果每个属性值具有相同的记录数,则Vi:P(v)=1/k,而划分信息等于logk这说明如果某个属性产生了大量的划分,它的划分信息将会很大,从而降低了增益率。4.3.5决策树归纳算法算法4.1给出了称作 TreeGrowth的决策树归纳算法的框架。该算法的输入是训练记录集E和属性集F。算法递归地选择最优的属性来划分数据(步骤7),并扩展树的叶结点(步骤11和步骤12),直到满足结束条件(步骤1)。算法的细节如下。(1)函数createdNode()为决策树建立新结点。决策树的结点或者是一个测试条件,记作node.testcond,或者是一个类标号,记作node.labelo(2)函数find_best_split()确定应当选择哪个属性作为划分训练记录的测试条件。如前所述,测试条件的选择取决于使用哪种不纯性度量来评估划分,一些广泛使用的度量包括熵、Gini指标和x2统计量。(3)函数classify()为叶结点确定类标号。对于每个叶结点t令p(it表示该结点上属于类i的训练记录所占的比例,在大多数情况下,都将叶结点指派到具有多数记录的类: leaf.label= argmax p()(4-8)i其中,操作 argmax返回最大化p(it)的参数值ip(il除了提供确定叶结点类标号所需要的信息之外,还可以用来估计分配到叶结点t的记录属于类i的概率。5.7.2节和5.7.3节讨论如何使用这种概率估计,在不同的代价函数下,确定决策树的性能。(4)函数 stopping_cond()通过检查是否所有的记录都属于同一个类,或者都具有相同的属性值,决定是否终止决策树的增长。终止递归函数的另一种方法是,检查记录数是否小于某个最小阈值。算法4.1决策树归纳算法的框架 TreeGrowth(E, F) 1: if stopping_cond(E, F)=true then 2: leaf= createNode() 3: leaf.label= Classify(E 4: return leaf 5:else 6: root createNode() 7: root.test_cond= find_best_split(E, F)8:令={vv是oot. testcond的一个可能的输出}9:for每个 veV do10:E=eoot.test_cond(e)=v并且e∈e 11: child=TreeGrowth(E, F)12:将 child作为root的派生结点添加到树中,并将边(root-child→)标记为v 13: end for 14: end if 15: return root
建立决策树之后,可以进行树剪枝(tee-pruning),以减小决策树的规模。决策树过大容易受所谓过分拟合(overfitting)现象的影响。通过修剪初始决策树的分支,剪枝有助于提高决策树的泛化能力。过分拟合和树剪枝问题将在4.4节更详细地讨论。4.3.6例子:Web机器人检测Web使用挖掘就是利用数据挖掘的技术,从Web访问日志中提取有用的模式。这些模式能够揭示站点访问者的一些有趣特性:例如,一个人频繁地访问某个Web站点,并打开介绍同一产品的网页,如果商家提供一些打折或免费运输的优惠,这个人很可能会购买这种商品。在eb使用挖掘中,重要的是要区分用户访问和Web机器人(Web robot)访问。Web机器人(又称eb爬虫)是一个软件程序,它可以自动跟踪嵌入网页中的超链接,定位和获取 Internet上的信息。这些程序安装在搜索引擎的入口收集索引网页必须的文档。在应用Web挖掘技术分析人类的浏览习惯之前,必须过滤掉Web机器人的访问。本节介绍如何使用决策树分类法来区分正常的用户访问和由Web机器人产生的访问。输入数据取自Web服务器日志,它的一个样本显示在图4-17a中,每行对应于Web客户(用户或Web机器人)的一个页面访问请求。Web日志记录的字段包括客户端的IP地址、请求的时间戳、请求访问的文档的网址、文档的大小、客户的身份(通过用户代理字段获得)。Web会话是客户在一次网站访问期间发出的请求序列,每个eb会话都可以用有向图来建模,其中结点对应于网页,而有向边对应于连接网页的超链。图4-17b显示Web服务器日志中第一次web会话的图形表示。会话P地址时间戳方法请求的Web页面协议状态字节数提交者用户代理 1160.11.11.11 08/aug/2004 get http://www.cs.umn10:15:21 (compatible; MSIE 6.0; Windows NT 5.0) 160.11.11.11 08/aug/2004  get http:/www.cs. umn, edu/ 41378 http:/www.cs.umn.edu/ Mozilla/4.010:15:34 (compatible; MSIE 6.0: Windows NT 5.0) 160.11.11.11 08/aug/2004  get http://w10:15:41 HTTP/1.1200 1018516  http:/www.cs.umn.edu/  mozilla/4.0 -kumar/MINDS (compatible; MSIE 6.0: wWindows NT 5.0)116.1.1.08/aug/200 1160.11.11.11 08/aug/2004 get http::/ 1.1200 7463 http:/www.cs.umn.edu/ Mozilla/4.010:16:1 kumar (compatible: MSIE 6.0; html 2 35.9.2.2 08/aug/2004  get http: /www.cs.umn.edu/ HTTP/1.0 2003149 Windows NT 5.0)10:16:15 ~steinbac windows NT 5.1; en-US;rv:.7) Gecko/20040616(a)eb服务器日志样本属性名描述 http://www.cs.umn.edu/-kumar totalPages一次Web会话提取的页面总数 ImagePages一次eb会话提取的图像页总数 TotalTime网站访问者所用的时间 RepeatedAccess一次web会话多次请求同一页面 MINDs ErrorRequest请求网页的错误 GET使用GET方式提出的请求百分比 papers/papers. html POST使用POST方式提出的请求百分比 HEAD使用HEAD方式提出的请求百分比 BreadthWeb遍历的宽 MINDS/MINDS_papers.htm DepthWeb遍历的深度 MultiIP使用多个IP地址的会话 MultiAgent使用多个代理的会话(b)Web会话图(c)web机器人检测的导出属性图4-17Web机器人检测的输入数据 PDG
为了对Web会话进行分类,需要构造描述每次会话特性的特征。图4-17c显示了Web机器人检测任务使用的一些特征。显著的特征有遍历的深度和宽度。深度确定请求页面的最大距离,其中距离用自网站入口点的超链数量度量。例如,假设主页hp:wwcs.umn.edu ,http: //www.cs.umn.edu/-kumar的深度为0,则 0,  http://www.cs.umn.edu/-kumar/MINDS/MINDS_P. htm 4-17b的深度为2.根据图4-17b中的Web图,第一次会话的深度等于2。宽度属性度量Web图的宽度。例如,图4-17b中显示的Web会话的宽度等于2用于分类的数据集包含2916个记录,We机器人(类1)和正常用户(类2)会话的个数相等,10%的数据用于训练,而90%的数据用于检验。生成的决策树模型显示在图4-18中,该决策树在训练集上的错误率为3.8%,在检验集上的错误率为5.3%该模型表明可以从以下4个方面区分出Web机器人和正常用户。(1)Web机器人的访问倾向于宽而浅,而正常用户访问比较集中(窄而深)(2)与正常用户不同,Web机器人很少访问与Web文档相关的图片页(3)Web机器人的会话的长度趋于较长,包含了大量请求页面(4)Web机器更可能对相同的文档发出重复的请求,因为正常用户访问的网页常常会被浏览器保存。决策树: depth=1: I breadth> 7: class 1 breadth<= 7: breadth 3: II Image Pages<=0.375: s81 totalPages<= 6: class totalPages> 6: I11II breadth <=1: class 1 breadth>1: class 0 width >3: MultilP 0: I ImagePages<=0.1333: class 1 I ImagePages> 0.1333: breadth 6: cla breadth<=6: class MultilP=1: TotalTime <=361: class 0 Totalime 361: class 1 I MultiAgent=: depth >2: class!! depth<2: MutilP=: class 0 MultilP =0: ClaBs!!!! breadth>6: RepeatedAccess <=0.322: class 0 I II RepeatedAccess >0.322: class 1 MultiAgent=1: totalPages <=81: class II totalPages >81: class 1硺,图4-18Web机器人检测的决策树模型4.3.7决策树归纳的特点下面是对决策树归纳算法重要特点的总结。(1)决策树归纳是一种构建分类模型的非参数方法。换句话说,它不要求任何先验假设,不假定类和其他属性服从一定的概率分布(不像第5章介绍的一些技术)
(2)找到最佳的决策树是NP完全问题。许多决策树算法都采取启发式的方法指导对假设空间的搜索。例如,4.3.5节中介绍的算法就采用了一种贪心的、自顶向下的递归划分策略建立决策树。(3)已开发的构建决策树技术不需要昂贵的计算代价,即使训练集非常大,也可以快速建立模型。此外,决策树一旦建立,未知样本分类非常快,最坏情况下的时间复杂度是O(w),其中w是树的最大深度。(4)决策树相对容易解释,特别是小型的决策树。在很多简单的数据集上,决策树的准确率也可以与其他分类算法相媲美。(5)决策树是学习离散值函数的典型代表。然而,它不能很好地推广到某些特定的布尔问题。个著名的例子是奇偶函数,当奇数(偶数)个布尔属性为真时其值为0(1)。对这样的函数准确建模需要一棵具有2个结点的满决策树,其中d是布尔属性的个数(见本章习题1)(6)决策树算法对于噪声的干扰具有相当好的鲁棒性,采用避免过分拟合的方法之后尤其如此。避免过分拟合的方法将在4.4节介绍。(7)冗余属性不会对决策树的准确率造成不利的影响。一个属性如果在数据中它与另一个属性是强相关的,那么它是冗余的。在两个冗余的属性中,如果已经选择其中一个作为用于划分的属性,则另一个将被忽略。然而,如果数据集中含有很多不相关的属性(即对分类任务没有用的属性),则某些不相关属性可能在树的构造过程中偶然被选中,导致决策树过于庞大。通过在预处理阶段删除不相关属性,特征选择技术能够帮助提高决策树的准确率。我们将在4.4.3节考察不相关属性过多的问题。(8)由于大多数的决策树算法都采用自顶向下的递归划分方法,因此沿着树向下,记录会越来越少。在叶结点,记录可能太少,对于叶结点代表的类,不能做出具有统计意义的判决,这就是所谓的数据碎片(data fragmentation)问题。解决该问题的一种可行的方法是,当样本数小于某个特定阈值时停止分裂。(9)子树可能在决策树中重复多次,如图4-9所示,这使得决策树过于复杂,并且可能更难解释。当决策树的每个内部结点都依赖单个属性测试条件时,就会出现这种情形。由于大多数的决策树算法都采用分治划分策略,因此在属性空间的不同部分可以使用相同的测试条件,从而导致子树重复问题。0图4-19子树重复问题。相同的子树可能出现在不同的分支 PDG(10)迄今为止,本章介绍的测试条件每次都只涉及一个属性。这样,可以将决策树的生长过程看成划分属性空间为不相交的区域的过程,直到每个区域都只包含同一类的记录(见图
4-20)。两个不同类的相邻区域之间的边界称作决策边界(decision boundary)由于测试条件只涉及单个属性,因此决策边界是直线,即平行于“坐标轴”,这就限制了决策树对连续属性之间复杂关系建模的表达能力。图4-21显示了一个数据集,使用一次只涉及一个属性的测试条件的决策树算法很难有效地对它进行分类。10.9x<0.430.807卧) No0.60.5y<0.47)(y<0.330.40.3 Yes No Yes No0.2400:40.10:4:3:00.10.20.30.40.50.60.70.80.9图4-20二维数据集的决策树及其决策边界示例10.90.80.70.60.50.40.30.2+0.10.10.20.30.40.50.60.70.80.9图4-21使用仅涉及单个属性的测试条件不能有效划分的数据集的例子斜决策树(oblique decision tree)可以克服以上的局限,因为它允许测试条件涉及多个属性。图4-21中的数据集可以很容易地用斜决策树表示,该斜决策树只有一个结点,其测试条件为:x+y<1尽管这种技术具有更强的表达能力,并且能够产生更紧凑的决策树,但是为给定的结点找出最佳测试条件的计算可能是相当复杂的。构造归纳( constructive induction)提供另一种将数据划分成齐次非矩形区域的方法(见2.3.5节),该方法创建复合属性,代表已有属性的算术或逻辑组合。新属性提供了更好的类区分能力,并在决策树归纳之前就增广到数据集中。与斜决策树不同,构造归纳不需要昂贵的花费,因为在构造决策树之前,它只需要一次性地确定属性的所有相关组合。相比之下,在扩展每个内部结点时,斜决策树都需要动态地确定正确的属性组合。然而,构造归纳会产生冗余的属性,因为新创建的属性是已有属性的组合。
(11)研究表明不纯性度量方法的选择对决策树算法的性能影响很小,这是因为许多度量方法相互之间都是一致的,如图4-13所示。实际上,树剪枝对最终决策树的影响比不纯性度量的选择的影响更大。4.4模型的过分拟合分类模型的误差大致分为两种:训练误差(training error)和泛化误差(generalization error训练误差也称再代入误差(resubstitution error或表现误差(apparent error),是在训练记录上误分类样本比例,而泛化误差是模型在未知记录上的期望误差。回顾4.2节,一个好的分类模型不仅要能够很好地拟合训练数据,而且对未知样本也要能准确地分类。换句话说,一个好的分类模型必须具有低训练误差和低泛化误差。这一点非常重要,因为对训练数据拟合度过高的模型,其泛化误差可能比具有较高训练误差的模型高。这种情况就是所谓的模型过分拟合。二维数据过分拟合的例子关于过分拟合问题的具体例子,考虑图4-22所示的二维数据集。数据集中的数据点属于两个类,分别标记为类“o”和类“+”,类“”的数据点由三个高斯分布混合产生,而类“+”的数据点用一个均匀分布产生。数据集中,总共有1200个数据点是属于类“”,1800个属于类“+”,其中30%的点用于训练,剩下的70%用于检验。对训练集使用以Gini指标作为不纯性度量的决策树分类法。为了研究过分拟合的影响,对初始的、完全生长的决策树进行了不同程度的剪枝。图4-23显示了决策树的训练误差和检验误差。训练集201814121086220图具有两个类的数据集的例子注意当决策树很小时训练和检验误差都很大这种情况称作模型拟合不足(model underfitting出现拟合不是的原因模型尚学习到数据的真实结构,因此,模型在训练集和检验集上的性能都很差随着法策树中结点数的增加,模型的训练误差和检验误差都会随之降低。然而,一旦树的规模变得太大,即使训练误差还在继续降低,是检验误差开始增大,这种现象称为模型过分拟哈(model overfitting)
0.4训练误差0.35检验误差….3…0.250.20.15.0.05050100150200250300结点数图4-23训练误差和检验误差为了理解过分拟合现象,注意模型的训练误差随模型的复杂度增加而降低,例如,可以扩展树的叶结点,直到它完全拟合训练数据虽然这样一棵复杂的决策树的训练误差为0,但是检验误差可能很大,因为该树可能包含这样的结点,它们偶然地拟合训练数据中某些噪声。这些结点降低了决策树的性能,因为它们不能很好的泛化到检验样本。图4-24是两棵具有不同结点数的决策树,结点数少的决策树具有较高的训练误差,但是与更复杂的树相比,它具有较低的检验误差。21253x<1329Ax1<6.5193x2<12631211x178x2<17.35<12.68x1<13.294.0621<188+x1<6.561<2.15An9x2152<17.1412.89Ax2<864x1<7241380x2<1.38Ax1<12.112<16750A<18.88163(a)包含11个叶结点的决策树(b)包含24个叶结点的决策树图4-24具有不同模型复杂度的决策树过分拟合与拟合不足是两种与模型复杂度有关的异常现象。本节余下的部分将继续讨论造成模型过分拟合的一些潜在因素。4.4.1噪声导致的过分拟合考虑表4-3和表44中哺乳动物的分类问题的训练数据集合和检验数据集合。十个训练记录中有两个被错误地标记:蝙蝠和鲸被错误地标记为非哺乳类动物,而不是哺乳类动物。
表4-3哺乳类动物分类的训练数据集样本。打星号的类标号代表错误标记的记录名称体温胎生4条腿冬眠类标号豪猪恒温猫恒温是是蝙蝠恒温否*鲸恒温否*蝾螈冷血科莫多巨蜥冷血蟒蛇冷血鲑鱼冷血是是是是否否否否否是是是否否是是否否否否是否是否是否是否否否鹰恒温否否否否否否虹鳉冷血表4-4哺乳类动物分类的检验数据集样本名称体温胎生4条腿冬眠类标号人恒温鸽子恒温象恒温豹纹鲨冷血海龟冷血企鹅冷血鳗冷血海豚恒温是否是是否否否是否否否否是否是否否否是是否否否否否否否否是是是否是否否否否是是否针鼹恒温希拉毒蜥冷血完全拟合训练数据的决策树显示在图4-2a中。虽然该树的训练误差为0,但它在检验数据上的误差高达30%人和海豚都被误分类为非哺乳类动物,因为它们在属性体温、胎生、4条腿上的属性值与训练数据中被错误标记的样本属性值相同。另一方面,针鼹是个例外,其检验记录中的类标号与训练集中相似的记录的类标号相反。例外导致的错误是不可避免的,它设定了分类器可以达到的最小错误率。体温体温恒温冷血恒温冷血非哺乳非哺乳胎生类动物类动物是非哺乳哺乳类非哺乳4条腿类动物动物类动物是否哺乳类非哺乳动物类动物(a)模型M(b)模型M2 PDG图4-25根据表4-3中的数据集建立的决策树
相反,图4-25b中决策树M2具有较低的检验误差(10%),尽管它的训练误差较高(20%)很明显,决策树M1过分拟合了训练数据,因为存在一个更简单、但在检验数据集上具有更低检验误差的模型。模型M1中的属性测试条件4条脚具有欺骗性,因为它拟合了误标记的训练记录,导致了对检验集中记录的误分类。4.4.2缺乏代表性样本导致的过分拟合根据少量训练记录做出分类决策的模型也容易受过分拟合的影响。由于训练数据缺乏具有代表性的样本,在没有多少训练记录的情况下学习算法仍然继续细化模型就会产生这样的模型。下面举例说明。考虑表4-5中的五个训练记录,表中所有的记录都是正确标记的,对应的决策树在图4-26中。尽管它的训练误差为0,但是它的检验误差却高达30%表4-5哺乳动物分类的训练集样本名称本温胎生4条腿冬眠类标号蝾螈冷血虹鳉冷血鹰恒温弱夜鹰恒温否是否否否是否否否是是否否是是否否否否是鸭嘴兽恒温体温恒温冷血非哺乳冬眠类动物是否非哺乳4条腿类动物是否哺乳类非哺乳动物类动物图4-26根据表4-5中的数据集建立的决策树人、大象和海豚都被误分类,因为决策树把恒温但不冬眠的脊柱动物划分为非哺乳动物。决策树做出这样的分类决策是因为只有一个训练记录(鹰)具有这些特性。这个例子清楚地表明,当决策树的叶结点没有足够的代表性样本时,很可能做出错误的预测。4.4.3过分拟合与多重比较过程模型的过分拟合可能出现在使用所谓的多重比较过程( multiple comparison procedure)的学习算法中。为了理解多重比较过程,考虑预测未来10个交易日股市是升还是降的任务。如果股票分析家简单地随机猜测,则对任意交易日预测正确的概率是0.5,然而,10次猜测至少正确预测8次的概率是:
 Cio+Cio Cio=0.0547这看起来不大可能。假设我们想从50个股票分析家中选择一个投资顾问,策略是选择在未来的10个交易日做出最多正确预测的分析家。该策略的缺点是,即使所有的分析家都用随机猜测做出预测,至少有一个分析家做出8次正确预测的概率是:1-(1-0.0547)=0.9399这相当高。尽管每个分析家做出8次正确预测的概率很低,但是把他们放在一起,找到一个能够做出8次正确预测的分析家的概率却很高。此外,不能保证这样的分析家以后还能通过随机猜测继续做出准确的预测。多重比较过程与模型过分拟合有什么关系呢许多学习算法都利用一个独立的候选集{},然后从中选取最大化给定标准的Ymax算法将把ymax添加到当前模型中,以提高模型的整体性能。重复这一过程,直到没有进一步的提高。例如,在决策树增长过程中,可以进行多种测试,以确定哪个属性能够最好地划分训练数据,只要观察到的改进是统计显著的,就选取导致最佳划分的属性来扩展决策树。设T是初始决策树,Tx是插入属性x的内部结点后的决策树。原则上,如果观察到的增益△(To,Tx)大于某个预先定义的阈值a,就可以将x添加到树中。如果只有一个属性测试条件,则可以通过选择足够大的阈值a来避免插入错误的结点。然而,在实践中,可用的属性测试条件不止一个,并且决策树算法必须从候选集{x1x2,x}中选择最佳属性xmax来划分数据。在这种情况下,算法实际上是使用多重比较过程来决定是否需要扩展决策树。更具体地说,这是测试△(T,T>a,而不是测试△(To,Tx)>a。随着候选个数k的增加,找到△(To,T)>a的几率也在增大。 0Xma除非根据k修改增益函数或阈值,否则算法会不经意间在模型上增加一些欺骗性的结点,导致模型过分拟合。当选择属性xmax的训练记录集很小时,这种影响就变得更加明显,因为当训练记录较少时,函数△(T,T)的方差会很大。因此,当训练记录很少时,找到△(T,T)>a的概率就增大了。决策树增长到一定深度就会经常发生这种情形,这样会降低结点所覆盖的记录数,提高了添加不必要结点的可能性。大量的候选属性和少量的训练记录最后导致了模型的过分拟合。4.4.4泛化误差估计虽然过分拟合的主要原因一直是个争辩的话题,大家还是普遍同意模型的复杂度对模型的过分拟合有影响,如图4-23所示。问题是,如何确定正确的模型复杂度?理想的复杂度是能产生最低泛化误差的模型的复杂度。然而,在建立模型的过程中,学习算法只能访问训练数据集(见图4-3),对检验数据集,它一无所知,因此也不知道所建立的决策树在未知记录上的性能。我们所能做的就是估计决策树的泛化误差。本节提供一些估计泛化误差的方法。1.使用再代入估计再代入估计方法假设训练数据集可以很好地代表整体数据因而,可以使用训练误差(又称再代入误差)提供对泛化误差的乐观估计。在这样的前提下,决策树归纳算法简单地选择产生最低训练误差的模型作为最终的模型。然而,训练误差通常是泛化误差的一种很差的估计。
例4.1考虑图4-27中的二叉决策树。假设两棵决策树都由相同的训练数据产生,并且都根据每个叶结点多数类做出分类决策。注意,左边的树T复杂一些,它扩展了右边决策树TR的某些叶结点。左决策树的训练误差是e(T)4/24=0.167,而右决策树的训练误差是e(TR)=6/24=0.25。根据再代入估计,左决策树要优于右决策树。:3+:5+24+一30+:3:6+3+2+:0+:1+3+:0一:1-:1-2-2:15决策树TL决策树TR图4-27由相同的训练数据产生的两棵决策树2.结合模型复杂度如前所述,模型越是复杂,出现过分拟合的几率就越高,因此,我们更喜欢采用较为简单的模型。这种策略与应用众所周知的奥卡姆剃刀(Occam's' razor)或节俭原则(principle of parsimony)一致。定义4.2奥卡姆剃刀:给定两个具有相同泛化误差的模型,较简单的模型比较复杂的模型更可取。奥卡姆剃刀是很直观的原则,因为复杂模型中的附加成分很大程度上是完全对偶然的拟合。用爱因斯坦的话来说,“所有事情都应该尽可能简单,但不是简化。”下面我们介绍两种把模型复杂度与分类模型评估结合在一起的方法。悲观误差评估第一种方法明确使用训练误差与模型复杂度罚项(penalty term)的和计算泛化误差。结果泛化误差可以看作模型的悲观误差估计(pessimistic error estimate)例如,设n(t是结点t分类的训练记录数,e(t)是被误分类的记录数。决策树T的悲观误差估计e(可以用下式计算: ()=( ) (t; e()+(T)(tN其中,k是决策树的叶结点数,e(T)决策树的总训练误差,N是训练记录数,()是每个结点t对应的罚项。例4.2考虑图4-27中的二决策树。如果罚项等于0.5,左边的决策树的悲观误差估计为:PDG
e()=4+7×0.57.5==0.31252424右边的决策树的悲观误差估计为:6+4×0.58e()===0.33332424这样,左边的决策树比右边的决策树具有更好的悲观误差估计。对二叉树来说,0.5的罚项意味着只要至少能够改善一个训练记录的分类结点就应当扩展,因为扩展一个结点等价于总误差增加0.5,代价比犯一个训练错误小。如果对于所有的结点t,(t)=1,左边的决策树的悲观误差估计为e(T)=11/24=0.458,右边的决策树的悲观误差估计为e(TR)=10/24=0.417。因此,右边的决策树比左边的决策树具有更好的悲观错误率。这样,除非能够减少一个以上训练记录的误分类,否则结点不应当扩展。口最小描述长度原则另一种结合模型复杂度的方法是基于称作最小描述长度(minimum description length,MDL)原则的信息论方法为了解释说明该原则,考虑图428中的例子。在该例中,A和B都是已知属性x值的给定记录集。另外,A知道每个记录的确切类标号,而B却不知道这些信息。B可以通过要求A顺序传送类标号而获得每个记录的分类。一条消息需要e(n)比特的信息,其中n是记录总数。 Yes0?BC B2A C2B8标记的未标记的X1? X2?3?4?图4-28最小描述长度(MDL)原则另一种可能是,A决定建立一个分类模型,概括x和y之间的关系。在传送给B前,模型用压缩形式编码。如果模型的准确率是100%,那么传输的代价就等于模型编码的代价。否则,A还必须传输哪些记录被模型错误分类信息。传输的总代价是: Cost(model, data)= Cost(model)+ Cost(data model)(4-9)其中,等式右边的第一项是模型编码的开销,而第二项是误分类记录编码的开销。根据MDL原则,我们寻找最小化开销函数的模型。本章习题9给出了一个如何计算决策树总描述长度的例子。3.估计统计上界泛化误差也可以用训练误差的统计修正来估计。因为泛化误差倾向于比训练误差大,所以统
计修正通常是计算训练误差的上界,考虑到达决策树一个特定叶结点的训练记录数。例如,决策树算法C4.5中,假定每个叶结点上的错误服从二项分布。为了计算泛化误差,我们需要确定训练误差的上限,在下面的例子中解释说明。例4.3考虑图4-27所示的二叉决策树的最左分支,注意,TR中的最左叶结点被扩展为T中的两个子女结点。在划分前,该结点的错误率是2=0.286。用正态分布近似二项分布,可以推导出错误率e的上界是:+(1-e)+ eupper(N, e, a) =-2N 4N2(4-10)1+2N其中,a是置信水平,o2是标准正态分布的标准化值,而N是计算e的训练记录总数将a=25%,N=7,e=2n7代入,错误率的上限是ep(7,2/,0.25)=0.503,对应于7×0.503=3.521个错误。如果我们扩展结点为T中的子女结点,子女结点的训练误差分别为1/4=0.250,1/3=0.333使用公式(4-10),错误率上限分别是upe(1/0.25)=0.537,ppe(31/3,0.25)=0.650子女结点的总训练误差是4×0.537+3×0.650=4.98,大于T中相应结点的估计误差。4.使用确认集在该方法中,不是用训练集估计泛化误差,而是把原始的训练数据集分为两个较小的子集,一个子集用于训练,而另一个称作确认集,用于估计泛化误差。典型的做法是,保留2/3的训练集来建立模型,剩余的1/3用作误差估计。该方法常常用于通过参数控制获得具有不同复杂度模型的分类技术。通过调整学习算法中的参数(如决策树中剪枝的程度),直到学习算法产生的模型在确认集上达到最低的错误率,可以估计最佳模型的复杂度。虽然该方法为评估模型在未知样本上的性能提供了较好办法,但用于训练的记录减少了。4.4.5处理决策树归纳中的过分拟合在前面章节中,我们介绍了一些估计分类模型泛化误差的方法。对于泛化误差可靠的估计能让学习算法搜索到准确的模型,而且不会对训练数据过分拟合本节介绍两种在决策树归纳上避免过分拟合的策略。先剪枝(提前终止规则)在这种方法中,树增长算法在产生完全拟合整个训练数据集的完全增长的决策树之前就停止决策树的生长。为了做到这一点,需要采用更具限制性的结束条件,例如,当观察到的不纯性度量的增益(或估计的泛化误差的改进)低于某个确定的阈值时就停止扩展叶结点。这种方法的优点在于避免产生过分拟合训练数据的过于复杂的子树,然而,很难为提前终止选取正确的阈值。阈值太高将导致拟合不足的模型,而阈值太低就不能充分地解决过分拟合的问题。此外,即便使用已有的属性测试条件得不到显著的增益,接下来的划分也可能产生较好的子树。后剪枝在该方法中,初始决策树按照最大规模生长,然后进行剪枝的步骤,按照自底向上的方式修剪完全增长的决策树。修剪有两种做法:(1)用新的叶结点替换子树,该叶结点的类标号
由子树下记录中的多数类确定;或者(2)用子树中最常使用的分支代替子树。当模型不能再改进时终止剪枝步骤。与先剪枝相比,后剪枝技倾向于产生更好的结果,因为不像先剪枝,后剪枝是根据完全增长的决策树做出的剪枝决策,先剪枝则可能过早终止决策树的生长。然而,对于后剪枝,当子树被剪掉后,生长完全决策树的额外的计算就被浪费了图4-29展示了4.3.6节Web机器人检测的简化后的决策树模型。注意根在 depth=1的子树已经用涉及属性 ImagePages的一个分支替换这种方法又称子树提升( subtree raising); depth>1且 MultiAgent=0的子树被类标号为0的叶结点替换,这种方法称作子树替换(subtree replacement); depth>1 MultiAgent=1的子树完整保留。决策树 depth=1: breadth>7: class 1 breadth<= 7: II breadth3: III ImagePages> 0.375: class 0 ImagePages<=0.375: II totalPages<= 6: class 1简化后的决策树 I totalPages>6: II breadth <=1: class 1 depth =1: breadth> 1: class IimagePages <=0.1333: class 1 width 3:子树提升 ImagePages>0.1333: magePages<=0.1333: class 1] breadth >6: class 1 1 ImagePages> 0.1333: depth >1: libreadth <=6: class MultiAgent =0: class breadth 6: class 1_--. Multilp= MultiAgent =1 II TotalTime <=361: class I totaiPages <=81: class I I TotalTime 361: class 1 II totalPages >81: class 1 deptha MultiAgent =: lidepth >2: class子树替换: depth <=2: MuitilP=1: class MultilP=0: I breadth <=6: class I lil I breadth>6: RepeatedAccess <=0. 322: classo RepeatedAccess 0.322: class 1 I MultiAgent= 1: totalPages <=81: class totalPages >81: class 1图4-29Web机器人检测决策树的后剪枝4.5评估分类器的性能4.4.4节中介绍了几种在训练过程中估计模型泛化误差的方法。估计误差有助于学习算法进行模型选择(model selection),即找到一个具有合适复杂度、不易发生过分拟合的模型。模型一旦建立,就可以应用到检验数据集上,预测未知记录的类标号。测试模型在检验集上的性能是有用的,因为这样的测量给出模型泛化误差的无偏估计。在检验集上计算出的准确率或错误率可以用来比较不同分类器在相同领域上的性能。然而,为了做到这一点,检验记录的类标号必须是已知的本节回顾一些常用的评估分类器性能的方法。4.5.1保持方法在保持(Holdout)方法中,将被标记的原始数据划分成两个不相交的集合,分别称为训练PDG集和检验集。在训练数据集上归纳分类模型,在检验集上评估模型的性能。训练集和检验集的划
分比例通常根据分析家的判断(例如,50-50,或者2/3作为训练集、1/3作为检验集)。分类器的准确率根据模型在检验集上的准确率估计。保持方法有一些众所周知的局限性。第一,用于训练的被标记样本较少,因为要保留一部分。记录用于检验,因此,建立的模型不如使用所有被标记样本建立的模型好。第二,模型可能高度依赖于训练集和检验集的构成。一方面,训练集越小,模型的方差越大,另一方面,如果训练集太大,根据用较小的检验集估计的准确率又不太可靠。这样的估计具有很宽的置信区间。最后,训练集和检验集不再是相互独立的。因为训练集和检验集来源于同一个数据集,在一个子集中超出比例的类在另一个子集就低于比例,反之亦然。4.5.2随机二次抽样可以多次重复保持方法来改进对分类器性能的估计,这种方法称作随机二次抽样(random subsampling)设acc是第i次迭代的模型准确率,总准确率是 accsubacc随机二次抽样也会遇到一些与保持方法同样的问题,因为在训练阶段也没有利用尽可能多的数据。并且,由于它没有控制每个记录用于训练和检验的次数,因此,有些用于训练的记录使用的频率可能比其他记录高很多。4.5.3交叉验证替代随机二次抽样的一种方法是交叉验证( cross-validation-)。在该方法中,每个记录用于训练的次数相同,并且恰好检验一次。为了解释该方法,假设把数据分为相同大小的两个子集,首先,我们选择一个子集作训练集,而另一个作检验集,然后交换两个集合的角色,原先作训练集的现在做检验集,反之亦然,这种方法叫二折交叉验证。总误差通过对两次运行的误差求和得到。在这个例子中,每个样本各作一次训练样本和检验样本。k折交叉验证是对该方法的推广,把数据分为大小相同的k份,在每次运行,选择其中一份作检验集,而其余的全作为训练集,该过程重复k次,使得每份数据都用于检验恰好一次。同样,总误差是所有k次运行的误差之和。k折交叉验证方法的一种特殊情况是令k=N,其中N是数据集的大小,在这种所谓留一(leave-one-out)方法中,每个检验集只有一个记录。该方法的优点是使用尽可能多的训练记录,此外,检验集之间是互斥的,并且有效地覆盖了整个数据集;该方法的缺点是整个过程重复N次,计算上开销很大,此外,因为每个检验集只有一个记录,性能估计度量的方差偏高。4.5.4自助法迄今为止,我们介绍的方法都是假定训练记录采用不放回抽样,因此,训练集和检验集都不包含重复记录。在自助( bootstrap)方法中,训练记录采用有放回抽样,即已经选作训练的记录将放回原来的记录集中,使得它等机率地被重新抽取。如果原始数据有N个记录,可以证明,平均来说,大小为N的自助样本大约包含原始数据中63.2%的纪录。这是因为一个记录被自助抽样抽取的概率是1-(1-1/N),当N充分大时,该概率逐渐逼近1-e-1=0.632。没有抽中的记录就成为检验集的一部分,将训练集建立的模型应用到检验集上,得到自助样本准确率的一个估计抽样过程重复b次,产生b个自助样本。按照如何计算分类器的总准确率,有几种不同的自助抽样法。常用的方法之一是.632自助(《.632bootstap《),它通过组合每个自助样本的准确率()和由包含所有标记样本的训练集计算
的准确率(acc)计算总准确率(acCboot): accboot=2(0.632x+0.368×acc)(4-11)4.6比较分类器的方法比较不同分类器的性能,以确定在给定的数据集上哪个分类器效果更好是很有用的。但是,依据数据集的大小,两个分类器准确率上的差异可能不是统计显著的。本节介绍一些统计检验方法,可以用来比较不同模型和分类器的性能。为了更好地解释,考虑一对分类模型MA和MB假设MA在包含30个记录的检验集上的准确率达到85%,而MB在包含5000个记录的不同检验集上达到75%的准确率。根据这些信息,MA比M好吗?上面的例子提出了涉及性能度量的统计显著性的两个关键问题。(1)尽管MA的准确率比MB高,但是它是在较小的检验集上检验的。MA的准确率的置信程度有多高?(2)可以把准确率的差解释为检验集的复合的变差吗?第一个问题与估计给定模型准确率的置信区间有关,第二个问题涉及检验观测离差的统计显著性。本节余下部分将详细考察这些问题。4.6.1估计准确度的置信区间为确定置信区间,需要建立支配准确率度量的概率分布。本节介绍一种方法,通过将分类任务用二项式实验建模来推导置信区间。二项式实验的特性如下。(1)实验由N个独立的试验组成,其中每个试验有两种可能的结果:成功或失败。(2)每个试验成功的概率p是常数。二项式实验的一个例子是统计N次抛硬币正面朝上的次数。如果X是N次试验观察到的成功次数,则X取一个特定值v的概率由均值为Np、方差为Np(1-p)的二项分布给出: P(X=)=CNP'(1-p)-例如,如果硬币是均匀的(p=0.5),抛50次硬币,正面朝上20次的概率是:P(X=20)=C30.520(1-0.5)30=0.0419如果该实验重复多次,正面朝上的期望平均次数为50×0.5=25,方差为50×0.5×0.5=12.5预测检验记录类标号的任务也可以看作是二项式实验。给定一个包含N个记录的检验集,令X是被模型正确预测的记录数,p是模型真正准确率。通过把预测任务用二项式实验建模,X服从均值为Np、方差为Np(1-p)的二项分布。可以证明经验准确率acc=xN也是均值为p,方差为p(1-p)N的二项分布(见习题12)。尽管可以用二项分布来估计acc的置信区间,但是当N充分大时,通常用正态分布来近似。根据正态分布,可以推导出acc的置信区间为:p-zacc-p≤p(-p)nZa2=1-a(4-12)其中Za12和Z1-a12分别是在置信水平(1-a)下由标准正态分布得到的上界和下界。因为标准正
态分布关于Z=0对称,于是我们有Za12=z1-a12重新整理不等式,得到p的置信区间如下 2xNxacc+ Zan+ Zan za12+4Nacc-4Nacc2(4-13)2(N+Z22)下表给出了在不同置信水平下Za12的值1-a0.990.980.950.90.80.70.5Z22.582.331.961.651.281.040.67例4.4考虑一个模型,它在100个检验记录上具有80%的准确率。在95%的置信水平下,模型的真实准确率的置信区间是什么?根据上面的表,95%的置信水平对应于Za12=1.96将它代入公式(4-13)得到置信区间在71.1%和86.%之间。下表给出了随着记录数N的增大所产生的置信区间:N20501005001005000置信0.5840.6700.7110.7630.7740.789区间-0.919-0.888-0.867-0833-0.824-0.81注意,随着N的增大,置信区间变得更加紧凑。4.6.2比较两个模型的性能考虑一对模型M1和M2,它们在两个独立的检验集D1和D2上进行评估,令n是D1中的记录数,n2是D2中的记录数。另外,假设M1在D1上的错误率为e1,M2在D2上的错误率为e2目标是检验e1与e2的观察差是否是统计显著的。假设n1和n2都充分大,e1和e2可以使用正态分布来近似。如果用d=e-e2表示错误率的观测差,则d服从均值为d2(其实际差)、方差为o2的正态分布。d的方差为:=(1-e)(1-e2)n(4-14)其中e(1-e1)n1和e2(1-e2)n2是错误率的方差。最后,在置信水平(1-a)%下,可以证明实际差d的置信区间由下式给出:d=d±za12(4-15)例4.5考虑本节开始所描述的问题。模型M在N1=30个检验记录上的错误率e1=0.15,而MB在N2=5000个检验记录上的错误率e2=0.25.错误率的观察差d=|0.15-0.25=0.1在这个例子中,我们使用双侧检验来检查d1=0还是d1≠0。错误率观察差的估计方差计算如下:2=0.15(00435000或=0.0655。把该值代入公式(4-15),我们得到在95%的置信水平下,d置信区间如下d=0.1±1.96×0.0655=0.1±0.128 PDG由于该区间跨越了值0,我们可以断言在95%的置信水平下,该观察差不是统计显著的。
在什么置信水平下,我们可以拒绝假设d1=0?为此,需要确定乙a2的值,使得d2的置信区间不会跨越值0,可以颠倒前面的计算,找出使不等式d>Z2O成立的Z2的值。代入d和的值,得到Zn2<1.527,当(1-a)<0.936时这个值第一次出现(对于双侧检验)。该结果表明在93.6%或者更低的置信水平下,我们可以拒绝原假设。4.6.3比较两种分类法的性能假设我们想用k折交叉验证的方法比较两种分类法的性能。首先,把数据集D划分为k个大小相等部分,然后,使用每种分类法,在k-1份数据上构建模型,并在剩余的划分上进行检验,这个步骤重复k次,每次使用不同的划分进行检验。令M表示分类技术L在第j次迭代产生的模型注意,每对模型M和M2在相同的划分j上进行检验。用e和e2分别表示它们的错误率,它们在第j折上的错误率之差可以记作d=eye2j。如果k充分大,则d服从于均值为d(错误率的真实差)、方差为的正态分布。与前面的方法不同,观察的差的总方差用下式进行估计:(4-16)k(k-1)其中,d是平均差。对于这个方法,我们需要用t分布计算d的置信区间:系数(-,k-可以通过两个参数(置信水平(1-a)和自由度k-1)查概率表得到。该t分布的概率表在表46中给出。例4.6假设两个分类技术产生的模型的准确率估计差的均值等于0.05,标准差等于0.002如果使用30折交叉验证方法估计准确率,则在95%置信水平下,真实准确率差为:d=0.05±1.70×0.002(4-17)因为置信区间不跨越0值,两个分类法的观察差是统计显著的。口表46t分布的概率表k-1(1-a0.900.950.9750.990.99513.086.3112.731.863.72491.892.924.306.969.921532.132.783.754.601.381.832.262.823.25141.341.762.142.622.98191.331.732.092.542.86241.321.712.062.492.80291311.702.042.462.76文献注释早期的分类系统是针对组织大量对象的集合。例如杜威数字和国会图书馆的分类系统就是为大量的图书进行分类和索引设计的。分类是在领域专家的帮助下,用人工方式进行。
很多年来,自动分类一直是热门的研究课题。对于经典统计学分类的研究有时称作判别分析(discriminant analysis),其目标是根据预测子变量集来预测对象的组成员关系。著名的典型方法就是 Fisher线性判别分析[117],它寻求产生不同类对象之间最大区分能力的数据的线性投影。许多模式识别问题也需要区分不同类的对象例子包括语音识别、手写字符识别和图像分类。对用于模式识别的分类技术的应用感兴趣的读者可以参阅Jain等[122]和 Kulkarni等[128]的综述文章,或者参阅 Bishop[107]、Duda等[114]和 Fukunaga[18]的经典模式识别的书籍。分类也是神经网络、统计学习和机器学习领域的一个主要研究课题。有关各种分类技术的深入讨论请参阅 Cherkassky和 Mulier[12]、 Hastie等[120]、 Michie等[133]和 Mitchell[136]的书籍。关于决策树归纳算法的全面评述可以在 Buntine[110]、 Moret[137]、 Murthy[138]和 Safavian等[147]的综述文章中找到。一些著名的决策树算法包括CART[108]、ID3[143]、C4.5[145]和 CHAID[125]ID3和C4.5都采用熵度量作为划分函数。对C4.5决策树算法的深入讨论请参阅 Quinlan[145]。除了解释决策树的生长和剪枝方法外, Quinlan[145]还介绍了怎样修改算法处理具有遗漏值的数据集。CART算法是 Breiman等[108]开发的,它使用Gini指标作为划分函数。 CHAID[125]在决策树生长过程中使用x2统计检验确定最佳的划分点。本章所介绍的决策树算法都假定划分条件一次只选择一个属性。斜决策树可以使用多个属性,在内部结点形成属性测试条件[121,152. Breiman等[108]提供了一个选项,可以在他们CART实现中使用属性的线性组合。归纳斜决策树的其他方法由 Heath等[121]、 Murthy等[139] Cantu--paz和 Kamath[1 Utgoff和 Brodley[152]提出。尽管斜决策树提高了决策树的表达能力,但在每个结点确定合适的测试条件在计算代价上仍然是一个挑战。另一种提高决策树表达能力,而不使用斜决策树的方法是构造归纳constructive induction)[132].该方法通过由原始属性创建复合特征,简化了学习复杂的划分函数的任务。除自顶向下方法外,其他生长决策树的策略包括 Landeweerd等[130]、 Pattipati和 Alexandridis[142]提出自底向上的方法,Kim和 Landgrebe[126]提出的双向的方法。Schuermann和 Doster[50]、Wang和Suen[154]提出了使用一种软划分标准( soft splitting criterion)来解决数据碎片问题。在这个方法中,每个记录以不同的概率指派到决策树的不同分支。模型的过分拟合是一个必须解决的重要问题,确保决策树分类器在未知记录上也有同样好的性能很多作者都讨论过模型的过分拟合问题,包括 Breiman等[08]、Schaffer[148]、 Mingers[135] Jensen和 Cohen[123]尽管噪声的存在通常被认为是产生过分拟合的主要原因之一[135,140],但是 Jensen和 Cohen[123]却认为过分拟合是因为在多比较过程中使用了不正确的假设检验。 Schapire[149]定义泛化误差是“错误分类新样本的概率”,而检验误差是“在新抽取的检验集上出错的比例。”这样,泛化误差被认为是分类器的期望检验误差。泛化误差有时也可以认为是模型的真实误差[136],即关于从训练集抽样的相同总体分布随机抽取数据点,它的期望误差。这些定义事实上是等价的,如果训练集和检验集来自相同的总体分布,这种情况在很多数据挖掘和机器学习应用中都经常遇到。奥卡姆剃刀原理通常被认为是哲学家“奥卡姆的威廉”提出的。Domingos[113告诫不能把奥卡姆剃刀误解为比较具有相似训练误差,而不是泛化误差的模型。关于决策树避免过分拟合的剪枝方法的综述由 Breslow和Aha[109]、 Esposito等[116]给出。其他典型的剪枝方法包括降低误差的剪枝[144]、悲观误差剪枝[144]、最小误差剪枝[141]、临界值剪枝[134]、代价复杂度剪枝[108]和基于误差的剪枝[145]。 Quinlan和 Rivest提出使用最小描述长度原则对决策树剪枝[146]。
 Kohavi[127]使用不同的评估方法,做了广泛的实验研究来比较性能度量,评估方法包括随机二次抽样、自助抽样和飞折交叉验证,他们的结果表明最佳的评估方法是基于10折分层的交叉验证。 Efron和 Tibshirani[115]在理论和实验上比较了交叉验证和称作632+规则的自助方法。当前的技术(如C4.5)要求整个训练数据集都能装入内存。为开发决策树归纳算法的并行和可伸缩的版本,已经做了大量工作。已提出的算法包括 Mehta等[31s、 Shafer等[151 SPRINT、Wang和 Zaniolo[153]的CMP、 Alsabti等[06]的 CLOUDS、 Gehrke等[119]的 RainForest和 Joshi等[124]的 ScalParC.关于数据挖掘的并行算法综述请参阅[129]参考文献 [106] K. Alsabti, S. Ranka, and V. Singh. CLOUDS: A Decision Tree Classifier for Large Datasets. In Proc. of the 4th Intl. Conf. on Knowledge Discovery and Data Mining, pages 2-8, New York, NY, August1998 [107]. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, U.K.,1995. [108] L. Breiman, J.. Friedman, R. Olshen, and C. J. Stone. Classification and Regression Trees. Chapman&hall, New York,1984. [109] L. A. Breslow and D. W. Aha. Simplifying Decision Trees: Survey. Knowledge Engineering Review,12(1):1-40,1997 [110] W. Buntine. Learning classification trees. In Artificial Intelligence Frontiers in Statistics, pages 182- 201. Chapman Hall, London, 1993 [111] E. Cantu-Paz and C. Kamath. Using evolutionary algorithms to induce oblique decision trees. In Proc. of the Genetic and Evolutionary Computation Conf., pages 1053-1060, San Francisco, CA, 2000. [112] V. Cherkassky and F. Mulier. Learning from Data: Concepts, Theory, and Methods. Wiley Interscience, 1998. [113] P. Domingos. The Role of Occam's Razor in Knowledge Discovery. Data Mining and Knowledge Discovery,3(4):409-425,1999 [114] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley Sons, Inc., New York, 2nd edition, 2001. [115] B. Efron and R. Tibshirani. Cross-validation and the Bootstrap: Estimating the Error Rate of a Prediction Rule. Technical report, Stanford University, 1995. [116] F. Esposito, D. Malerba, and G. Semeraro. A Comparative Analysis of Methods for Pruning Decision Trees. IEEE Trans. Pattern Analysis and Machine Intelligence, 19(5): 476-491, May 1997 [117] R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7: 179-188,1936 [118]. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, New York, 1990 [119] J. Gehrke, R. Ramakrishnan, and V. Ganti. RainForest-A Framework for Fast Decision Tree Construction of Large Datasets. Data Mining and Knowledge Discovery, 4(2/3):127-162, 2000. [120] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, Prediction. Springer, New York, 2001. [121] D. Heath, S. Kasif, and S. Salzberg. Induction of Oblique Decision Trees. In Proc. of the 13th Intl. Joint Conf. on Artificial Intelligence, pages 1002-1007, Chambery, France, August 1993 [122] A. K. Jain,. P.. Duin, and J. Mao Statistical Pattern Recognition: A Review.IEEE Tran. Patt. Anal. and Mach. Intellig., 22(1): 4-37, 2000. [123] D. Jensen and P. R. Cohen. Multiple Comparisons in Induction Algorithms. Machine Learning,38(3):309-338, March2000. [124] M. V. Joshi, G. Karypis, and V. Kumar. ScalParC: A New Scalable and Efficient Paralle Classification Algorithm for Mining Large Datasets. In Proc. of 12th Intl. Parallel Processing Symp. (IPPS/SPDP), pages 573-579, Orlando, FL, April 1998.
 [125] G. V. Kass. An Exploratory Technique for Investigating Large Quantities of Categorical Data. Applied Statistics, 29: 119-127, 1980. [126] B. Kim and D. Landgrebe. Hierarchical decision classifiers in high-dimensional and large class data IEEE Trans. on Geoscience and Remote Sensing, 29(4):518-528, 1991. [127] R. Kohavi. Study on Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. In Proc. of the 15th Intl. Joint Conf. on Artificial Intelligence, pages 1137-1145, Montreal, Canada, August 1995. [128] S.R. Kulkarni, G. Lugosi, and S. S. Venkatesh. Learning Pattern Classification-A Survey. IEEEtran.Inf. Theory,44(6):2178-2206,1998 [129] V. Kumar, M. V. Joshi, E.-H. Han, P. N Tan, and M. Steinbach. High Performance Data Mining.In High Performance Computing for Computational Science (VECPAR 2002), pages 111-125.Springer2002 [130] G. Landeweerd, T. Timmers, E. Gersema, M. Bins, and. Halic. Binary tree versus single level tree classification of white blood cells. Pattern Recognition, 16: 571-577, 1983 [131]. Mehta, R. Agrawal, and J. Rissanen. SLIQ: A Fast Scalable Classifier for Data Mining. In Proc. of the 5th Intl. Conf. on Extending Database Technology pages 18-32, Avignon, France, March 1996. [132] R. S. Michalski. theory and methodology of inductive learning. Artificial Intelligence, 20: 111-1161983 [133] D. Michie, D. J. Spiegelhalter, and C.. Taylor. Machine Learning, Neural and Statistical Classification. Ellis Horwood, Upper Saddle River, NJ, 1994. [134]. Mingers. Expert Systems-Rule Induction with Statistical Data. J Operational Research Society,38:39-47,1987. [135] J. Mingers. An empirical comparison of pruning methods for decision tree induction. Machine Learning,4:227-243,1989. [136]. Mitchell. Machine Learning. McGraw-Hill, Boston, MA, 1997. [137] B. M.. Moret. Decision Trees and Diagrams. Computing Surveys, 14(4): 593-623, 1982. [138] S. K. Murthy. Automatic Construction of Decision Trees from Data: A Multi-Disciplinary Survey. Data Mining and Knowledge Discovery, 2(4): 345-389, 1998. [139] S. K. Murthy, S. Kasif, and S. Salzberg. system for induction of oblique decision trees. J of Artificial Intelligence Research, 2: 1-33, 1994. [140] T. Niblett. Constructing decision trees in noisy domains. In Proc. of the 2nd European Working Session on Learning, pages 67-78, Bled, Yugoslavia, May 1987. [141] T. Niblett and I. Bratko. Learning Decision Rules in Noisy Domains. In Research and Development in Expert Systems III, Cambridge, 1986. Cambridge University Press. [142] K. R. Pattipati andM. G. Alexandridis Application of heuristic search and information theory to sequential fault diagnosis. IEEE Trans. on Systems, Man, and Cybernetics, 20(4): 872-887, 1990 [143] J. R. Quinlan. Discovering rules by induction from large collection of examples. In D. Michic, editor, Expert Systems in the Micro Electronic Age. Edinburgh University Press, Edinburgh, UK, 1979. [144] J. R. Quinlan. Simplifying Decision Trees. Intl. J. Man-Machine Studies, 27: 221-234,1987. [145] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan-Kaufmann Publishers, San Mateo, CA,1993 [146] J. R. Quinlan and R. L. Rivest. Inferring Decision Trees Using the Minimum Description Length Principle. Information and Computation, 80(3): 227-248, 1989. (147] S. R. Safavian and D. Landgrebe A Survey of Decision Tree Classifier Methodology. IEEE Trans. Systems, Man and Cybemnetics, 22: 660-674, May/June 1998 [148] C. Schaffer. Overfitting avoidence as bias. Machine Learning, 10: 153-178, 1993. [149] R. E. Schapire. The Boosting Approach to Machine Learning: An Overview. In MSRI Workshop o Nonlinear Estimation and Classification, 2002 [150] J. Schuermann and W. Doster. A decision-theoret approach in hierarchical classifier design. Pattern Recognition,17:359-369,1984. [151] J. C. Shafer, R. Agrawal, and M. Mehta. SPRINT: Scalable Parallel Classifier for Data Mining. In
 Proc. of the 22nd VLDB Conf., pages 544-555, Bombay, India, September 1996. [152] P. E. Utgoff and C. E. Brodley. An incremental method for finding multivariate splits for decision trees. In Proc. of the 7th Int. Conf. on Machine Learning pages 58-65, Austin, TX, June 1990. [153] H. Wang and C. Zaniolo. CMP: A Fast Decision Tree Classifier Using Multivariate Predictions. In Proc. of the 16th Intl. Conf. on Data Engineering pages 449-460, San Diego, CA, March 2000. [154]Q. R. Wang and C. Y. Suen. Large tree classifier with heuristic search and global training. IEEE Trans. on Pattern Analysis and Machine Intelligence 9(1): 91-102, 1987.习题1.为四个布尔属性A,B,C和D的奇偶函数画一棵完全决策树。可以简化该决策树吗?2.考虑表4-7中二元分类问题的训练样本。(a)计算整个训练样本集的Gini指标值。(b)计算属性顾客ID的Gini指标值。(c)计算属性性别的Gini指标值。(d)计算使用多路划分属性车型的Gini指标值。(e)计算使用多路划分属性衬衣尺码的Gini指标值。()下面哪个属性更好,性别、车型还是衬衣尺码?(g)解释为什么属性顾客ID的Gini值最低,但却不能作为属性测试条件。表4-7习题2的数据集顾客I性别车型衬衣尺码类1家用运动23456789运动运动小中中大运动加大运动加大运动运动运动10豪华11家用12家用加大34家用豪华加大1516男男男男男男女女女女男男男男女女女女女女豪华豪华17豪华小小中大大中小小中中中大18豪华19豪华20豪华3.考虑表4-8中的二元分类问题的训练样本集。(a)整个训练样本集关于类属性的熵是多少?(b)关于这些训练样本,a1和a2的信息增益是多少?(c)对于连续属性a3,计算所有可能的划分的信息增益。 POG(d)根据信息增益,哪个是最佳划分(在a1,a2和a3中)?
(e)根据分类错误率,哪个是最佳划分(在a1和a2中)?(f)根据Gini指标,哪个是最佳划分(在a1和a2中)?表48练习3的数据集实例a1a2a3目标类TT1.0TT6.0123456789TF5.0+一FF4.0FT7.0FT3.0FF8.0一T7.0+FT5.04.证明:将结点划分为更小的后继结点之后,结点熵不会增加。5.考虑如下二元分类问题的数据集。类标号TTTTTFFFTTBFTTFTFFFT++一(a)计算按照属性A和B划分时的信息增益。决策树归纳算法将会选择哪个属性?(b)计算按照属性A和B划分时Gini指标。决策树归纳算法将会选择哪个属性?(c)从图4-13可以看出熵和Gini指标在区间[0,.5]都是单调递增的,而在区间[0.5,1]都是单调递减的。有没有可能信息增益和Gini指标增益支持不同的属性?解释你的理由。6.考虑如下训练样本集。 XYZC1类样本数C2类样本数00040001500150100114510010505010125110111502015(a)用本章所介绍的贪心法计算两层的决策树使用分类错误率作为划分标准。决策树的总错误率是多少?
(b)使用X作为第一个划分属性,两个后继结点分别在剩余的属性中选择最佳的划分属性,重复步骤(a)所构造决策树的错误率是多少?(c)比较(a)和(b)的结果。评述在划分属性选择上启发式贪心法的作用。7.下表汇总了具有三个属性A,B,C,以及两个类标号十,一的数据集。建立一棵两层决策树。实例数 A BCTT二00FTFTF+50000200TTTFFTTFFTTTFFF050002F5(a)根据分类错误率,哪个属性应当选作第一个划分属性?对每个属性,给出相依表和分类错误率的增益。(b)对根结点的两个子女重复以上问题。(c)最终的决策树错误分类的实例数是多少?(d)使用C作为划分属性,重复(a)、(b)和(c)(e)使用(c)和(d)中的结果分析决策树归纳算法贪心的本质。8.考虑图4-30中的决策树。(a)使用乐观方法计算决策树的泛化错误率。(b)使用悲观方法计算决策树的泛化错误率。(为了简单起见,使用在每个叶结点增加因子0.5的方法。)(c)使用提供的确认集计算决策树的泛化误差。这种方法叫作降低误差剪枝(reduced error pruning)训练集实例AB类0002001+3010+400115100+6100+B1108010910100++确认集实例AB类11000+12101131101410115100图4-30习题8的决策树和数据集
9.考虑图4-31中的决策树。假设产生决策树的数据集包含16个二元属性三个类C1、C2和 C3o根据最小描述长度原则计算每棵决策树的总描述长度。·树的整体描述长度由下式给出: Cost(tree, data)=Cost(tree)+ Cost(data tree)·树的每个内部结点用划分属性的ID进行编码。如果有m个属性,为每个属性编码的代价是log2m个二进位。每个叶结点使用与之相关联的类的ID编码。如果有k个类,为每个类编码的代价是log2k个二进位。cost(tree)是对决策树的所有结点编码的开销。为了简化计算,可以假设决策树的总开销是对每个内部结点和叶结点编码开销的总和。Cost(data tree)是对决策树在训练集上分类错误编码的开销。每个错误用log2n个二进位编码,其中n是训练实例的总数。根据MDL原则,哪棵决策树更好?CC C2C3 C, C(a)具有7个错误的决策树(b)具有4个错误的决策树图4-31习题9的决策树10.尽管.632自助方法可以对模型的准确率做出可靠的估计,但是该方法也有明显的局限性127]。考虑一个二类的问题,其中数据包含的正样本和负样本的数目相等,假设每个样本的类标号都是随机产生的,所使用的分类器是一棵未进行剪枝的决策树(即完全忠实的写照)。使用下面的方法确定分类器的准确率。(a)保持方法,使用三分之二的数据作为训练数据,剩余的三分之一作检验数据。(b)十折交叉验证。(c).632自助方法。(d)从(a)、(b)、(c)的结果看,哪种方法对分类器的准确率提供了可靠的估计?11.考虑如下测试分类法A是否优于另一个分类法B的方法。设N是数据集的大小,PA是分类法A的准确率,PB是分类法B的准确率,而p=pa+p)2是两种分类法的平均准确率。为了测试分类法A是否显著优于B,使用如下Z统计量:
 Z=PA-PB2p(1-p)N如果Z>1.96,则认为分类法A优于分类法B表49在不同的数据集上比较了三个不同分类法的准确率:决策树分类法,朴素贝叶斯分类法和支持向量机。(后两种分类法在第5章中介绍。)表49各种分类法准确率的比较数据集大小N)决策树(%)朴素贝叶斯(%)支持向量机(%) Anneal 898 92.0979.6287.19 Australia69085.5176.8184.78 Auto20581.9558.0570.73 Breast69995.1495.9996.42 Cleve30376.2483.5084.49 Credit69085.8077.5485.07 Diabetes76872.4075.9176.82 German 100070.9074.7074.40 Glass21467.2948.5959.81 Heart27080.0084.0783.70 Hepatitis15581.9483.2387.10 Horse36885.3378.8082.61 Ionosphere35189.1782.3488.89 Iris15094.6795.3396.00 Labor5778.9594.7492.98 Led7320073.3473.1673.56 Lymphography 14877.0383.1186.49 Pima76874.3576.0476.95 Sonar20878.8569.7176.92 Tic-tac-toe95883.7270.0498.33 Vehicle84671.0445.0474.94 Wine17894.3896.6398.88 Zoo10193.0793.0796.04用下面的3×3的表格汇总表4-9中给定的分类法在数据上的分类性能:赢输平局决策树朴素贝叶斯支持向量机决策树0-0-23朴素贝叶斯0-0-23支持向量机0-0-23表格中每个单元的内容包含比较行与列的两个分类器时的赢、输和平局的数目。12.设X是一个均值为Np、方差为p(1p)的二元随机变量。证明比率XN也服从均值为p、方差为p(1-p)N的二项分布。 POG
第章分类:其他技术上一章介绍了一种简单但很有效的分类技术,称为决策树归纳。该章还详细地讨论了模型的过分拟合和分类器的评估问题本章讲述构建分类模型的其他技术—从最简单的基于规则的分类器和最近邻分类器到更高级的支持向量机和组合方法。其他重要问题,如类失衡和多类问题等,也在本章后面部分进行讨论。5.1基于规则的分类器基于规则的分类器是使用一组“if... then ..”规则来对记录进行分类的技术。表5-1的例子中给出脊椎动物分类问题的基于规则的分类器产生的一个模型。该模型的规则用析取范式R=(rr2VV)表示,其中R称作规则集,而r是分类规则或析取项。表5-1脊椎动物分类问题的规则集举例n:(胎生=否)(飞行动物=是)→鸟类r2:(胎生=否)(水生动物=是)→鱼类3:(胎生=是)(体温=恒温)→哺乳类r4:(胎生=否)人(飞行动物=否)→爬行类r:(水生动物=半)→两栖类每一个分类规则可以表示为如下形式:r(条件→y(5-1)规则左边称为规则前件(rule antecedent)或前提(precondition)它是属性测试的合取:条件=(A1opv)(A2opv2)…( op)(5-2)其中(A,)是属性一值对,op是比较运算符,取自集合{=,≠<,>,≤,≥}。每一个属性测试(Aop称为一个合取项。规则右边称为规则后件(rule consequent),包含预测类yio如果规则r的前件和记录x的属性匹配,则称r覆盖x当r覆盖给定的记录时,称r被激发或被触发。作为例子,我们考虑表5-1中的规则r1和两种脊椎动物鹰和灰熊的以下属性:名称体温表皮覆盖胎生水生动物飞行动物有腿冬眠鹰恒温羽毛否灰熊恒温软毛是否否是是否否是是r1覆盖第一种脊椎动物,因为鹰的属性满足它的前件r1不覆盖第二种脊椎动物,因为灰熊是胎生的且不能飞,故而违背了r1的前件。分类规则的质量可以用覆盖率(coverage)和准确率(accuracy)来度量给定数据集D和
分类规则r:A→y,规则的覆盖率定义为D中触发规则r的记录所占的比例。另一方面,准确率或置信因子定义为触发r的记录中类标号等于y的记录所占的比例。这两个度量的形式化定义如下: Coverage(r)=A Accuracy(r)= Any(5-3)其中A是满足规则前件的记录数,Any是同时满足规则前件和后件的记录数,D是记录总数。例5.1考虑表5-2中的数据集。规则胎生=是)(体温=恒温)→哺乳类的覆盖率是33%,因为15个记录中有5个满足规则前件。该规则的准确率是100%,因为规则覆盖的五个脊椎动物都是哺乳类。表5-2脊椎动物数据集名字体温表皮覆盖胎生水生动物飞行动物有腿冬眠类标号人类恒温毛发哺乳类蟒蛇冷血鳞片爬行类鲑鱼冷血鳞片鱼类鲸恒温毛发否哺乳类青蛙冷血无两栖类巨蜥冷血鳞片是否爬行类蝙蝠恒温毛发鸽子恒温羽毛猫恒温软毛虹鳞冷血鳞片美洲鳄冷血鳞片企鹅恒温羽毛否是否否是否是是否否是否否是是半否否否否是半半否是半否否否否是是否否否否否否否否否是是是是是否是是是否是否否是否是否否否否否是否是哺乳类鸟类哺乳类鱼类爬行类鸟类豪猪恒温刚毛哺乳类鳗鲡冷血鳞片鱼类蝾螈冷血无两栖类5.1.1基于规则的分类器的工作原理基于规则的分类器根据测试记录所触发的规则来对记录进行分类。为了说明一个基于规则的分类器是怎样工作的,考虑表5-1所示的规则集和下面的脊椎动物:名字体温表皮覆盖胎生水生动物飞行动物有腿冬眠狐猴恒温软毛否否是海龟冷血鳞片是否是半否否角鲛鲨冷血鳞片是否是是否否·第一个脊椎动物——狐猴,是恒温动物,能生育幼仔。这触发规则r3,因此归为哺乳类。·第二个脊椎动物海龟,同时触发规则r4和r5。由于两个规则预测的类别相互冲突(爬行类和两栖类),它们的冲突类别必须得到解决。·没有规则可以用来分类角鲛鲨。在这种情况下,即使测试记录不被规则覆盖,我们需要
确保分类器仍能对记录做出可靠的预测。上面的例子表明基于规则的分类器所产生的规则集的两个重要性质。互斥规则( Mutually Exclusive Rule)如果规则集R中不存在两条规则被同一条记录触发,则称规则集R中的规则是互斥的。这个性质确保每条记录至多被R中的一条规则覆盖。表5-3是一个互斥规则集的例子。穷举规则(Exhaustive Rule)如果对属性值的任一组合,R中都存在一条规则加以覆盖,则称规则集R具有穷举覆盖。这个性质确保每一条记录都至少被R中的一条规则覆盖。假设体温和胎生是二元变量,则表5-3中的规则集具有穷举覆盖。表5-3一个互斥和穷举的规则集的例子:(体温=冷血)→非哺乳类r2:(体温=恒温)入(胎生=是)→哺乳类(体温=恒温)(胎生=否)→非哺乳类这两个性质共同作用,保证每一条记录被且仅被一条规则覆盖。然而,很多基于规则的分类器,包括表5-1中所示的分类器,都不满足这两个性质。如果规则集不是穷举的,那么必须添加一个默认规则r→ya来覆盖那些未被覆盖的记录。默认规则的前件为空,当所有其他规则失效时触发。ya是默认类,通常被指定为没有被现存规则覆盖的训练记录的多数类。如果规则集不是互斥的,那么一条记录可能被多条规则覆盖,这些规则的预测可能会相互冲突。解决这个问题有如下两种方法。有序规则(ordered rule)在这种方法中,规则集中的规则按照优先级降序排列,优先级的定义有多种方法(如基于准确率、覆盖率、总描述长度或规则产生的顺序等)。有序的规则集也称为决策表(decision list)当测试记录出现时,由覆盖记录的最高秩的规则对其进行分类,这就避免由多条分类规则来预测而产生的类冲突的问题。无序规则(unordered rule)这种方法允许一条测试记录触发多条分类规则,把每条被触发规则的后件看作是对相应类的一次投票然后计票确定测试记录的类标号。通常把记录指派到得票最多的类。在某些情况下,投票可以用规则的准确率加权。使用无序规则来建立基于规则的分类器有利也有弊。首先,无序规则方法在分类一个测试记录时,不易受由于选择不当的规则而产生的错误的影响(而基于有序规则的分类器则对规则排序方法的选择非常敏感)。其次,建立模型的开销也相对较小,因为不必维护规则的顺序。然而,对测试记录进行分类却是一件很繁重的任务,因为测试记录的属性要与规则集中的每一条规则的前件作比较。在本节的剩余部分,我们将重点讨论使用有序规则的基于规则的分类器。5.1.2规则的排序方案对规则的排序可以逐条规则进行或者逐个类进行,图5-1给出两种方案的区别。基于规则的排序方案这个方案依据规则质量的某种度量对规则排序这种排序方案确保每一个测试记录都是由覆盖它的“最好的”规则来分类。该方案的潜在缺点是规则的秩越低越难解释,因为每个规则都假设所有排在它前面的规则不成立。例如,图5-1左图基于规则的排序中第
四条规则(水生动物=半)→两栖类有如下解释:如果一种脊椎动物没有羽毛或不能飞,并且是冷血的和半水生的,那么它属于两栖类。附加条件(脊椎动物没有羽毛或不能飞,并且是冷血的)是因为该脊椎动物不满足前三条规则。如果规则的数量很大,则解释处在列表尾部的规则将是一件非常麻烦的任务。基于类的排序方案在这种方案中,属于同一个类的规则在规则集R中一起出现。然后,这些规则根据它们所属的类信息一起排序。同一个类的规则之间的相对顺序并不重要,只要其中一个规则被激发,类标号就会赋给测试记录这使得规则的解释稍微容易一些。然而,质量较差的规则可能碰巧预测较高秩的类,从而导致高质量的规则被忽略。基于规则的排序基于类的排序(表皮覆盖=羽毛,飞行动物=是)=>鸟类(表皮覆盖=羽毛,飞行动物=是)=>鸟类(体温=恒温,胎生=是)=>哺乳类(体温=恒温,胎生=否)=>鸟类(体温=恒温,胎生=否)=>鸟类(体温=恒温,胎生=是)=>哺乳类(水生动物=半)=>两栖类(水生动物=半)=>两栖类(表皮覆盖=鳞片,水生动物=否)=>爬行类(表皮覆盖=无)=>两栖类(表皮覆盖=鳞片,水生动物=是)=鱼类(表皮覆盖=鳞片,水生动物=否)=>爬行类(表皮覆盖=无)=>两栖类(表皮覆盖=鳞片,水生动物=是)=>鱼类图5-1基于规则的排序方案和基于类的排序方案的比较由于大多数著名的基于规则的分类器(如C4.5规则和 RIPPER)都采用基于类的排序方案,本节剩余部分的讨论将主要集中在这种排序方案上。5.1.3如何建立基于规则的分类器为了建立基于规则的分类器,需要提取一组规则来识别数据集的属性和类标号之间的关键联系。提取分类规则的方法有两大类:(1)直接方法,直接从数据中提取分类规则;(2)间接方法,从其他分类模型(如决策树和神经网络)中提取分类规则。直接方法把属性空间分为较小的子空间,便于属于一个子空间的所有记录可以使用一个分类规则进行分类。间接方法使用分类规则为较复杂的分类模型提供简洁的描述。5.1.4节和5.1.5节分别对这两种方法进行详细讨论。5.1.4规则提取的直接方法顺序覆盖( sequential covering)算法经常被用来直接从数据中提取规则,规则基于某种评估度量以贪心的方式增长。该算法从包含多个类的数据集中一次提取一个类的规则。对于脊椎动物分类问题,顺序覆盖算法可能先产生对鸟类进行分类的规则,然后依次是哺乳类、两栖类、爬行类,最后是鱼类的分类规则(见图5-1)。决定哪一个类规则最先产生的标准取决于多种因素,如类的普遍性(即训练记录中属于特定类的记录的比例),或者给定类中误分类记录的代价。算法5.1给出顺序覆盖算法的描述。算法开始时决策表R为空。接下 Learn-来用函数-one-rule
提取类y的覆盖当前训练记录集的最佳规则。提取规则时,类y的所有训练记录被看作是正例,而其他类的训练记录则被当成反例。如果一个规则覆盖大多数正例,没有或仅覆盖极少数反例,那么该规则是可取的。一旦找到这样的规则就删掉它所覆盖的训练记录,并把新规则追加到决策表R的尾部。重复这个过程,直到满足终止条件。然后,算法继续产生下一个类的规则。算法5.1顺序覆盖算法1:令E是训练记录,A是属性-值对的集合{(Av)}2:令Y是类的有序集{y1,y2,y}3:令R=}是初始规则列表4:for每个类y∈yo-{y}do5: while终止条件不满足do 6:+Learn-One-Rule(E,A, y)7:从E中删除被r覆盖的训练记录8:追加r到规则列表尾部:RRvr 9: end while 10: end for11:把默认规则{}→y插入到规则列表R尾部图5-2演示在包含一组正例和反例的数据集上顺序覆盖算法是怎样工作的。规则R1首先被提取出来,(覆盖如图5-2b所示)因为它覆盖的正例最多。接下来去掉R1覆盖的所有训练记录,算法继续寻找下一个最好的规则,即R2。+++一一+一+++++++一+(a)原始数据(b)第一步 R1 R1+〤+++++++ R2+(c)第二步(d)第三步图5-2顺序覆盖算法示例1. Learn--one-rule函数 Learn--one-rule函数的目标是提取一个分类规则,该规则覆盖训练集中的大量正例,没有或
仅覆盖少量反例。然而,由于搜索空间呈指数大小,要找到一个最佳的规则的计算开销很大。 Learn--One-Rule函数通过以一种贪心的方式的增长规则来解决指数搜索问题。它先产生一个初始规则r,并不断对该规则求精,直到满足某种终止条件为止。然后,修剪该规则,以改进它的泛化误差。规则增长策略常见的分类规则增长策略有两种:从一般到特殊和从特殊到一般。在从一般到特殊的策略中,先建立一个初始规则r:{}y,其中左边是一个空集,右边包含目标类。该规则的质量很差,因为它覆盖训练集中的所有样例。接着加入新的合取项来提高规则的质量。图5-3a显示脊椎动物分类问题的从一般到特殊的规则增长策略。合取项体温=恒温首先被选择作为规则的前件。算法接下来探查所有可能的候选,并贪心地选择下一个合取项胎生=是,将其添加到规则的前件中。继续该过程,直到满足终止条件为止(例如,加入的合取项已不能提高规则的质量)对于从特殊到一般的策略,可以随机地选择一个正例作为规则增长的初始种子。在求精步,通过删除规则的一个合取项,使其覆盖更多的正例来泛化规则。图5-3b给出了脊椎动物分类问题的从特殊到一般的方法。假设选择哺乳类的一个正例作为初始种子。初始规则与种子的属性值包含相同的合取项。为了提高覆盖率,删除合取项冬眠=否,以泛化规则。重复求精步,直到满足终止条件为止,例如,当规则开始覆盖反例时停止。由于规则的贪心的方式增长,以上方法可能会产生次优规则。为了避免这种问题,可以采用束状搜索(beam search)算法维护k个最佳候选规则,各候选规则各自在其前件中添加或删除合取项而独立地增长。评估候选规则的质量,选出k个最佳候选进入下一轮迭代。}=>哺乳类表皮覆盖=毛体温=恒温有腿=否发=>哺乳类>哺乳类>哺乳类体温=恒温,有腿=是体温=恒温,胎生=是>哺乳类=>哺乳类(a)从一般到特殊体温=恒温,表皮覆盖=毛发,胎生=是,水生动物=否,飞行动物=否,有腿=是,冬眠=否=>哺乳类表皮覆盖=毛发,胎生=是,水生体温=恒温,表皮覆盖=毛发,胎动物=否,飞行动物=否,有腿=生=是,水生动物=否,飞行动物是,冬眠=否>哺乳类=否,有腿=是=>哺乳类(b)从特殊到一般图5-3从一般到特殊和从特殊到一般的规则增长策略 PDG
规则评估在规则增长过程中,需要一种评估度量来确定应该添加(或删除)哪个合取项准确率就是一个很明显的选择,因为它明确地给出了被规则正确分类的训练样例的比例。然而,把准确率作为标准的一个潜在的局限性是它没有考虑规则的覆盖率。例如,考虑一个训练集,它包含60个正例和100个反例。假设有如下两个候选规则。规则r1:覆盖50个正例和5个反例,规则r2:覆盖2个正例和0个反例。r1和r2的准确率分别为90.9%和100%。然而,r1是较好的规则,尽管其准确率较低。r2的高准确率具有潜在的欺骗性,因为它的覆盖率太低了。下面的方法可以用来处理该问题。(1)可以使用统计检验剪除覆盖率较低的规则。例如,我们可以计算下面的似然比(likelihood ratio)统计量: R=, log(f,e,)其中,k是类的个数,f是被规则覆盖的类i的样本的观测频率,e是规则作随机猜测的期望频率。注意R是满足自由度为k-1的x2分布。较大的R值说明该规则做出的正确预测数显著地大于随机猜测的结果。例如,由于r1覆盖55个样例,则正类的期望频率为e+=5560/160=20.625,而负类的期望频率为e-=55×100/160=34.375。因此r1的似然比为:R(r1)=2[50xlog2(50/20.625)+5xlog2(5/34.375)]=99.9同理,r2的期望频率分别为e=260/160=0.5和e=2100/160=1.25.r2的似然比统计量为:R(r2)=2x[2xlog2(2/0.75)+0xlog2(0/1.25)]=5.66因此,该统计量显示规则r比r2好。(2)可以使用一种考虑规则覆盖率的评估度量。考虑如下评估度量: Laplace=f+1(5-4) n+km估计=f+k+(5-5) n+k其中n是规则覆盖的样例数,f是规则覆盖的正例数,k是类的总数,p是正类的先验概率。注意,当p+=1/k时,m估计 Laplace等价于度量由于规则的覆盖率,这两个度量达到了准确率和正类先验概率之间的平衡。如果规则不覆盖任何训练样例,那么 Laplace度量减小到1/k,该值等于类符合均匀分布时正类的先验概率。当n=0时,m估计也降到先验概率(p+)。然而,当规则的覆盖率很高时,两个度量都渐近地趋向于规则的准确率fn。回到前面的例子,r1的 Laplace度量为51/57=89.47%,很接近它的准确率相反,r2的 Laplace度量(75%)比它的准确率小很多,这是因为r2的覆盖率太小了。(3)另一种可以使用的评估度量是考虑规则的支持度计数的评估度量。FOL信息增益(o's' information gain)就是一种这样的度量。规则的支持度计数对应于它所覆盖的正例数。假设规则r:A→+覆盖po个正例和n个反例。增加新的合取项B,扩展后的规则r:A^B→+覆
盖p1个正例和n1个反例。根据以上信息,扩展后规则的FO信息增益定义为:+, log2-Po Pfo信息增益1og2p1+n1+no(5-6)由于该度量与p1和p1(1+n1)成正比,所以它更倾向于选择那些高支持度计数和高准确率的规则。上例中r1和r2的FOL信息增益分别为63.8和2.83,因此规则r比r2好。规则剪枝可以对 Learn--one-rule函数产生的规则进行剪枝,以改善它们的泛化误差。为了确定是否需要进行剪枝,我们可以使用4.4节所介绍的方法来估计规则的泛化误差。例如,如果剪枝后,确认集上的误差减少了,那么就保持简化后的规则。另一种方法是比较剪枝前后规则的悲观误差(见4.4.4节)。如果剪枝后改进了悲观误差,就用简化后的规则替换原规则。2.顺序覆盖基本原理规则提取出来后,顺序覆盖算法必须删除该规则所覆盖的所有正例和反例。下面的例子给出了这样做的理由。 R2类=+类=图5-4在顺序覆盖算法中删除训练记录。R1,R2和R3分别代表三个不同规则所覆盖的区域图5-4显示了从包含29个正例和21个反例的数据集中提取的三个可能的规则R1、R2和R3R1、R2和R3的准确率分别是12/15(80%)、7/10(70%)和8/12(66.7%)。R1先产生,因为它的准确率最高。R1产生后,很明显需要删除它所覆盖的所有正例,以便算法产生的下一条规则不同于R1.下一步,假设算法可以产生R2和R3尽管R2的准确率比R3高,但是R1和R3一起覆盖了18个正例和5个反例(总体准确率达到78.3%),而R1和R2一起覆盖了19个正例和6个反例(总体准确率只有76%)。如果在计算准确率之前就删除R1所覆盖的正例和反例,那么R2或R3对准确率的这种增量影响就会更明显。具体地说,如果不删除R1所覆盖的正例,那么我们就会高估R3的准确率;如果不删除R所覆盖的反例,则会低估R3的准确率。在后一种情况下,我们最终可能会选择规则R2,尽管R3所造成的虚假正例误差有一半已经被先前的规则R1所解决。3. RIPPER算法为了阐明规则提取的直接方法,考虑一种广泛使用的规则归纳算法,叫作 RIPPER算法。该算法的复杂度几乎线性地随训练样例的数目增长,并且特别适合为类分布不平衡的数据集建立模型。 RIPPER也能很好地处理噪声数据集,因为它使用一个确认数据集来防止模型过分拟合。对两类问题, RIPPER算法选择以多数类作为默认类,并为预测少数类学习规则。对于多类
问题,先按类的频率对类进行排序,设(y1,y2,,y)是排序后的类,其中y1是最不频繁的类,而y是最频繁的类。在第一次迭代中,把属于y1样例标记为正例,而把其他类的样例标记为反例,使用顺序覆盖算法产生区分正例和反例的规则。接下来, RIPPER提取区分y2和其他类的规则。重复该过程,直到剩下类yc,此时y作为默认类。规则增长 RIPPER算法使用从一般到特殊的策略进行规则增长,使用FOIL信息增益来选择最佳合取项添加到规则前件中。当规则开始覆盖反例时,停止添加合取项。新规则根据其在确认集上的性能进行剪枝。计算下面的度量来确定规则是否需要剪枝:(p-n)(p+n),其中p和n分别是被规则覆盖的确认集中的正例和反例数目,关于规则在确认集上的准确率,该度量是单调的。如果剪枝后该度量值增加,那么就去掉该合取项。剪枝是从最后添加的合取项开始的。例如,给定规则ABCD→y, RIPPER算法先检查D是否应该剪枝,然后是CD、BCD等。尽管原来的规则仅覆盖正例,但是剪枝后的规则可能会覆盖训练集中的一些反例。建立规则集规则生成后,它所覆盖的所有正例和反例都要被删除。只要该规则不违反基于最小描述长度原则的终止条件,就将它添加到规则集中。如果新规则把规则集的总描述长度增加了至少d个比特位,那么 RIPPER就停止把该规则加入到规则集(默认的d是64位) RIPPER使用的另一个终止条件是规则在确认集上的错误率不超过50%。 RIPPER算法也采用其他的优化步骤来决定规则集中现存的某些规则能否被更好的规则替代。对优化方法的细节感兴趣的读者可以查阅本章后面提到的参考文献。5.1.5规则提取的间接方法本节介绍一种由决策树生成规则集的方法。原则上,决策树从根结点到叶结点的每一条路径都可以表示为一个分类规则。路径中的测试条件构成规则前件的合取项,叶结点的类标号赋给规则后件。图5-5显示了一个由决策树生成规则集的例子。注意,规则集是完全的,包含的规则是互斥的。但是,如下面的例子所示,其中某些规则可以加以简化。 NoYes规则集 NoYes NoYesr1:(pnoqno)==>- r2: (P=No,Q=Yes)==>+ r3: (P=Yes, R=No)==>+ No/Yes r4: (P=Yes, R=Yes, Q=No)==>- r5: (P=Yes, R=Yes,Q=Yes)==>+图5-5把决策树转化为分类规则例5.2考虑图5-5中的以下三个规则:r2:(p=no)(q=yes)→+r3:(p=yes)(R=no)→+ 5: (=Yes)^(=Yes)^(=Yes)-+
观察到,当Q的值是Yes时,规则集总是预测正类。因此,可以把这些规则简化为:r2:(q=yes)+r3:(=yes)(=no)→+保留r3来覆盖正类的剩余样例。尽管简化后的规则不再是互斥的,但它们比较简单并易于解释。口下面,介绍C4.5规则算法所采用的从决策树生成规则集的方法。图5-6给出了表5-2中的数据集对应的决策树及生成的分类规则。基于规则的分类器:胎生(胎生=否,飞行动物=是)=>鸟类胎生=否,水生动物=是)鱼类是否(胎生=是)=>哺乳类哺乳类水生(胎生=否,飞行动物=否,水生动物=否)=>爬行类动物0=>两栖类是否半鱼类两栖类飞行动物是鸟类爬行类图5-6脊椎动物分类问题的决策树生成的分类规则规则产生决策树中从根结点到叶结点的每一条路径都产生一条分类规则给定一个分类规则rA→y,考虑简化后的规则r:A→y,其中A是从A去掉一个合取项后得到的。只要简化后的规则的误差率低于原规则的误差率,就保留其中悲观误差率最低的规则。重复规则剪枝步骤,直到规则的悲观误差不能再改进为止。由于某些规则在剪枝后会变得相同,因此必须丢弃重复规则。规则排序产生规则集后,C4.5规则算法使用基于类的排序方案对提取的规则定序。预测同一个类的规则分到同一个子集中。计算每个子集的总描述长度,然后各类按照总描述长度由小到大排序。具有最小描述长度的类优先级最高,因为期望它包含最好的规则集。类的总描述长度等于 FLexceptiong× Lmodel其中 Lexception是对误分类样例编码所需的比特位数, Lmodel是对模型编码所需要的比特位数,而g是调节参数,默认值为0.5。调节参数的值取决于模型中冗余属性的数量,如果模型含有很多冗余属性,那么调节参数的值会很小。5.1.6基于规则的分类器的特征基于规则的分类器有如下特点。·规则集的表达能力几乎等价于决策树,因为决策树可以用互斥和穷举的规则集表示。基于规则的分类器和决策树分类器都对属性空间进行直线划分,并将类指派到每个划分。 PDG然而,如果基于规则的分类器允许一条记录触发多条规则的话,就可以构造一个更加复
杂的决策边界。·基于规则的分类器通常被用来产生更易于解释的描述性模型,而模型的性能却可与决策树分类器相媲美。·被很多基于规则的分类器(如 RIPPER)所采用的基于类的规则定序方法非常适于处理类分布不平衡的数据集。5.2最近邻分类器图4-3中显示的分类框架包括两个步骤:(1)归纳步,由训练数据建立分类模型;(2演绎步,把模型应用于测试样例。决策树和基于规则的分类器是积极学习方法(eager learner)的例子,因为如果训练数据可用,它们就开始学习从输入属性到类标号的映射模型。与之相反的策略是推迟对训练数据的建模,直到需要分类测试样例时再进行。采用这种策略的技术被称为消极学习方法(lazy learner)。消极学习的一个例子是ote分类器(Rote classifier),它记住整个训练数据,仅当测试实例的属性和某个训练样例完全匹配时才进行分类。该方法一个明显的缺点是有些测试记录不能被分类,因为没有任何训练样例与它们相匹配。使该方法更灵活的一个途径是找出和测试样例的属性相对接近的所有训练样例这些训练样例称为最近邻(nearest neighbor),可以用来确定测试样例的类标号。使用最近邻确定类标号的合理性用下面的谚语最能说明:“如果走像鸭子,叫像鸭子,看起来还像鸭子,那么它很可能就是一只鸭子。”最近邻分类器把每个样例看作d维空间上的一个数据点,其中d是属性个数。给定一个测试样例,我们使用2.4节中介绍的任意一种邻近性度量,计算该测试样例与训练集中其他数据点的邻近度。给定样例乙的k最近邻是指和距离最近的k个数据点。图5-7给出了位于圆圈中心的数据点的1最近邻、2最近邻和3最近邻该数据点根据其近邻的类标号进行分类。如果数据点的近邻中含有多个类标号,则将该数据点指派到其最近邻的多数类。在图5-7a中,数据点的1-最近邻是一个负例,因此该点被指派到负类。如果最近邻是三个,如图5-7c所示,其中包括两个正例和一个负例,根据多数表决方案,该点被指派到正类。在最近邻中正例和负例个数相同的情况下(见图5-7b),可随机选择一个类标号来分类该点。++++++++(a)1-最近邻(b)2最近邻(c)3-最近邻图5-7一个实例的1-最近邻、2-最近邻和3-最近邻前面的讨论中强调了选择合适的k值的重要性。如果k太小,则最近邻分类器容易受到由于训练数据中的噪声而产生的过分拟合的影响相反,如果k太大,最近邻分类器可能会误分类测试样例,因为最近邻列表中可能包含远离其近邻的数据点(见图5-8)
++++图5-8k较大时的k-最近邻分类5.2.1算法算法5.2是对最近邻分类方法的一个高层描述。对每一个测试样例z=(x,y),算法计算它和所有训练样例(x,y)∈D之间的距离(或相似度),以确定其最近邻列表D2。如果训练样例的数目很大,那么这种计算的开销就会很大。然而,高效的索引技术可以降低为测试样例找最近邻时的计算量。算法5.2k-最近邻分类算法1:令k是最近邻数目,D是训练样例的集合2:for每个测试样例z=(x,y)do3:计算乙和每个样例(x,y)∈D之间的距离d(x,x)4:选择离最近的k个训练样例的集合D2D 5: y' argmax (,; ) (v yi) 6: end for一旦得到最近邻列表,测试样例就会根据最近邻中的多数类进行分类:多数表决:y '=argmax∑(v=y)(5-7)其中,v是类标号,y是一个最近邻的类标号,I()是指示函数,如果其参数为真,则返回1,否则,返回0在多数表决方法中,每个近邻对分类的影响都一样,这使得算法对k的选择很敏感,如图5-7所示。降低k的影响的一种途径就是根据每个最近邻x距离的不同对其作用加权:wi=1/d(x,x)2结果使得远离乙的训练样例对分类的影响要比那些靠近z的训练样例弱一些。使用距离加权表决方案,类标号可以由下面的公式确定:距离加权表决:y=argmax∑×(v=y)(5-8)(;.);5.2.2最近邻分类器的特征最近邻分类器的特点总结如下。最近邻分类属于一类更广泛的技术,这种技术称为基于实例的学习,它使用具体的训练实例进行预测,而不必维护源自数据的抽象(或模型)。基于实例的学习算法需要邻近性
度量来确定实例间的相似性或距离,还需要分类函数根据测试实例与其他实例的邻近性返回测试实例的预测类标号。·像最近邻分类器这样的消极学习方法不需要建立模型,然而,分类测试样例的开销很大,因为需要逐个计算测试样例和训练样例之间的相似度。相反,积极学习方法通常花费大量计算资源来建立模型,模型一旦建立,分类测试样例就会非常快。最近邻分类器基于局部信息进行预测,而决策树和基于规则的分类器则试图找到一个拟合整个输入空间的全局模型。正是因为这样的局部分类决策,最近邻分类器(k很小时)对噪声非常敏感。最近邻分类器可以生成任意形状的决策边界,这样的决策边界与决策树和基于规则的分类器通常所局限的直线决策边界相比,能提供更加灵活的模型表示。最近邻分类器的决策边界还有很高的可变性,因为它们依赖于训练样例的组合。增加最近邻的数目可以降低这种可变性。除非采用适当的邻近性度量和数据预处理,否则最近邻分类器可能做出错误的预测。例如,我们想根据身高(以米为单位)和体重(以磅为单位)等属性来对一群人分类。属性高度的可变性很小,从1.5米到1.85米,而体重范围则可能是从90磅到250磅。如果不考虑属性值的单位,那么邻近性度量可能就会被人的体重差异所左右。5.3贝叶斯分类器在很多应用中,属性集和类变量之间的关系是不确定的。换句话说,尽管测试记录的属性集和某些训练样例相同,但是也不能正确地预测它的类标号。这种情况产生的原因可能是噪声,或者出现了某些影响分类的因素却没有包含在分析中。例如,考虑根据一个人的饮食和锻炼的频率来预测他是否有患心脏病的危险尽管大多数饮食健康、经常锻炼身体的人患心脏病的机率较小,但仍有人由于遗传、过量抽烟、酗酒等其他原因而患病。确定一个人的饮食是否健康、体育锻炼是否充分也是需要论证的课题,这反过来也会给学习问题带来不确定性。本节将介绍一种对属性集和类变量的概率关系建模的方法。首先介绍贝叶斯定理(Bayes theorem),它是一种把类的先验知识和从数据中收集的新证据相结合的统计原理;然后解释贝叶斯定理在分类问题中的应用,接下来描述贝叶斯分类器的两种实现:朴素贝叶斯和贝叶斯信念网络。5.3.1贝叶斯定理考虑两队之间的足球比赛:队0和队1。假设65%的比赛队0胜出,剩余的比赛队1获胜。队0获胜的比赛中只有30%是在队1的主场,而队1取胜的比赛中75%是主场获胜。如果下一场比赛在队1的主场进行,哪一支球队最有可能胜出呢?这个问题可以由著名的贝叶斯定理来解答。为了完整起见,我们先讲一下概率论中的一些基本定义。假设X,是一对随机变量,它们的联合概率P(=x,y=y)是指X取值x且Y取值y的概率,条件概率是指一随机变量在另一随机变量取值已知的情况下取某一特定值的概率。例如,条件概率P(Y=yX=x)是指在变量X取值x的情况下,变量Y取值y的概率X和Y的联合概率和条件概率满足如下关系:
 P(X,)=P() P() P(X)()(5-9)调整公式(5-9)最后两个表达式得到下面公式,称为贝叶斯定理: P(Y|)= P(X |)P(Y)(5-10) P(X)贝叶斯定理可以用来解决本节开头的预测问题。为表述方便,用随机变量X代表东道主,随机变量Y代表比赛的胜利者。X和Y可在集合{0,1}中取值。那么问题中给出的信息可总结如下队0取胜的概率是P(Y=0)=0.65,队1取胜的概率是P(Y=1)=1-P(Y=0)=0.35,队1取胜时作为东道主的概率是P(X=1|Y=1)=0.75,队0取胜时队1作为东道主的概率是P(X=1|Y=0)=0.3我们的目的是计算P(Y=1|X=1),即队1在主场获胜的概率,并与P(Y=0X=1)比较。应用贝叶斯定理得到:P(y=1x=1)=P(x=1|y=1)P(=1)P(X=1)p(x=1y=1)(y=1P(=1,y=1)+p(x=1,y=0)P(X=1y=1)P(y=1)=(x=1y=1)P(y=1)+P(x=1|y=0)P(Y=0)0.75×0.350.75×0.35+0.3×0.65=0.5738其中,在第二行中应用了全概率公式。进一步,P(Y=0X=1)=1-P(Y=1X=1)=0.4262。因为P(Y=1X=1)>P(y=0x=1),所以,队1更有机会赢得下一场比赛。5.3.2贝叶斯定理在分类中的应用在描述贝叶斯定理怎样应用于分类之前,我们先从统计学的角度对分类问题加以形式化。设表示属性集,Y表示类变量。如果类变量和属性之间的关系不确定,那么我们可以把X和Y看作随机变量,用P(Y)以概率的方式捕捉二者之间的关系。这个条件概率又称为Y的后验概率(posterior probability),与之相对地,P()为Y的先验概率(prior probability)在训练阶段,我们要根据从训练数据中收集的信息,对X和Y的每一种组合学习后验概率(知道这些概率后,通过找出使后验概率P(最大的类Y可以对测试记录X进行分类。为了解释这种方法,考虑任务:预测一个贷款者是否会拖欠还款。图5-9中的训练集有如下属性:有房、婚姻状况和年收入。拖欠还款的贷款者属于类yes,还清贷款的贷款者属于类o假设给定一测试记录有如下属性集:=(有房=否,婚姻状况=已婚,年收入=$120K)。要分类该记录,我们需要利用训练数据中的可用信息计算后验概率P(es)和P(No)。如果P(esx)>(nox),那么记录分类为es,反之,分类为No PDG
二元变量分类变量连续变量类变量Td有房婚姻状况年收入拖欠贷款12是否单身125K否已婚100K3否单身70K4是已婚120K5否离异95K6否已婚60K7是离异220K8否单身85K否否否是否否是否是9否已婚75K10否单身90K图5-9预测贷款拖欠问题的训练集准确估计类标号和属性值的每一种可能组合的后验概率非常困难,因为即便属性数目不是很大,仍然需要很大的训练集。此时,贝叶斯定理很有用,因为它允许我们用先验概率P、类条件lass-conditional)概率P(X|Y)和证据P(X)来表示后验概率: P(Y|X)=P(X|)P()(5-11) P(X)在比较不同Y值的后验概率时,分母P(X)总是常数,因此可以忽略。先验概率P(可以通过计算训练集中属于每个类的训练记录所占的比例很容易地估计。对类条件概率P(|)的估计,我们介绍两种贝叶斯分类方法的实现:朴素贝叶斯分类器和贝叶斯信念网络。5.3.3节和5.3.5节分别描述了这两种实现方法。5.3.3.朴素贝叶斯分类器给定类标号y,朴素贝叶斯分类器在估计类条件概率时假设属性之间条件独立。条件独立假设可形式化地表述如下: P(X]=y)= P(X,=y)(5-12)其中每个属性集X={x,x2,…,Xa}包含d个属性。1.条件独立性在深入研究朴素贝叶斯分类法如何工作的细节之前,让我们先介绍条件独立概念。设x,y和Z表示三个随机变量的集合。给定Z,X条件独立于Y,如果下面的条件成立: P(XY, Z)=P()(5-13)条件独立的一个例子是一个人的手臂长短和他(她)的阅读能力之间的关系。你可能会发现手臂较长的人阅读能力也较强。这种关系可以用另一个因素解释,那就是年龄。小孩子的手臂往往比较短,也不具备成人的阅读能力。如果年龄一定,则观察到的手臂长度和阅读能力之间的关系就消失了。因此,我们可以得出结论,在年龄一定时,手臂长度和阅读能力二者条件独立。X和Y之间的条件独立也可以写成类似于公式(5-12)的形式:
 P(X, YZ)= P(X, Y, Z) P(Z) P(X, Y, Z)P(Y, Z) P(Y, Z) P(Z) =P(XY,)x P() =P()x P()(5-14)其中,公式(5-13)用于得到公式(5-14)的最后一行。2.朴素贝叶斯分类器如何工作有了条件独立假设,就不必计算X的每一个组合的类条件概率,只需对给定的Y,计算每一个X的条件概率。后一种方法更实用,因为它不需要很大的训练集就能获得较好的概率估计。分类测试记录时,朴素贝叶斯分类器对每个类Y计算后验概率: P(YX)-P()P(X,)(5-15) P()由于对所有的Y,P()是固定的,因此只要找出使分子Pp)最大的类就足够了。在接下来的两部分,我们描述几种估计分类属性和连续属性的条件概率P(的方法。3.估计分类属性的条件概率对分类属性X,根据类y中属性值等于x2的训练实例的比例来估计条件概率P(X=x|Y=y)例如,在图5-9给出的训练集中,还清贷款的7个人中3个人有房,因此,条件概率P(有房=是No)等于3/7。同理,拖欠还款的人中单身的条件概率P(婚姻状况=单身|es)=2/34.估计连续属性的条件概率朴素贝叶斯分类法使用两种方法估计连续属性的类条件概率。(1)可以把每一个连续的属性离散化,然后用相应的离散区间替换连续属性值。这种方法把连续属性转换成序数属性。通过计算类y的训练记录中落入X对应区间的比例来估计条件概率P(y=y)。估计误差由离散策略(见2.3.6节)和离散区间的数目决定。如果离散区间的数目太大,则就会因为每一个区间中训练记录太少而不能对P(做出可靠的估计。相反,如果区间数目太小,有些区间就会含有来自不同类的记录,因此失去了正确的决策边界。(2)可以假设连续变量服从某种概率分布,然后使用训练数据估计分布的参数。高斯分布通常被用来表示连续属性的类条件概率分布。该分布有两个参数,均值和方差2。对每个类y属性X的类条件概率等于:1 P(==y)=e(5-16)√2参数可以用类y的所有训练记录关于x的样本均值()来估计同理,参数可以用这些训练记录的样本方差(2)来估计。例如,考虑图5-9中年收入这一属性。该属性关于类No的样本均值和方差如下: PDG
x=125+100+70+…+75=1007s215-11)2(010…+(-110=29757(6)s=√2975=54.54给定一测试记录,应征税的收入等于120K美元,其类条件概率计算如下:P(收入=$120Ko)=1(120-110)2e2x2975=0.0072√2元(54.54)注意,前面对类条件概率的解释有一定的误导性。公式(5-16)的右边对应于一个概率密度函数(probability density function)f(xii)因为该函数是连续的,所以随机变量取某一特定值的概率为0取而代之,我们应该计算X落在区间x到x+的条件概率,其中e是一个很小的常数:(x≤x≤x+ey=y)=f(xiga)dxf(xiy,)(5-17)由于e是每个类的一个常量乘法因子,在对后验概率P()进行规范化的时候就抵消掉了。因此,我们仍可以使用公式(5-16)来估计类条件概率P(x5.朴素贝叶斯分类器举例考虑图5-10a中的数据集。我们可以计算每个分类属性的类条件概率,同时利用前面介绍的方法计算连续属性的样本均值和方差。这些概率汇总在图5-10b中P(有房=是o)=37P(有房=否o)=4/7tid有房婚姻状况年收入拖欠贷款P(有房=是es)=01是单身125KP(有房=否es)=12否已婚100KP(婚姻状况=单身o)=2ㄇP(婚姻状况=离婚o)=13否单身70K婚姻状况已o)=44是已婚120KP(婚姻状况单身es)=235否离婚95KP(婚姻状况=离婚es)=1/3P(婚姻状况=已婚|es)=06否已婚60K7是离婚220K年收入:8否单身85K否否否否是否否是否是如果类=No:样本均值=110样本方差=29759否已婚75K如果类=yes:样本均值=9010否单身90K样本方差=25(a) (b)图5-10贷款分类问题的朴素贝叶斯分类器为了预测测试记录X=(有房=否,婚姻状况=已婚,年收入$120K)的类标号,需要计算后验概率P(o)和P(es)。回想一下我们前面的讨论,这些后验概率可以通过计算先验概率P(Y和类条件概率P(x的乘积来估计,对应于公式(5-15)右端的分子
每个类的先验概率可以通过计算属于该类的训练记录所占的比例来估计因为有3个记录属于类yes,7个记录属于类No,所以P(es)=0.,P(o)=0.7使用图5-10b中提供的信息,类条件概率计算如下:P(NO)=P(有房=否No)P(婚姻状况=已婚|o)xp(年收入=$20kno)=4/7×4/7×0.0072=0.0024P(yes)=p(有房=否yes)×P(婚姻状况=已婚yes)xp(年收入=$20KYes)=1×0×1.2×10=0放到一起可得到类No的后验概率P(NOX)=ax7/100.0024=0.0016a,其中a=1/P(X)是个常量。同理,可以得到类yes的后验概率等于0,因为它的类条件概率等于0。因为P(NoX)>P(YesX),所以记录分类为No6.条件概率的m估计前面的例子体现了从训练数据估计后验概率时的一个潜在问题:如果有一个属性的类条件概率等于0,则整个类的后验概率就等于0。仅使用记录比例来估计类条件概率的方法显得太脆弱了,尤其是当训练样例很少而属性数目又很大时。一种更极端的情况是,当训练样例不能覆盖那么多的属性值时,我们可能就无法分类某些测试记录。例如,如果P(婚姻状况=离婚)为0而不是17,那么具有属性集X=(有房=是,婚姻状况=离婚,年收入=$120K)的记录的类条件概率如下:(xno)=3/700.0072=0P(yes)=0×1/3×1.2×10-=0朴素贝叶斯分类器无法分类该记录。解决该问题的途径是使用m估计(m--estimate)方法来估计条件概率:(xly,)=+mp(5-18) n+m其中,n是类y中的实例总数,n是类y的训练样例中取值x的样例数,m是称为等价样本大小的参数,而p是用户指定的参数。如果没有训练集(即n=0),则P(x=p因此p可以看作是在类y的记录中观察属性值x的先验概率。等价样本大小决定先验概率p和观测概率nn之间的平衡。在前面的例子中,条件概率P(婚姻状况=已婚es)=0,因为类中没有训练样例含有该属性值。使用m估计方法,m=3,p=1/3,则条件概率不再是0:P(婚姻状况=已婚|es)=(0+3×1/3)/(3+3)=1/6如果假设对类yes的所有属性p=1/3,对类No的所有属性p=23,则P(NO)=P(有房=否o)×P(婚姻状况=已婚o)p(年收入=$120ko)=6/10×6/10×0.0072=0.0026P(yes)=p(有房=否yes)P(婚姻状况=已婚yes)xp(年收入=$120kyes)=4/6×1/61.2×10-9=1.3×10-10
类No的后验概率P(XNo)=ax7/10×0.0026=.0018a,而类es的后验概率(xyes)=a×3/10×1.3×10-10=4.0×10-a.尽管分类结果不变,但是当训练样例较少时,m估计通常是一种更加健壮的概率估计方法。7.朴素贝叶斯分类器的特征朴素贝叶斯分类器一般具有以下特点。·面对孤立的噪声点,朴素贝叶斯分类器是健壮的。因为在从数据中估计条件概率时,这些点被平均。通过在建模和分类时忽略样例朴素贝叶斯分类器也可以处理属性值遗漏问题。·面对无关属性,该分类器是健壮的。如果X是无关属性,那么P(几乎变成了均匀分布。X的类条件概率不会对总的后验概率的计算产生影响。·相关属性可能会降低朴素贝叶斯分类器的性能因为对这些属性,条件独立的假设已不成立。例如,考虑下面的概率:P(A=0|Y=0)=0.4,P(A=1|=0)=0.6P(A=0=1)=0.6,P(A=1|y=1)=0.4其中,A是二元属性,Y是二元类变量。假设存在另一个二值属性B,当y=0时,B与A完全相关;当Y=1时,B与A相互独立。简单地说,假设B的类条件概率与A相同。给定一个记录,含有属性A=0,B=0,其后验概率计算如下:P(Y=0|A=0,B=0)=(A=0P(A=0y=0)P(B=0|y=0)P(Y=0)P(A=0,B=0)0.16×P(Y=0)P(A=0,B=0)(=1a=0,B=0)=(=oy=)p(b=y=1)P(y=1)P(A=0,B=00.36×P(Y=1)=P(A=0,B=0)如果P(Y=0)=P(Y=1),则朴素贝叶斯分类器将把该记录指派到类1然而,事实上P(A=0,B=0Y=0)=P(=0|y=0)=0.4因为当Y=0时,A和B完全相关。结果,Y=0的后验概率是:P(Y=0A=0,B=0)=P(A=0,B=0|y=0)P(y=0)P(A=0,B=00.4P(=0)P(A=0,B=0)比Y=1的后验概率大,因此,该记录实际应该分类为类0 POG5.3.4贝叶斯误差率假设我们知道支配P(X|)的真实概率分布。使用贝叶斯分类方法,我们就能确定分类任务的
理想决策边界,如下例所示。例5.3考虑任务:根据体长区分美洲鳄和鳄鱼。一条成年鳄鱼的平均体长大约15英尺,而一条成年美洲鳄的体长大约12英尺。假设它们的体长x服从标准差为2英尺的高斯分布,那么二者的类条件概率表示如下:11(x-15)2P(鳄鱼)=2222(5-19)(x美洲鳄)=1expX-12)2(5-20)√2元22图5-11给出了鳄鱼和美洲鳄类条件概率的比较假设它们的先验概率相同,理想决策边界满足:P(X=x鳄鱼)=P(X=x|美洲鳄)利用公式(5-19)和公式(5-20),得到:(x-15)2(x-12)222解得x=13.5。该例的决策边界处在两个均值的中点。0.20.18美洲鳄鳄鱼0.160.140.120.10.080.060.040.02010体长,x15图5-1鳄鱼和美洲鳄似然函数比较当先验概率不同时,决策边界朝着先验概率较小的类移动(见习题10)此外,给定数据上的任何分类器所达到的最小误差率都是可计算的。上例中的理想决策边界把体长小于的分类为美洲鳄,把体长大于的分类为鳄鱼。该分类器的误差率等于鳄鱼的后验概率曲线下面的区域(从0到x)加上美洲鳄后验概率曲线下面的区域(从到∞):Errp(鳄鱼x)dx+P(美洲鳄|x)dx PDG总误差率称为贝叶斯误差率(Bayes error rate)
5.3.5贝叶斯信念网络朴素贝叶斯分类器的条件独立假设似乎太严格了,特别是对那些属性之间有一定相关性的分类问题。本节介绍一种更灵活的类条件概率P()的建模方法。该方法不要求给定类的所有属性都条件独立,而是允许指定哪些属性条件独立。我们先讨论怎样表示和建立该概率模型,接着举例说明怎样使用模型进行推理。1.模型表示贝叶斯信念网络(Bayesian belief networks,BN),简称贝叶斯网络,用图形表示一组随机变量之间的概率关系。贝叶斯网络有两个主要成分。(1)一个有向无环图(dag),表示变量之间的依赖关系。(2)一个概率表,把各结点和它的直接父结点关联起来。考虑三个随机变量A、B和C,其中A和B相互独立,并且都直接影响第三个变量C三个变量之间的关系可以用图5-12a中的有向无环图概括。图中每个结点表示一个变量,每条弧表示两个变量之间的依赖关系。如果从X到Y有一条有向弧,则X是Y的父母,Y是X的子女。另外,如果网络中存在一条从X到Z的有向路经,则是Z的祖先,而Z是X的后代。例如,在图5-12b中,A是D的后代,D是B的祖先,而且B和D都不是A的后代结点。贝叶斯网络的一个重要性质表述如下:性质1条件独立贝叶斯网络中的一个结点如果它的父母结点已知,则它条件独立于它的所有非后代结点。图5-12b中,给定C,A条件独立于B和D,因为B和D都是A的非后代结点。朴素贝叶斯分类器中的条件独立假设也可以用贝叶斯网络来表示,如图5-12c所示,其中y是目标类,{x,2,Xa}是属性集。(a) (b)(c)图5-12使用有向无环图表示概率关系除了网络拓扑结构要求的条件独立性外,每个结点还关联一个概率表。(1)如果结点X没有父母结点,则表中只包含先验概率P(X)(2)如果结点X只有一个父母结点Y,则表中包含条件概率P((3)如果结点X有多个父母结点{Y1,Y2,…,,则表中包含条件概率P(1,2,,图5-13是贝叶斯网络的一个例子,对心脏病或心口痛患者建模。假设图中每个变量都是二值的。心脏病结点(HD)的父母结点对应于影响该疾病的危险因素,例如锻炼(E)和饮食(D)等。心脏病结点的子结点对应于该病的症状,如胸痛(CP)和高血压(BP)等。如图所示,心口痛(Hb)可能源于不健康的饮食,同时又可能导致胸痛。影响疾病的危险因素对应的结点只包含先验概率,而心脏病、心口痛以及它们的相应症状所对应的结点都包含条件概率。为了节省空间,图中省略了一些概率。注意P(X=x)=1-P(=x),
P(X=x|=1-P(=x),其中x表示和x相反的结果。因此,省略的概率可以很容易求得。例如,条件概率:P(心脏病=N锻炼=NO,饮食=健康)=1-(心脏病=yes|锻炼=N,饮食=健康)=1-0.55=0.45 E=YesD=健康0.70.25锻炼饮食 Hb=Ye HD=YesD=健康0.2 E=Yes0.25D=不健康0.85D=健康 E=YesD=不健康0.45 E=No心脏病心口痛D=健康0.55 E=No CP=YesD=不健康0.75 HD=Yes Hb=Yes 0.8 HD=Yes0.6BP=高胸痛 Hb=No HD=Yes 0.85血压 HD=No HD=No 0.2 Hb=Yes0.4 HD=No 0.1 Hb=No图5-13发现心脏病和心口痛病人的贝叶斯网络2.建立模型贝叶斯网络的建模包括两个步骤:(1)创建网络结构;(2)估计每一个结点的概率表中的概率值。网络拓扑结构可以通过对主观的领域专家知识编码获得。算法53给出了归纳贝叶斯网络拓扑结构的一个系统的过程。算法5.3贝叶斯网络拓扑结构的生成算法1:设T=(x1,x2,…,)表示变量的全序 2: for j= to d do3:令Xr表示T中第j个次序最高的变量4:令()={1,Xx2,X-1)}表示排在前面的变量的集合5:从π(X)中去掉对X没有影响的变量(使用先验知识)6:在X和(X)中剩余的变量之间画弧 7: end for例5.4考虑图5-13中的变量。执行步骤1后,设变量次序为(E,D,HD,Hb,CP,BP)从变量D开始,经过步骤2到步骤7,我们得到如下条件概率。P(DE)化简为P(D) PDGP(HDE,D)不能化简。P(HbHD,E,D)化简为P(HbD)
P(CPHb,HD,E,D)化简为P(CPHb,HD)。P(BPCP,Hb,HD,E,D化简为P(BPHD)基于以上条件概率,创建结点之间的弧(EHD),(D,HD),(D,Hb),(HD,CP),(Hb,CP)和(HD,BP)这些弧构成了图5-13所示的网络结构。算法5.3保证生成的拓扑结构不包含环,这一点也很容易证明。如果存在环,那么至少有一条弧从低序结点指向高序结点,并且至少存在另一条弧从高序结点指向低序结点。由于算法5.3不允许从低序结点到高序结点的弧存在,因此拓扑结构中不存在环。然而,如果我们对变量采用不同的排序方案得到的网络拓扑结构可能会有变化。某些拓扑结构可能质量很差,因为它在不同的结点对之间产生了很多条弧。从理论上讲,可能需要检查所。有d种可能的排序才能确定最佳的拓扑结构,这是一项计算开销很大的任务。替代的方法是把变量分为原因变量和结果变量,然后从各原因变量向其对应的结果变量画弧。这种方法简化了贝叶斯网络结构的建立。一旦找到了合适的拓扑结构,与各结点关联的概率表就确定了。对这些概率的估计比较容易,与朴素贝叶斯分类器中所用的方法类似。3.使用BBN进行推理举例假设我们对使用图5-13中的BBN来诊断一个人是否患有心脏病感兴趣下面阐释在不同的情况下如何做出诊断。情况一:没有先验信息在没有任何先验信息的情况下,可以通过计算先验概率P(D=yes)和P(HD=NO)来确定一个人是否可能患心脏病。为了表述方便,设a∈{yes,no}表示锻炼的两个值,B∈{健康,不健康}表示饮食的两个值。 P(HD=Yes)=P(HD =Yes|E=a, D)P(=a,=) =ZEP(HD =Yes|E=a, D B)(E a)P(D= B)=0.25×0.7×0.25+0.45×0.7×0.75+0.55×0.3×0.25+0.75×0.3×0.75=0.49因为P(HD=N)=1-P(HD=yes)=0.51,所以,此人不得心脏病的机率略微大一点。情况二:高血压如果一个人有高血压,可以通过比较后验概率P(HD=Yes BP=高)和P(HD=NO|BP=高)来诊断他是否患有心脏病为此,我们必须先计算P(BP=高):P(BP=高)=p(B=高D=)P(HD=)7=0.85×0.49+0.2×0.51=0.5185其中y∈{Yes,No}。因此,此人患心脏病的后验概率是:P(=Yes BP=高)p(p=高hd=yes)p(hd=yes)P(BP=高) PDG0.85×0.49=0.80330.5185
同理,P(HD=NOBP=高)=1-0.8033=0.1967。因此,当一个人有高血压时,他患心脏病的危险就增加了。情况三:高血压、饮食健康、经常锻炼身体假设得知此人经常锻炼身体并且饮食健康。这些新信息会对诊断造成怎样的影响呢?加上这些新信息,此人患心脏病的后验概率:P(HD=YesBP=高,D=健康,E=yes)P(B=高HD=yes,d=健康,e=yes)P(HD=yes=健康,E=yes)P(BP=高|D=健康,E=es)P(B=高HD=yes)(Hd=yesd=健康,E=yes),P(BP=高HD=y)P(HD=yD=健康,E=yes)0.85×0.250.85×0.25+0.2×0.75=0.5862而此人不患心脏病的概率是:P(HD=OBP=高,D=健康,E=yes)=1-0.5862=0.4138因此模型暗示健康的饮食和有规律的体育锻炼可以降低患心脏病的危险。4.BBN的特点下面是BBN模型的一般特点。(1)BBN提供了一种用图形模型来捕获特定领域的先验知识的方法。网络还可以用来对变量间的因果依赖关系进行编码。(2)构造网络可能既费时又费力。然而,一旦网络结构确定下来,添加新变量就十分容易。(3)贝叶斯网络很适合处理不完整的数据。对有属性遗漏的实例可以通过对该属性的所有可能取值的概率求和或求积分来加以处理。(4)因为数据和先验知识以概率的方式结合起来了,所以该方法对模型的过分拟合问题是非常鲁棒的。5.4人工神经网络人工神经网络(ANN)的研究是由试图模拟生物神经系统而激发的。人类的大脑主要由称为神经元(neuron)的神经细胞组成,神经元通过叫作轴突(axon)的纤维丝连在一起。当神经元受到刺激时,神经脉冲通过轴突从一个神经元传到另一个神经元。一个神经元通过树突( dendrite)连接到其他神经元的轴突,树突是神经元细胞体的延伸物。树突和轴突的连接点叫作神经键(synapse)。神经学家发现,人的大脑通过在同一个脉冲反复刺激下改变神经元之间的神经键连接强度来进行学习。类似于人脑的结构,ANN由一组相互连接的结点和有向链构成本节将分析一系列ANN模型,从介绍最简单的模型—感知器(perceptron)开始,看看如何训练这种模型来解决分类问题。 PDG
5.4.1感知器考虑图5-14中的图表。左边的表显示一个数据集包含三个布尔变量(x1,x2,x3)和一个输出变量y,当三个输入中至少有两个是0时,y取1,而至少有两个大于0时,y取+1。1x23y100-1输入结点1011110110.3输出结点1111 X2->O0.3y001-1010-10111X3→0.3000-1t=0.4(a)数据集(b)感知器图5-14使用感知器模拟一个布尔函数图5-14b展示了一个简单的神经网络结构感知器。感知器包含两种结点:几个输入结点,用来表示输入属性;一个输出结点,用来提供模型输出。神经网络结构中的结点通常叫作神经元或单元。在感知器中,每个输入结点都通过一个加权的链连接到输出结点。这个加权的链用来模拟神经元间神经键连接的强度。像生物神经系统一样,训练一个感知器模型就相当于不断调整链的权值,直到能拟合训练数据的输入输出关系为止。感知器对输入加权求和,再减去偏置因子,然后考察结果的符号,得到输出值图5-14b中的模型有三个输入结点,各结点到输出结点的权值都等于0.3,偏置因子t=0.4。模型的输出计算公式如下:y={1如果0.3x+0.3x2+0.3x3-0.4>0(5-21)1如果0.3x+0.3x2+0.3x3-0.4<0例如,如果x1=1,x2=1,x3=0,那么y=+1因为0.3x1+0.3x2+0.3x3-0.4是正的。另外,如果x1=0,x2=1,x3=0,那么y=-1,因为加权和减去偏置因子值为负。注意感知器的输入结点和输出结点之间的区别。输入结点简单地把接收到的值传送给输出链,而不作任何转换。输出结点则是一个数学装置,计算输入的加权和,减去偏置项,然后根据结果的符号产生输出。更具体地,感知器模型的输出可以用如下数学方式表示:(5-22)其中,1W2,Wa是输入链的权值,而x1x2,x是输入属性值。符号函数,作为输出神经元的激活函数(activation function),当参数为正时输出+1,参数为负时输出-1。感知器模型可以写成下面更简洁的形式:y=sign[w+wa-ixa-++wix1+ Woxo=sign(wx)(5-23)其中,wo=-t,x=1,wx是权值向量w和输入属性向量x的点积。学习感知器模型 * DG在感知器模型的训练阶段,权值参数w不断调整直到输出和训练样例的实际输出一致。算法54中给出了感知器学习算法的概述。
算法5.4感知器学习算法1:令D={(xy)=1,2,…N}是训练样例集2:用随机值初始化权值向量w) 3:repeat4:for每个训练样例(x,y)∈Ddo5:计算预测输出6:for每个权值wdo7:更新权值w=w+y-yx 8: end for 9: end for10: until满足终止条件算法的主要计算是第7步中的权值更新公式: w +t1)=w() +(y-)(5-24)其中w是第k次循环后第i个输入链上的权值,参数λ称为学习率(learning rate),x是训练样例x的第j个属性值。权值更新公式的理由是相当直观的。从公式(5-24)可以看出,新权值w+1)是等于旧权值w加上一个正比于预测误差y-y)的项。如果预测正确,那么权值保持不变。否则,按照如下方法更新。·如果y=+1,y=-1,那么预测误差(y-)=2。为了补偿这个误差,需要通过提高所有正输入链的权值、降低所有负输入链的权值来提高预测输出值。·如果y=-1,y=+1,那么预测误差(yy)=-2为了补偿这个误差,我们需要通过降低所有正输入链的权值、提高所有负输入链的权值来减少预测输出值。在权值更新公式中,对误差项影响最大的链需要的调整最大。然而,权值不能改变太大,因为仅仅对当前训练样例计算了误差项。否则的话,以前的循环中所作的调整就会失效。学习率λ其值在0和1之间,可以用来控制每次循环时的调整量。如果接近0,那么新权值主要受旧权值的影响;相反,如果接近1,则新权值对当前循环中的调整量更加敏感。在某些情况下,可以使用一个自适应的值:在前几次循环时值相对较大,而在接下来的循环中逐渐减小。公式(5-23)中所示的感知器模型关于参数w和属性x是线性的。因此,设=0,得到的感知器的决策边界是一个把数据分为-1和1两个类的线性超平面。图5-15显示了把感知器学习算法应用到图5-14中的数据集上所得到的决策边界。对于线性可分的分类问题,感知器学习算法保证收敛到一个最优解(只要学习率足够小)。如果问题不是线性可分的,那么算法就不会收敛。图5-16给出了一个由XOR函数得到的非线性可分数据的例子。感知器找不到该数据的正确解,因为没有线性超平面可以把训练实例完全分开。x20.50.51.50.58 POGx图5-15图5-14中的数据的感知器决策边界图5-16XR分类问题。没有线性超平面可以分开这两个类
5.4.2多层人工神经网络人工神经网络结构比感知器模型更复杂。这些额外的复杂性来源于多个方面。(1)网络的输入层和输出层之间可能包含多个中间层,这些中间层叫作隐藏层(hidden layer),隐藏层中的结点称为隐藏结点(hidden node)。这种结构称为多层神经网络(见图5-17)。在前馈(ed -forward-)神经网络中,每一层的结点仅和下一层的结点相连。感知器就是一个单层的前馈神经网络,因为它只有一个结点层一输出层进行复杂的数学运算。在递归(recurrent)神经网络中,允许同一层结点相连或一层的结点连到前面各层中的结点。1x23×4输入层隐藏层输出层图5-17多层前馈人工神经网络(ANN)举例(2)除了符号函数外,网络还可以使用其他激活函数,如图5-18所示的线性函数、S型(逻辑斯缔)函数、双曲正切函数等。这些激活函数允许隐藏结点和输出结点的输出值与输入参数呈非线性关系。050.5-0.50.5-0.500.50.500.5线性函数S型函数1.510.50.50.50.50.500.51-1.5-1-0.500.51双曲正切函数符号函数 PDG图5-18人工神经网络中激活函数的类型
这些附加的复杂性使得多层神经网络可以对输入和输出变量间更复杂的关系建模。例如,考虑上一节中描述的XOR问题。实例可以用两个超平面进行分类,这两个超平面把输入空间划分到各自的类,如图5-19a所示。因为感知器只能构造一个超平面,所以它无法找到最优解。该问题可以使用两层前馈神经网络加以解决,见图5-19b。直观上,我们可以把每个隐藏结点看作一个感知器,每个感知器构造两个超平面中的一个,输出结点简单地综合各感知器的结果,得到的决策边界如图5-19a所示。1.5输入层隐藏层输出层x205W5341W328 wsW42a)决策边界b)神经网络拓扑结构图5-19XOR问题的两层前馈神经网络要学习ANN模型的权值,需要一个有效的算法该算法在训练数据充足时可以收敛到正确的解。一种方法是把网络中的每个隐藏结点或输出结点看作一个独立的感知器单元,使用与公式(5-24)相同的权值更新公式。显然,这种方法行不通,因为缺少隐藏结点的真实输出的先验知识。这使得很难确定各隐藏结点的误差项(y-y)。下面介绍一种基于梯度下降的神经网络权值学习方法。1.学习ANN模型ANN学习算法的目的是确定一组权值w,最小化误差的平方和:E(w)=20-)2(5-25)注意,误差平方和取决于w,因为预测类是赋予隐藏结点和输出结点的权值的函数图5-20显示了一个误差曲面的例子,该曲面是两个参数w1和2的函数。当是参数w的线性函数时,通常得到这种类型的误差曲面。如果将y=w·x代入公式(5-25),则误差函数变成参数的二次函数,就可以很容易地找到全局最小解。硺所0.50.5 w2图5-20两个参数模型的误差曲面E(w1,w2)
大多数情况下,由于激活函数的选择(如S型或双曲正切函数),ANN的输出是参数的非线性函数。这样,就不能直接推导出w的全局最优解了。像基于梯度下降的方法等贪心算法可以很有效地求解优化问题。梯度下降方法使用的权值更新公式可以写成:W←w,-0E(w)(5-26) aw;其中,是学习率。式中第二项说的是权值应该沿着使总体误差项减小的方向增加。然而,由于误差函数是非线性的,因此,梯度下降方法可能会陷入局部极小值。梯度下降方法可以用来学习神经网络中输出结点和隐藏结点的权值。对于隐藏结点,学习的计算量并不小,因为在不知道输出值的情况下,很难估计结点的误差项EO。一种称为反向传播(back-propagation-)的技术可以用来解决该问题。该算法的每一次迭代包括两个阶段:前向阶段和后向阶段。在前向阶段,使用前一次迭代所得到的权值计算网络中每一个神经元的输出值。计算是向前进行的,即先计算第k层神经元的输出,再计算第k+1层的输出。在后向阶段,以相反的方向应用权值更新公式,即先更新k+1层的权值,再更新第k层的权值。使用反向传播方法,可以用第k+1层神经元的误差来估计第k层神经元的误差。2.ANN学习中的设计问题在训练神经网络来学习分类任务之前,应该先考虑以下设计问题。(1)确定输入层的结点数目。每一个数值输入变量或二元输入变量对应一个输入结点。如果输入变量是分类变量,则可以为每一个分类值创建一个结点,也可以用[ogk个输入结点对k元变量进行编码。(2)确定输出层的结点数目。对于2类问题一个输出结点足矣;而对于k类问题,则需要k个输出结点。(3)选择网络拓扑结构(例如,隐藏层数和隐藏结点数,前馈还是递归网络结构)。注意,目标函数表示取决于链上的权值、隐藏结点数和隐藏层数、结点的偏置以及激活函数的类型。找出合适的拓扑结构不是件容易的事。一种方法是,开始的时候使用一个有足够多的结点和隐藏层的全连接网络,然后使用较少的结点重复该建模过程。这种方法非常耗时。另一种方法是,不重复建模过程,而是删除一些结点,然后重复模型评价过程来选择合适的模型复杂度。(4)初始化权值和偏置。随机赋值常常是可取的。(5)去掉有遗漏值的训练样例,或者用最合理的值来代替。5.4.3人工神经网络的特点人工神经网络的一般特点概括如下。(1)至少含有一个隐藏层的多层神经网络是一种普适近似( universal approximator),即可以用来近似任何目标函数。由于ANN具有丰富的假设空间,因此对于给定的问题,选择合适的拓扑结构来防止模型的过分拟合是很重要的。(2)ANN可以处理冗余特征,因为权值在训练过程中自动学习。冗余特征的权值非常小。(3)神经网络对训练数据中的噪声非常敏感。处理噪声问题的一种方法是使用确认集来确定模型的泛化误差,另一种方法是每次迭代把权值减少一个因子。(4)ANN权值学习使用的梯度下降方法经常会收敛到局部极小值。避免局部极小值的方法是
在权值更新公式中加上一个动量项(momentum term)(5)训练ANN是一个很耗时的过程,特别是当隐藏结点数量很大时然而,测试样例分类时非常快。5.5支持向量机支持向量机( support vector machine,SVM)已经成为一种倍受关注的分类技术。这种技术具有坚实的统计学理论基础,并在许多实际应用(如手写数字的识别、文本分类等)中展示了大有可为的实践效用。此外,SVM可以很好地应用于高维数据,避免了维灾难问题。这种方法具有一个独特的特点,它使用训练实例的一个子集来表示决策边界,该子集称作支持向量(support vector).为了解释SVM的基本思想,首先介绍最大边缘超平面( maximal margin hyperplane)的概念以及选择它的基本原理。然后,描述在线性可分的数据上怎样训练一个线性的SVM,从而明确地找到这种最大边缘超平面。最后,介绍如何将SVM方法扩展到非线性可分的数据上。5.5.1最大边缘超平面图5-21显示了一个数据集,包含属于两个不同类的样本,分别用方块和圆圈表示。这个数据集是线性可分的,即可以找到这样一个超平面,使得所有的方块位于这个超平面的一侧,而所有的圆圈位于它的另一侧。然而,正如图5-1所示,可能存在无穷多个那样的超平面。虽然它们的训练误差都等于零,但是不能保证这些超平面在未知实例上运行得同样好。根据在检验样本上的运行效果,分类器必须从这些超平面中选择一个来表示它的决策边界。。o。。图5-21一个线性可分数据集上的可能决策边界为了更好地理解不同的超平面对泛化误差的影响,考虑两个决策边界B1和B2,如图5-22所示。这两个决策边界都能准确无误地将训练样本划分到各自的类中。每个决策边界B都对应着一对超平面,分别记为b1和b2。其中,b1这样得到的:平行移动一个和决策边界平行的超平面,直到触到最近的方块为止;类似地,平行移动一个和决策边界平行的超平面,直到触到最近的圆圈,可以得到b。这两个超平面之间的间距称为分类器的边缘通过图5-22中的图解,注意到B1的边缘显著大于B2的边缘。在这个例子中,B1就是训练样本的最大边缘超平面。
 b21 B2 b2B2的边缘■ .b11B1的边缘b12图5-22决策边界的边缘最大边缘的基本原理具有较大边缘的决策边界比那些具有较小边缘的决策边界具有更好的泛化误差。直觉上,如果边缘比较小,决策边界任何轻微的扰动都可能对分类产生显著的影响。因此,那些决策边界边缘较小的分类器对模型的过分拟合更加敏感,从而在未知的样本上的泛化能力很差。统计学习理论给出了线性分类器边缘与其泛化误差之间关系的形式化解释,我们称这种理论为结构风险最小化(structural risk minimization,SRM)理论。该理论根据分类器的训练误差R训练样本数N和模型的复杂度h(即它的能力(capacity)),给出了分类器的泛化误差的一个上界R。具体地说,在概率1-n下,分类器的泛化误差在最坏情况下满足 h log(n)R≤R+(5-27)N其中,是能力h的单调增函数。上面的不等式读者可能感觉很熟悉,这是因为它和4.4.4节最小描述长度(MDL)原理中的等式十分相似。在这一点上,SRM是泛化误差的另外一种表达方式,它体现了训练误差和模型复杂度之间的折中。线性模型的能力与它的边缘逆相关。具有较小边缘的模型具有较高的能力,因为与具有较大边缘的模型不同,具有较小边缘的模型更灵活、能拟合更多的训练集。然而,依据SRM原理,随着能力增加,泛化误差的上界也随之提高。因此,需要设计最大化决策边界的边缘的线性分类器,以确保最坏情况下的泛化误差最小线性SM(linear SVM)就是这样的分类器,下一节将要详细介绍。5.5.2线性支持向量机:可分情况线性SVM是这样一个分类器,它寻找具有最大边缘的超平面,因此它也经常被称为最大边缘分类器( maximal margin classifier)为了深刻理解SVM是如何学习这样的边界的,我们首先对线性分类器的决策边界和边缘进行一些初步的讨论。考虑一个包含N个训练样本的二元分类问题。每个样本表示为一个二元(xy)(=1,2PDG1.线性决策边界N),其中x=(xi,xi,xia),对应于第i个样本的属性集。为方便计,令y∈{-1,1表它的类标
号。一个线性分类器的决策边界可以写成如下形式:w·x+b=0(5-28)其中,w和b是模型的参数。图5-23显示了包含圆圈和方块的二维训练集。图中的实线表示决策边界,它将训练样本一分为二,划入各自的类中。任何位于决策边界上的样本都必须满足公式(5-28)。例如,如果x和x是两个位于决策边界上的点,则w·xa+b=0w·xb+b=0两个方程相减便得到:w.(x-x)=0其中,xb-x是一个平行于决策边界的向量,它的方向是从x到x由于点积的结果为零,因此w的方向必然垂直于决策边界,如图5-23所示。 w.+=0口口口口 w.x+b=1 w.+b=-图5-23SVM的决策边界和边缘对于任何位于决策边界上方的方块x,我们可以证明:wxs+b=k(5-29)其中k>0。同理,对于任何位于决策边界下方的圆圈x,我们可以证明:w·+b=k(5-30)其中k<0如果标记所有的方块的类标号为+1,标记所有的圆圈的类标号为-1,则可以用以下的方式预测任何测试样本z的类标号y1如果wz+b>0 y=(5-31)-1如果w.z+b<02.线性分类器的边缘考虑那些离决策边界最近的方块和圆圈。由于该方块位于决策边界的上方,因此对于某个正G值k,它必然满足公式(5-29);而对于某个负值k,圆圈必须满足公式(5-30)。调整决策边界的参数w和b,两个平行的超平面b1和b2可以表示如下:
 bi:w.x+b=1(5-32) bi:w.x+b=-1(5-33)决策边界的边缘由这两个超平面之间的距离给定。为了计算边缘,令x1是b上的一个数据点,x2是b2上的一个数据点,如图5-23所示。将x1x2分别代入公式(5-32)和公式(5-33)中,则边缘d可以通过两式相减得到:w.(x1-x2)=2 llwxd=2∴d=2(5-34)3.学习线性SVM模型SVM的训练阶段包括从训练数据中估计决策边界的参数w和b选择的参数必须满足下面两个条件:w·x+b≥1如果y=1wxi+b≤-1如果yi=-1(5-35)这些条件要求所有类标号为1的训练实例(即方块)都必须位于超平面wx+b=1上或位于它的上方,而那些类标号为-1的训练实例(即圆圈)必须位于超平面wx+b=-1上或位于它的下方。这两个不等式可以概括为如下更紧凑的形式:y(wx+b)≥1,i=1,2,,N(5-36)尽管前面的条件也可以用于其他线性分类器(包括感知器),但是SVM增加了一个要求:其决策边界的边缘必须是最大的。然而,最大化边缘等价于最小化下面的目标函数:f(w)=w(5-37)2定义5.1线性SVM:可分情况SVM的学习任务可以形式化地描述为以下被约束的优化问题: min w2受限于y(wx+b)≥1,i=1,2,,n由于目标函数是二次的,而约束在参数w和b上是的线性的,因此这个问题是一个凸(convex)优化问题,可以通过标准的拉格朗日乘子Lagrange multiplier)方法求解。下面简要介绍一下求解这个优化问题的主要思想。首先,必须改写目标函数,考虑施加在解上的约束新目标函数称为该优化问题的拉格朗日函数: Lp=- (y, (w.x,+b)-1)(5-38)其中,参数称为拉格朗日乘子。拉格朗日函数中的第一项与原目标函数相同,而第二项则捕获了不等式约束。为了理解改写原目标函数的必要性,考虑公式(5-37)给出的原目标函数。容易证明当w=0(即零向量,它的每一个分量均为0)时函数取得最小值。然而,这样的解违背了定义5.1中给出的约束条件,因为b没有可行解。实上,如果w和b的解违反不等式约束,即
如果y(w·x+b)-1<0,则解是不可行的。公式(5-38)给出的拉格朗日函数通过从原目标函数减去约束条件的方式合并了约束条件。假定0,则任何不可行解仅仅是增加了拉格朗日函数的值。为了最小化拉格朗日函数 ,ale=0=w=,必须对Lp关于和b求偏导,并令它们等于零:(5-39)w aL=0y1=0(5-40)b因为拉格朗日乘子是未知的,因此我们仍然不能得到w和b的解。如果定义5.1只包含等式约束,而不是不等式约束,则我们可以利用从该等式约束中得到的N个方程,加上公式(5-39)和公式(5-40),从而得到w,b和的可行解。注意,等式约束的拉格朗日乘子是可以取任意值的自由参数。处理不等式约束的一种方法就是把它变换成一组等式约束。只要限制拉格朗日乘子非负,这种变换便是可行的。这种变换导致如下拉格朗日乘子约束,称作 Karuch--Tucher-kuhn-(kk)条件:≥0(5-41)[y:(w.x;+b)-1]=0(5-42)乍一看,拉格朗日乘子的数目好像和训练样本的数目一样多。事实上,应用公式(5-42)给定的约束后,许多拉格朗日乘子都变为零该约束表明,除非训练实例满足方程y(w·x+b)=1,否则拉格朗日乘子必须为零。那些>0的训练实例位于超平面b或b2上,称为支持向量。不在这些超平面上的训练实例肯定满足=0。公式(5-39)和公式(5-42)还表明,定义决策边界的参数w和b仅依赖于这些支持向量。对前面的优化问题求解仍是一项十分棘手的任务,因为它涉及大量参数:W,b和。通过将拉格朗日函数变换成仅包含拉格朗日乘子的函数(称作对偶问题),可以简化该问题。为了变换成对偶问题,首先将公式(5-39)和公式(5-40)代入到公式(5-38)中。这将导致该优化问题的如下对偶公式:=24-44,, *(5-43) i.j对偶拉格朗日函数和原拉格朗日函数的主要区别如下:(1)对偶拉格朗日函数仅涉及拉格朗日乘子和训练数据,而原拉格朗日函数除涉及拉格朗日乘子外还涉及决策边界的参数。尽管如此这两个优化问题的解是等价的。(2)公式(5-43)中的二次项前有个负号,这说明原来涉及拉格朗日函数L的最小化问题已经变换成了涉及对偶拉格朗日函数LD的最大化问题。对于大型数据集,对偶优化问题可以使用数值计算技术来求解,如使用二次规划(已经超出本书的范围)一旦找到一组,就可以通过公式(5-39)和公式(5-42)来求得w和b的可行解。决策边界可以表示成:: x +b=0(5 yPDG
b可以通过求解支持向量公式(5-42)得到由于么是通过数值计算得到的,因此可能存在数值误差,计算出的b值可能不唯一。它取决于公式(5-42)中使用的支持向量。实践中,使用b的平均值作为决策边界的参数。例5.5考虑图5-24给出的二维数据集,它包含8个训练实例。使用二次规划方法,可以求解公式(5-43)给出的优化问题,得到每一个训练实例的拉格朗日乘子,如表的最后一列所示。注意,仅前面两个实例具有非零的拉格朗日乘子。这些实例对应于该数据集的支持向量。令w=(W1,w2),b为决策边界的参数。使用公式(5-39),我们可以按如下方法求解w1和2w1=yx=65.5261×1×0.3858+65.261(-1)×0.4871=-6.64iw2=yxi2=65.5261×1×0.4687+65.5261(-1)x0.611=-9.32偏倚项b可以使用公式(5-42)对每个支持向量进行计算:b)=1-wx1=1-(-6.64)(0.3858)-(9.32)(0.4687)=7.9300b2=1-w.x2=-1-(-6.64)(0.4871)-(-9.32)(0.611)=7.9289对这些值取平均,得到b=7.93。对应于这些参数的决策边界显示在图5-24中口 X2y拉格朗日乘子0.38580.4687165.52610.48710.61165.52610.92180.41030.73820.8936-10.17630.057910.40570.352910.93550.8132-10000000.21460.009910.96.64x1-9.32x2+7.93=00.8口0.70.60.50.4口0.30.20.100800.40.60.8 X1图5-24一个线性可分数据集的例子确定了决策边界的参数之后,检验实例就可以按以下的公式来分类:
如果f(z)=1,则检验实例被分为到正类,否则分到负类。5.5.3线性支持向量机:不可分情况图5-25给出了一个和图5-22相似的数据集,不同处在于它包含了两个新样本P和Q。尽管决策边界B1误分类了新样本,而B2正确分类了它们,但是这并不表示B2是一个比B1好的决策边界,因为这些新样本可能只是训练数据集中的噪声。B1可能仍然比B2更可取,因为它具有较宽的边缘,从而对过分拟合不太敏感。然而,上一节给出的SVM公式只能构造没有错误的决策边界。这一节考察如何修正公式,利用一种称为软边缘( soft margin)的方法,学习允许一定训练错误的决策边界。更为重要的是,本节给出的方法允许SVM在一些类线性不可分的情况下构造线性的决策边界。为了做到这一点,SM学习算法必须考虑边缘的宽度与线性决策边界允许的训练错误数目之间的折中。21B2b22B2的边缘QBb11B1的边缘12图5-25不可分情况下SVM的决策边界尽管公式(5-37)给定的原目标函数仍然是可用的,但是决策边界B1不再满足公式(5-36)给定的所有约束。因此,必须放松不等式约束,以适应非线性可分数据。可以通过在优化问题的约束中引入正值的松弛变量(slack variable来实现,如下式所示:w·x+b≥1-5如果yi=1w·x+b≤-1+如果yi=-1(5-45)其中,Vi:>0为了理解松弛变量的意义,考虑图5-26。圆圈P是一个实例,它违反公式(5-35)给定的约束。设wx+b=-1+5是一条经过点P,且平行于决策边界的直线。可以证明它与超平面wx+b=-1之间的距离为。因此,提供了决策边界在训练样本P上的误差估计理论上,可以使用和前面相同的目标函数,然后加上公式(5-45)给定的约束来确定决策边界。然而,由于在决策边界误分样本的数量上没有限制,学习算法可能会找到这样的决策边界,它的边缘很宽,但是误分了许多训练实例如图5-27所示。为了避免这个问题,必须修改目标函数,以惩罚那些松弛变量值很大的决策边界。修改后的目标函数如下:
f(w)=+2其中C和k是用户指定的参数,表示对误分训练实例的惩罚。为了简化该问题,在本节的剩余部分假定k=1。参数C可以根据模型在确认集上的性能选择。 w.x+b=01.2口口0.8口口0.60.4w.x+b=-1+50.2w.x+b=-0.20.500.51.5 X1图5-26不可分数据的松弛变量Q图5-27一个具有宽边缘但训练误差很高的决策边界由此,被约束的优化问题的拉格朗日函数可以记作如下形式: Lp =ll+c-y (w., +b)-1+ 5,)-u,(5-46)其中,前面两项是需要最小化的目标函数,第三项表示与松弛变量相关的不等式约束,而最后一项是要求的值非负的结果。此外,利用如下的KKT条件,可以将不等式约束变换成等式约束:5≥0,≥0,≥0(5-47)y(w.x+b)-1+5}=0(5-48)5=0 PDG(5-49)注意,公式(5-48)中的拉格朗日乘子是非零的当且仅当训练实例位于直线w·x+b=±1上或
5>0。另一方面,对于许多误分类的训练实例(即满足5>0),公式(5-49)中的拉格朗日乘子都为零。令L关于w,b和的一阶导数为零,就得到如下公式:a=w-xy=0→w=zx(5-50) dw;b=-y=0y=0(5-51)LC--u1=0+=c(5-52)将公式(5-50)、(5-51)和(5-52)代入拉格朗日函数中,得到如下的对偶拉格朗日函数: Lo =4y.,+ -4((,y,: x, +b)-1+5)(5-53)-(c-)5=2-2yx它与线性可分数据上的对偶拉格朗日函数相同(参见公式(5-40))。尽管如此,施加于拉格朗日乘子上的约束与在线性可分情况下略微不同。在线性可分情况下,拉格朗日乘子必须是非负的,即≥0。另一方面,公式(5-52)表明不应该超过C(由于和都是非负的)因此,非线性可分数据的拉格朗日乘子被限制在0≤≤C然后,可以使用二次规划技术,求对偶问题的数值解,得到拉格朗日乘子可以将这些乘子代入公式(5-50)和KKT条件中,从而得到决策边界的参数。5.5.4非线性支持向量机上一节描述的SVM公式构建一个线性的决策边界,从而把训练实例划分到它们各自的类中。本节提出了一种把SVM应用到具有非线性决策边界数据集上的方法。这里的关键在于将数据从原先的坐标空间x变换到一个新的坐标空间更(x)中,从而可以在变换后的坐标空间中使用一个线性的决策边界来划分样本。进行变换后,就可以应用上一节介绍的方法在变换后的空间中找到一个线性的决策边界。1.属性变换为了说明怎样进行属性变换可以生成一个线性的决策边界,我们考察图5-28a给出的二维数据集,它包含方块(类标号y=1)和圆圈(标号y=-1)。数据集是这样生成的,所有的圆圈都聚集在图的中心附近,而所有的方块都分布在离中心较远的地方。可以使用下面的公式对数据集中的实例分类:y(x,x2)=1如果x-0.5)2+(x2-0.5)2>0.2(5-54) PDG1否则
因此,数据集的决策边界可以表示如下:√x1-0.5)2+(x2-0.5)2=0.2这可以进一步简化为下面的二次方程:x2-x1+x2-x2=-0.46需要一个非线性变换Φ,将数据从原先的特征空间映射到一个新的空间,决策边界在这个空间下成为线性的。假定选择下面的变换:中:(x1,x2)→(x2,x2,2x1,2x2,√2x1x2,1)(5-55)在变换后的空间中,我们找到参数w=(wo,w1,),使得: wsx2++ +w22x2 +w v2x x2 +wo=0例如,对于前面给定的数据,以x2-x1和x2-x2为坐标绘图。图5-28b显示在变换后的空间中,所有的圆圈都位于图的左下方。因此,可以构建一个线性的决策边界从而把数据划分到各自所属的类中。这种方法的一个潜在问题是,对于高维数据可能产生维灾难,在本节稍后,将介绍非线性SVM如何避免这个问题(使用一种称为核技术的方法)09口口080.7d0.6055口040.20.3口0.2口0.25口口010.10.2030.4050.60.70.80.90250.20.1501005 x-x,(a)原二维空间中的决策边界(b)变换后空间中的决策边界图5-28分类具有非线性决策边界的数据2.学习非线性SVM模型尽管属性变换方法看上去大有可为,但是存在一些实现问题。首先,它不清楚应当使用什么类型的映射函数,才可以确保在变换后的空间构建线性决策边界。一种选择是把数据变换到无限维空间中,但是这样的高维空间可能很难处理。其次,即使知道合适的映射函数,在高维特征空间中解约束优化问题仍然是计算代价很高的任务。为了解释这些问题并考察处理它们的方法,假定存在一个合适的函数中(x)来变换给定的数据集。变换后,我们需要构建一个线性的决策边界,把样本划分到它们各自所属的类中。在变换后的空间中,线性决策边界具有以下形式:w·(x)+b=0 PDG定义5.2非线性SVM非线性SVM的学习任务可以形式化地表达为以下的优化问题:
 min受限于y(w·(x)+b)≥1,i=1,2,…,N注意,非线性SVM的学习任务和线性SVM(参见定义5.1)很相似。主要的区别在于,学习任务是在变换后的属性(x),而不是在原属性x上执行的。采用5.5.2节和5.5.3节介绍的线性SVM所使用的方法,可以得到该受约束的优化问题的对偶拉格朗日函数:=, yy, ( (x,)(5-56)使用二次规划技术得到λ后,就可以通过下面的方程导出参数w和bw=y(x)(5-57)y(x)(x)+b)-1}=0(5-58)这类似于公式(5-39)和公式(5-40)的线性SVM最后,可以通过下式对检验实例z进行分类:f(z)=sign(w(z)+b)=signy(x)(z)+b(5-59)注意,除了公式(5-57)外,其余的计算公式(5-58)和公式(5-59)都涉及计算变换后的空间中向量对之间的点积(x)(x)(即相似度)。这种运算是相当麻烦的,可能导致维灾难问题。这个问题的一种突破性解决方案是一种称为核技术(kernel trick)的方法。3.核技术点积经常用来度量两个输入向量间的相似度。例如,在2.4.5节介绍的余弦相似度可以定义为规范化后具有单位长度的两个向量间的点积。类似地,点积重(x)中(x)可以看作两个实例x和在变换后的空间中的相似性度量。核技术是一种使用原属性集计算变换后的空间中的相似度的方法。考虑公式(5-55)中的映射函数。两个输入向量u和v在变换后的空间中的点积可以写成如下形式:中(u)(v)=(u2,2,2,2u2,2uu2,1)(2,2,2v,2v2,22,1)=22+u22+2uy1+2u22+2u1u22+1(5-60)=(u·v+1)2该分析表明,变换后的空间中的点积可以用原空间中的相似度函数表示:K(u,v)=d(u).(v)=(u.v+1)2(5-61)这个在原属性空间中计算的相似度函数K称为核函数(kernel function)。核技术有助于处理如何实现非线性SVM的一些问题。首先,由于在非线性SVM中使用的核函数必须满足一个称为 Mercer定理的数学原理,因此我们不需要知道映射函数的确切形式。 Mercer原理确保核函数总可以用某高维空间中两个输入向量的点积表示。SVM核的变后空间也称为再生核希尔伯特空间(Reproducing Kernel Hilbert Space,RKHs)其次,相对于使用变换后的属性集中(x),使用核函数计算点积的开销更小。第三,既然计算在原空间中进行,维灾难问题就可以避免。
图5-29显示了一个非线性决策边界,它是通过使用公式(5-61)给出的多项式核函数的sVM获得的。检验实例z可以通过下式分类: f (z)=sign(y, (x: ()+b)(5-62) =sign(,y, K( z)+b) sign(y (,+1) +b)其中b是使用公式(5-58)得到的参数。非线性SVM得到的决策边界与图5-28a中显示的真实决策边界非常相似。0.9口口0.80.70.60.50.40.30.20.1口0口00.10.20.30.40.50.60.70.80.9图5-29具有多项式核的非线性SVM产生的决策边界4. Mercer定理对非线性SVM使用的核函数主要的要求是,必须存在一个相应的变换,使得计算一对向量的核函数等价于在变换后的空间中计算这对向量的点积。这个要求可以用 Mercer定理形式化地陈述。定理5.1 Mercer定理核函数K可以表示为:K(u,v)=(u)(v)当且仅当对于任意满足g(x)2dx为有限值的函数g(x),则∫K(x,y)g(x)8(y)dxdy≥0满足定理5.1的核函数称为正定(positive definite)核函数。下面给出一些这种函数的例子:K(x,y)=(xy+1(5-63)K(xy)=e(564) K(x,y) =tanh(kx.y-5)(5-65)
例5.6考虑公式(5-63)给出的多项式核函数。令g(x)是一个具有有限L2范数的函数,即g(x)2d<∞+1)(x)(y)dxdy x.y)'g(x)(y)dxdy=()((((由于积分结果非负,因此多项式核函数满足 Mercer定理。5.5.5支持向量机的特征SVM具有许多很好的性质,因此它已经成为广泛使用的分类算法之一。下面简要总结一下SVM的一般特征。(1)SVM学习问题可以表示为凸优化问题,因此可以利用已知的有效算法发现目标函数的全局最小值。而其他的分类方法(如基于规则的分类器和人工神经网络)都采用一种基于贪心学习的策略来搜索假设空间,这种方法一般只能获得局部最优解。(2)SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此,用户必须提供其他参数,如使用的核函数类型、为了引入松弛变量所需的代价函数C等。(3)通过对数据中每个分类属性值引入一个哑变量,SVM可以应用于分类数据。例如,如果婚姻状况有三个值{单身,已婚,离异},可以对每一个属性值引入一个二元变量。(4)本节所给出的SVM公式表述是针对二类问题的。5.8节将给出把SVM扩展到多类问题的一些方法。5.6组合方法除最近邻方法外,本章迄今为止已经介绍的分类技术都是使用从训练数据得到的单个分类器来预测未知样本的类标号本节将介绍一些技术,通过聚集多个分类器的预测来提高分类准确率。这些技术称为组合(ensemble)或分类器组合(classifier combination)方法。组合方法由训练数据构建一组基分类器( base classifier),然后通过对每个基分类器的预测进行投票来进行分类。本节将解释为什么组合方法比任意单分类器的效果好,并提供构建组合分类器的技术。5.6.1组合方法的基本原理下面的例子说明了组合方法为什么能够改善分类器的性能。例5.7考虑25个二元分类器的组合,其中每一个分类器的误差ε均为0.35。组合分类器通过对这些基分类器的预测进行多数表决的方法来预测检验样本的类标号。如果所有基分类器都是等同的,则组合分类器也将对基分类器预测错误的样本误分类。因此,组合分类器的误差率仍然是0.35另一方面,如果基分类器是相互独立的(即它们的误差是不相关的),则仅当超过一
半的基分类器都预测错误时,组合分类器才会作出错误的预测。在这种情况下,组合分类器的误差率为: eensemble= e'(1-E)25-1 =0.06(5-66)远低于基分类器的误差率。图5-30显示对于不同的基分类器误差率(E),25个二元分类器的组合分类器误差率(ensemble对角线表示所有基分类器都是等同的情况,而实线则表示所有基分类器独立时的情况。注意,当E>0.5时,组合分类器的性能比不上基分类器。前面的例子说明,组合分类器的性能优于单个分类器必须满足两个必要的条件:(1)基分类器之间应该是相互独立的;(2)基分类器应当好于随机猜测分类器。实践上,很难保证基分类器之间完全独立。尽管如此,我们看到在基分类器轻微相关的情况下,组合方法可以提高分类的准确率。0.90.8007(60.5米0.400320.10.40.60.8基分类器误差图5-30基分类器和组合分类器误差的比较5.6.2构建组合分类器的方法图5-31给出了组合方法的逻辑视图。其基本的思想是,在原始数据上构建多个分类器,然后在分类未知样本时聚集它们的预测结果。下面构建组合分类器的几种方法。(1)通过处理训练数据集。这种方法根据某种抽样分布,通过对原始数据进行再抽样来得到多个训练集。抽样分布决定一个样本选作训练的可能性大小,并且可能因试验而异。然后,使用特定的学习算法为每个训练集建立一个分类器。装袋(bagging)和提升(boosting)是两种处理训练数据集的组合方法。这些方法将在56.4节和5.6.5节更详细地介绍。(2)通过处理输入特征。在这种方法中,通过选择输入特征的子集来形成每个训练集。子集可以随机选择,也可以根据领域专家的建议选择。一些研究表明,对那些含有大量冗余特征的数据集,这种方法的性能非常好。随机森林(Random forest)就是一种处理输入特征的组合方法,它使用决策树作为基分类器。随机森林将在5.6.6节介绍。(3)通过处理类标号。这种方法适用于类数足够多的情况。通过将类标号随机划分成两个不相交的子集A和A1,把训练数据变换为二类问题。类标号属于子集A0的训练样本指派到类0,而那些类标号属于子集A1的训练样本被指派到类1。然后,使用重新标记过的数据来训练一个基
分类器。重复重新标记类和构建模型步骤多次,就得到一组基分类器。当遇到一个检验样本时,使用每个基分类器C预测它的类标号。如果检验样本被预测为类0,则所有属于A0的类都得到一票。相反,如果它被预测为类1,则所有属于A1的类都得到一票。最后统计选票,将检验样本指派到得票最高的类。后面介绍的错误-纠正输出编码eror--correcting output coding)方法就是这种方法的一个例子。(4)通过处理学习算法。许多学习算法都可以这样来处理:在同一个训练数据集上多次执行算法可能得到不同的模型。例如,通过改变一个人工神经网络的拓扑结构或各个神经元之间联系的初始权值,就可以得到不同的模型。同样通过在树生成过程中注入随机性,可以得到决策树的组合分类器。例如,在每一个结点上,可以随机地从最好的k个属性中选择一个属性,而不是选择该结点上最好的属性来进行划分。原始训练数据步骤1:创建多个数据集D1 D2D1-1步骤2:构建多个分类器 Ct1步骤3:组合分类器图5-31组合学习方法的逻辑视图前三种属于一般性方法,适用于任何分类器,而第四种方法依赖于使用的分类器类型。对于大部分方法,基分类器可以顺序产生(一个接一个)或并行产生(一次性产生)。算法5.5显示了以顺序方式构建组合分类器的步骤。第一步是从原始数据集D中创建一个训练数据集。训练数据集可以与D相同或是D的轻微修改,这取决于所使用的组合方法的类型。训练集的大小一般和原始数据集保持一致,但样本的分布可能不同,即某些样本可能在训练集中多次出现,而有些样本可能一次也不出现。然后,为每个训练集D1构建一个基分类器C组合方法对于不稳定的分类器(unstable classifier)效果较好。不稳定的分类器是对训练集微小的变化都很敏感的基分类器。不稳定的分类器的例子包括决策树、基于规则的分类器和人工神经网络。正如将在5.6.3节中讨论的那样,训练样本的可变性是分类器误差的主要来源之一。通过聚集在不同的训练集上构建的基分类器,有助于减少这种类型的误差。最后,通过组合基分类器C(x)的预测来对检验样本x进行分类: C*(x) Vote(C (x), C2(x), " Ck(x)可以对单个预测值进行多数表决,或用基分类器的准确率对每个预测值进行加权来得到类标号。 PDG
算法5.5组合方法的一般过程1:令D表示原始训练数据集,k表示基分类器的个数,T表示检验数据集 2: for i= to k do3:由D创建训练集D4:由D构建基分类器C1 5: end for6:for每一个检验记录 xETdo 7: C*()= Vote(C (), C2(x),"", Ck(x)) 8: end for5.6.3偏倚-方差分解偏倚-方差分解是分析预测模型的预测误差的形式化方法。下面的例子给出了这种方法的直观解释。图5-32显示了以特定角度发射的射弹的弹道轨迹。假设射弹在某一位置x击中地面,距离目标位置t的距离为d。依赖于发射力,每次试验观察到的距离都不一样。观察到的距离可以分解为几个部分。第一部分称为偏倚(bias),度量目标位置与射弹击中地面的位置之间的平均距离。偏倚量依赖于射弹的发射角度。第二部分称为方差(variance),度量x和射弹击中地面的平均位置之间的偏差。方差可以解释为施加于射弹上的发射力改变的结果。最后,如果目标是不固定的,则观察到距离也受目标位置变化的影响这要考虑与目标位置的可变性相关的噪声(noise)部分。将这些成分放到一块,平均距离可以表示为: df, (, t)= Biase+ Variance Noise,(5-67)其中,f是发射力,0是发射的角度。目标t←→y“方差”“噪声”“偏倚”图5-32偏倚方差分解可以使用同样的方法来分析预测给定样本类标号的任务。对一个给定的分类器,一些预测可能是正确的,而另一些可能完全不沾边。我们可以将一个分类器的期望误差分解为公式(5-67)中的三项和,其中期望误差是分类器误分一个给定样本的概率。本节的剩余部分将介绍分类的偏倚、方差和噪声的含义。通常,训练分类器,以最小化训练误差。然而,分类器的用途在于,必须能够对它从没遇到过的样本的类标号作出预测。这要求分类器将它的决策边界泛化到没有训练样本可用的区域一种依赖于分类器的设计选择的决策。例如,决策树归纳的一个关键设计问题是得到具有最低期望误差的树所需的剪枝量。图5-33显示了两棵决策树T1和T2,它们从同一训练集上得到,但具有不同的复杂度。决策树T2是通过对决策树T1进行剪枝,直到最大深度为2得到的;另一方面,
T1却只在它的决策树上做了很少的剪枝。这些设计选择将导致分类器的偏倚,类似于前面例子中射弹发射的偏倚。一般来说,分类器关于它的决策边界性质所做的假定越强,分类器的偏倚就越大。因此,T2具有更大的偏倚,因为与T1相比,它对决策边界的假定更强(反映在树的大小上)其他可能导致分类器偏倚的设计选择包括人工神经网络的拓扑结构和最近邻分类器中考虑的近邻的个数。Ax2<1.9410。815。。。Ax1<-1.24x1<11.008Ax2<7.45x2<9.25+x1<1.58+-5+(a)决策树T一05101515oAx2<1.9410。8 p8°。8A1<-1.24x1<1100°++(b)决策树T215图5-33从相同训练数据上得到的复杂度不同的两棵决策树分类器的期望误差也受训练数据可变性的影响,因为训练集合的不同的成分可能导致不同的决策边界。这类似于施加于射弹上的发射力不同时x的方差。期望误差的最后一个成分与目标类的固有噪声相关。对某些领域来说,目标类可能是不确定的,即具有相同属性值的实例可能有不同的类标号。即使知道实际的决策边界,这样的误差也是不可避免的。期望误差中的偏倚和方差取决于使用的分类器的类型。图5-34比较了决策树产生的决策边界和1-最近邻分类器产生的决策边界。对于每种分类器,绘制从100个训练集归纳的“平均”模型的决策边界,其中每个训练集包含100个样本。同时用虚线画出从中产生这些数据的实际决策边界。实际决策边界和“平均”决策边界之间的差反映了分类器的偏倚。模型平均后,观察到实际决策边界和1-最近邻分类器的决策边界之间的差别要小于与决策树分类器的差别。这一结果表明1-最近邻分类器的偏倚要低于决策树分类器的偏倚。另一方面,1最近邻分类器对训练样本的组成更加敏感。如果考察从不同训练集上归纳得到的模型,1-最近邻分类器的决策边界的可变性比决策树分类器大。因此,相对于1最近邻分类器,决策树分类器的决策边界具有较低的方差。 PDG
3020810°3330830301030(a)决策树的决策边界(b)1-最近邻的决策边界图5-34决策树和1最近邻分类器的偏倚5.6.4装袋装袋(bagging)又称自助聚集(boot strap aggregating),是一种根据均匀概率分布从数据集中重复抽样(有放回的)的技术。每个自助样本集都和原数据集一样大。由于抽样过程是有放回的,因此一些样本可能在同一个训练数据集中出现多次,而其他一些却可能被忽略。一般来说,自助样本D大约包含63%的原训练数据,因为每一个样本抽到D1的概率为1-(1-1/N),如果N足够大,这个概率将收敛于1-1e≈0.632装袋的基本过程概括在算法5.6中。训练过k个分类器后,测试样本被指派到得票最高的类。算法5.6装袋算法1:设k为自助样本集的数目 2: for i=I to k do3:生成一个大小为N的自助样本集D4:在自助样本集D上训练一个基分类器C 5: end for 6: C'(x) argmax(C,(x)=y)如果参数为真则()=1,否则()=0}为了说明装袋如何进行,考虑表5-4给出的数据集。设x表示一维属性,y表示类标号。假设使用这样一个分类器,它是仅包含一层的二叉决策树,具有一个测试条件x≤k,其中k是使得叶结点熵最小的分裂点。这样的树也称为决策树桩(decision stump)表5-4用于构建装袋组合分类器的数据集例子x0.10.20.30.40.50.60.70.80.91111-1-1-1-1111不进行装袋,能产生的最好的决策树桩的分裂点为x≤0.35或x≤0.75。无论选择哪一个,树的准确率最多为70%假设我们在数据集上应用10个自助样本集的装袋过程,图5-35给出了每轮装袋选择的训练样本。在每个表的右边,给出了分类器产生的决策边界。 PDG
装袋第1轮x0.10.20.20.30.40.40.50.60.90.9x≤0.35y=1x>0.35y=-1装袋第2轮0.10.20.30.40.50.80.9x≤0.65y=1 Lyx>0.65→y=1装袋第3轮x0.10.20.30.40.40.50.70.70.80.9x≤0.35→y=1y111111x>0.35y=-1装袋第4轮x0.10.10.20.40.40.50.0.70.80.9x≤0.3y=1Ly111-1-1x>0.3y=-1装袋第5轮x0.10.10.20.50.60.60.61x≤0.35y=1y11装袋第6轮x0.20.40.50.60.70.70.70.80.9x≤0.75y=-1y1-1-1-1-1-111x>0.75y=1装袋第7轮x0.10.40.40.60.70.80.90.90.9x≤0.75y=-1x>0.75y=1装袋第8轮x0.10.20.50.50.50.70.70.80.9x≤0.75y=-1x>0.75y=1装袋第9轮x0.10.30.40.40.60.70.70.8x≤0.75y=-1x>0.75y=1装袋第10轮x0.10.10.10.10.30.30.80.80.90.9x≤0.05→y=-111x>0.05→y=1图5-35装袋的例子通过对每个基分类器所作的预测使用多数表决来分类表5-4给出的整个数据集。图5-36给出了预测结果。由于类标号是-1或+1,因此应用多数表决等价于对y的预测值求和,然后考察结果的符号(参看图5-36中的第二行到最后一行)。注意,组合分类器完全正确地分类了原始数据集中的10个样本。吗x=0.1x=0.2x=0.3x=0.4x=0.5x0.6x=0.7x0.8x0.9x=1.06849S乙111-1-1-1-1-111111-1111-1-1-1-1-122号12116±16116-111-1111116-62实际类11-11 PDG图5-36使用装袋方法构建组合分类器的例子
前面的例子也说明了使用组合方法的又一个优点:增强了目标函数的表达功能。即使每个基分类器都是一个决策树桩,组合的分类器也能表示一棵深度为2的决策树。装袋通过降低基分类器方差改善了泛化误差。装袋的性能依赖于基分类器的稳定性。如果基分类器是不稳定的,装袋有助于减低训练数据的随机波动导致的误差;如果基分类器是稳定的,即对训练数据集中的微小变化是鲁棒的,则组合分类器的误差主要是由基分类器的偏倚所引起的。在这种情况下,装袋可能不会对基分类器的性能有显著改善,装袋甚至可能降低分类器的性能,因为每个训练集的有效容量比原数据集大约小37%最后,由于每一个样本被选中的概率都相同因此装袋并不侧重于训练数据集中的任何特定实例。因此,用于噪声数据,装袋不太受过分拟合的影响。5.6.5提升提升是一个迭代的过程,用来自适应地改变训练样本的分布,使得基分类器聚焦在那些很难分的样本上。不像装袋,提升给每一个训练样本赋一个权值,而且可以在每一轮提升过程结束时自动地调整权值。训练样本的权值可以用于以下方面。(1)可以用作抽样分布,从原始数据集中提取出自助样本集。(2)基分类器可以使用权值学习有利于高权值样本的模型。本节描述一个算法,它利用样本的权值来确定其训练集的抽样分布。开始时,所有样本都赋予相同的权值1N,从而使得它们被选作训练的可能性都一样。根据训练样本的抽样分布来抽取样本,得到新的样本集。然后,由该训练集归纳一个分类器,并用它对原数据集中的所有样本进行分类。每一轮提升结束时更新训练样本的权值。增加被错误分类的样本的权值,而减小被正确分类的样本的权值。这迫使分类器在随后迭代中关注那些很难分类的样本。下表给出了每轮提升选择的样本。提升(第一轮)73287941063提升(第二轮)5494251742提升(第三轮)4810410454634开始,所有的样本都赋予相同的权值。然而,由于抽样是有放回的,因此某些样本可能被选中多次,如样本3和7。然后,使用由这些数据建立的分类器对所有样本进行分类。假定样本4很难分类,随着它被重复地误分类,该样本的权值在后面的迭代中将会增加。同时,前一轮没有被选中的样本(如样本1和5)也有更好的机会在下一轮被选中,因为前一轮对它们的预测多半是错误的。随着提升过程进行,最难分类的那些样本将有更大的机会被选中。通过聚集每个提升轮得到的基分类器,就得到最终的组合分类器。在过去的几年里,已经开发了几个提升算法的实现。这些算法的差别在于:(1)每轮提升结束时如何更新训练样本的权值;(2)如何组合每个分类器的预测。下面,主要考察称为 AdaBoost的实现。 AdaBoost令{(x,y)j=1,2,,N}表示包含N个训练样本的集合。在 AdaBoost算法中,基分类器C1的重要性依赖于它的错误率。错误率定义为:
 Ei=, (C(,) >(5-68)其中,如果谓词p为真,则I(p)=1,否则为0.基分类器C的重要性由如下参数给出: a; =-In-EE注意,如果错误率接近0,则a具有一个很大的正值,而当错误率接近1时,a有一个很大的负值,如图5-37所示。320-234500.20.40.60.8E图5-37作为训练误差的函数绘制a的曲线参数a也被用来更新训练样本的权值。为了说明这一点,假定表示在第j轮提升迭代中赋给样本(xy)的权值。 AdaBoost的权值更新机制由下式给出:y=ea如果C,(x)=y()(5-69)Z,e如果C,(x1)≠y其中,乙是一个正规因子,用来确保∑)=1。公式(569)给出的权值更新公式增加那些被错误分类样本的权值,并减少那些已经被正确分类的样本的权值。 AdaBoost算法将每一个分类器C的预测值根据a进行加权,而不是使用多数表决的方案。这种机制有助于 AdaBoost惩罚那些准确率很差的模型,如那些在较早的提升轮产生的模型。另外,如果任何中间轮产生高于50%的误差则权值将被恢复为开始的一致值w1=1/N,并重新进行抽样。算法5.7给出了算法的描述。现在看看提升方法在表5-4给出的数据集上是怎么工作的。最初,所有的样本具有相等的权值。三轮提升后,选作训练的样本如图5-38a所示。在每轮提升结束时使用公式(5-69)来更新每一个样本的权值。不使用提升,决策树桩的准确率至多达到70%。使用 AdaBoost,预测结果在图5-39b给出。组合分类器的最终预测结果通过取每个基分类器预测的加权平均得到,显示在图5-39b的最后一行。注意, AdaBoost完全正确地分类了训练数据集中的所有样本。
算法5.7 AdaBoost算法1:w={w=1/Nj=1,2,N}{初始化N个样本的权值。2:令k表示提升的轮数。 3: for i= to k do4:根据w,通过对D进行抽样(有放回)产生训练集D5:在D上训练基分类器C6:用C1对原训练集D中的所有样本分类。7:=(x)),计算加权误差。 8: if e>0.5 then9:w={w=1/Nj=1,2,N}{重新设置N个样本的权值。}10:返回步骤4。 11: end if12:a=2n13:根据公式(5-69)更新每个样本的权值。 14: end for15:c*x)= argmaxa(c,(x)=y)第1轮提升x0.10.40.50.60.60.70.70.70.81y1-1-1-1-1-1-1-11第2轮提升x0.10.10.20.20.20.20.30.30.30.3111111111第3轮提升x0.20.20.40.40.40.40.50.60.60.7y1-11-1-1(a)提升选择的训练记录轮x=0.1x=0.2x=0.3x=0.4x=0.5x0.6x=0.7x-0.8x=0.9x=110.10.10.10:10.10.10.10.10.10.120.3110.3110.3110.010.010.010.010.010.010.0130.0290.0290.0290.2280.2280.2280.2280.0090.0090.009(b)训练记录的权值图5-38提升的例子对提升的一个重要分析结果显示,组合分类器的训练误差受下式的限制: ensemble II[ A-E)](5-70)其中E1是基分类器i的错误率。如果基分类器的错误率低于50%,则E=0.5-%,其中度量了分类器比随机猜测强多少。则组合分类器的训练误差的边界变为: ensembleIV1--2≤exp-2(5-71)如果对所有的i都有<y*,则组合分类器的训练误差呈指数递减,从而导致算法快速收敛。尽管如此,由于它倾向于那些被错误分类的样本,提升技术很容易受过分拟合的影响。
轮123划分点左类右类a0.75-10.05111.7382.77840.34.1195(a)轮=0.1x=0.2x=0.3x=0.4x=0.5x=0.6x=0.7x=0.8x=0.9x=1.01231-111111111111-1111111-3--1-1111和5.165.165.16-3.08-3.08-3.08-3.080.3970.3970.397符号11-1-1-111 (b)图5-39使用 AdaBoost方法构建的组合分类器的例子5.6.6随机森林随机森林(random forest)是一类专门为决策树分类器设计的组合方法。它组合多棵决策树作出的预测,其中每棵树都是基于随机向量的一个独立集合的值产生的,如图5-40所示。与 AdaBoost使用的自适应方法不同, AdaBoost中概率分布是变化的,以关注难分类的样本,而随机森林则采用一个固定的概率分布来产生随机向量使用决策树装袋是随机森林的特例,通过随机地从原训练集中有放回地选取N个样本,将随机性加入到构建模型的过程中。整个模型构建过程中,装袋也使用同样的均匀概率分布来产生它的自助样本。原训练数据随机化步骤1:创建随机向量D Di-1步骤2:使用随机向量建立多决策树步骤3:组合决策树图5-40随机森林已经从理论上证明,当树的数目足够大时,随机森林的泛化误差的上界收敛于下面的表达式:泛化误差≤P(1-s2(5-72)S其中是树之间的平均相关系数,是度量树型分类器的“强度”的量。一组分类器的强度是指 PDG分类器的平均性能,而性能以分类器的余量(M)用概率算法度量:
 M(X, Y) P(=Y)-max P(=Z)(5-73)Z≠Y其中是根据某随机向量构建的分类器对X作出的预测类。余量越大,分类器正确预测给定的样本X的可能性就越大。公式(5-72)是相当直观的,随着树的相关性增加或组合分类器的强度降低,泛化误差的上界趋向于增加。随机化有助于减少决策树之间的相关性,从而改善组合分类器的泛化误差。每棵决策树都使用一个从某固定概率分布产生的随机向量。可以使用多种方法将随机向量合并到树的增长过程中。第一种方法是随机选择F个输入特征来对决策树的结点进行分裂。这样,分裂结点的决策是根据这F个选定的特征,而不是考察所有可用的特征来决定。然后,让树完全增长而不进行任何修剪,这可能有助于减少结果树的偏倚。树构建完毕之后,就可以使用多数表决的方法来组合预测。这种方法称为 Forest--RI,其中RI指随机输入选择。为了增加随机性,可以使用装袋来为 Forest-RI产生自助样本随机森林的强度和相关性都取决于F的大小。如果F足够小,树的相关性趋向于减弱;另一方面,树分类器的强度趋向于随着输入特征数F的增加而提高。作为折中,通常选取特征的数目为F=log2d+1,其中d是输入特征数。由于在每一个结点仅仅需要考察特征的一个子集,这种方法将显著减少算法的运行时间。如果原始特征d的数目太小,则很难选择一个独立的随机特征集合来建立决策树。一种加大特征空间的办法是创建输入特征的线性组合。具体地说,在每一个结点,新特征通过随机选择L个输入特征来构建。这些输入特征用区间[-1,1]上的均匀分布产生的系数进行线性组合。在每个结点,产生F个这种随机组合的新特征并且从中选择最好的来分裂结点。这种方法称为 Forest-RC.生成随机树的第三种方法是:在决策树的每一个结点,从F个最佳划分中随机选择一个。除非F足够大,否则这种方法可能产生比 Forest--ri和 Forest--c相关性更强的树。这种方法也没有 Forest--ri和 Forest-R-rc节省运行时间,因为算法需要在决策树的每个结点考察所有的分裂特征。实验表明,随机森林的分类准确率可以与 AdaBoost相媲美。它对噪声更加鲁棒,运行速度比 AdaBoost快得多。下一节将比较各种组合算法的分类准确率。5.6.7组合方法的实验比较表5-5显示了将决策树分类器与装袋、提升和随机森林的性能相比较的实验结果。每一种组合方法的基分类器都由50棵决策树组成。表中报告的分类准确率通过十折交叉验证得到。注意,在许多数据集上,组合分类器都优于单个的决策树分类器。表5-5决策树分类器与三种组合方法的准确率比较数据集(属性,类,记录)个数决策树(%)装袋(%)提升(%)Rf(%) Anneal(39,6,898)92.09944395.4395.43 australia(15,2,690)85.5187.1085.2285.80 Auto(26,7,205)81.9585.3785.3784.39 Breast(11,2,699)95.1496.4297.2896.14 Cleve(14,2,303)76.2481.5282.1882.18 Credit(16,2,690)85.886.2386.0985.8 Diabetes(9,2,768)72.4076.3073.1875.13 German(21,2,100070.9073.4073.0074.5
(续)数据集(属性,类,记录)个数决策树(%)装袋(%)提升(%)R(%) Glass(10,7,214)67.2976.1777.5778.04 Heart)(14,2,27080.0081.4880.7483.33 Hepatitis(20,2,155)81.9481.2983.8783.23 Horse(23,2,368)85.3385.8781.2585.33 Ionosphere(35,2,351)89.1792.0293.7393.45 Iris(5,3,150)94.6794.6794.0093.33 Labor(17,2,57)78.9584.2189.4784.21 Led7(8,10,3200)73.3473.6673.3473.06 Lymphography(19,4,148)77.0379.0585.1482.43 Pima(9,2,768)74.3576.6973.4477.60 Sonar(61,2,208)78.8578.8584.6285.58 Tic-tac-toe(10,2,958)83.7293.8498.5495.82 Vehicle(19,4,846)71.0474.1178.2574.94 Waveform22,3,5000)76.4483.3083.9084.04 Wine(14,3,178)94.3896.0797.7597.75 Zoo(17,7,101)93.0793.0795.0597.035.7不平衡类问题具有不平衡类分布的数据集在许多实际应用中都会见到。例如,一个监管产品生产线的下线产品的自动检测系统会发现,不合格产品的数量远远低于合格产品的数量。同样,在信用卡欺诈检测中,合法交易远远多于欺诈交易。在这两个例子中,属于不同类的实例数量都不成比例。不平衡程度随应用不同而不同一一个在六西格玛原则下运行的制造厂可能会在一百万件出售给顾客的产品中发现四件不合格品,而信用卡欺诈的量级可能是百分之一。尽管它们不常出现,但是在这些应用中,稀有类的正确分类比多数类的正确分类更有价值。然而,由于类分布是不平衡的,这就给那些已有的分类算法带来了很多问题。准确率经常用来比较分类器的性能,然而它可能不适合评价从不平衡数据集得到的模型。例如,如果1%的信用卡交易是欺骗行为,则预测每个交易都合法的模型具有99%的准确率,尽管它检测不到任何欺骗交易。另外,用来指导学习算法的度量(如决策树归纳中的信息增益)也需要进行修改,以关注那些稀有类。检测稀有类的实例好比大海捞针。因为这些实例很少出现,因此描述稀有类的模型趋向于是高度特殊化的例如,在基于规则的分类器中,为稀有类提取的规则通常涉及大量的属性,并很难简化为更一般的、具有很高覆盖率的规则(不像那些多数类的规则)。这样的模型也很容易受训练数据中噪声的影响。因此,许多已有的算法不能很好地检测稀有类的实例。本节将给出一些为处理不平衡类问题而开发的方法。首先,介绍除准确率外的一些可选度量,以及一种称为ROC分析的图形化方法。然后,描述如何使用代价敏感学习和基于抽样的方法来改善稀有类的检测。5.7.1可选度量 POG由于准确率度量将每个类看得同等重要,因此它可能不适合用来分析不平衡数据集。在不平
衡数据集中,稀有类比多数类更有意义。对于二元分类,稀有类通常记为正类,而多数类被认为是负类。表5-6显示了汇总分类模型正确和不正确预测的实例数目的混淆矩阵。表5-6类不是同等重要的二类分类问题的混淆矩阵预测的类+实际+(TP) f-(FN)的类 f--(TN)在谈到混淆矩阵列出的计数时,经常用到下面的术语。·真正( true positive,TP)或f++,对应于被分类模型正确预测的正样本数。假负( false negative,FN)或f,对应于被分类模型错误预测为负类的正样本数。·假正(《fasepostivFP4《,)或f+,对应于被分类模型错误预测为正类的负样本数。·真负(《turenegative,N《,TN)或f,对应于被分类模型正确预测的负样本数。混淆矩阵中的计数可以表示为百分比的形式。真正率(true positive rate,TPR)或灵敏度(sensitivity)定义为被模型正确预测的正样本的比例,即: TPR=TP/(TP+FN)同理,真负率(Ture Negative Rate,TNR)或特指度( specificity)定义为被模型正确预测的负样本的比例,即: TNR=TN/(TN+FP)最后,假正率(false positive rate,FPR)定义为被预测为正类的负样本比例,即 FPR=(TN+FP)而假负率( false negative rate,FNR)定义为被预测为负类的正样本比例,即: FNR=FN/(TP+FN)召回率(recall)和精度(precision)是两个广泛使用的度量,用于成功预测一个类比预测其他类更加重要的应用。下面给出精度(p)和召回率(r)的形式化定义: P-TP(5-74) TP+ FP TP r=(5-75) TP+ FN精度确定在分类器断言为正类的那部分记录中实际为正类的记录所占的比例。精度越高,分类器的假正类错误率就越低。召回率度量被分类器正确预测的正样本的比例。具有高召回率的分类器很少将正样本误分为负样本。实际上,召回率的值等于真正率。可以构造一个基线模型,它最大化其中一个度量而不管另一个。例如,将每一个记录都声明为正类的模型具有完美的召回率,但它的精度却很差。相反,将匹配训练集中任何一个正记录的检验记录都指派为正类的模型具有很高的精度,但召回率很低。建一个最大化精度和召回率的模型是分类算法的主要任务之一。精度和召回率可以合并成另一个度量,称为F1度量。
 2rp2×TP F1r+P 2xTP+FP+FN(5-76)原则上,F1表示召回率和精度的调和均值,即:2F1=11二+ rp两个数x和y的调和均值趋向于接近较小的数。因此,一个高的F1度量值确保精度和召回率都比较高。下面的例子比较了调和均值、几何均值和算术均值。例5.8考虑两个正数a=1和b=5。它们的算术均值=(a+b)/2=3,几何均值g=ab=2.236。它们的调和均值=(2×1×5)/6=1667,它比算术均值和几何均值更接近于a和b中的较小值。更一般地,可以用F度量考察召回率和精度之间的折中:(2+1=(82(2+1)×TP FB r+ (B2+1)TP+B2FP+FN(5-77)精度和召回率分别是B=0和B=∞时F的特例。低值使得F值接近于精度,高值使得F值接近于召回率。更一般地,俘获F值和准确率的度量是加权准确率度量,由下式定义:加权准确率= WTP+(5-78) WTP+W2FP+ FN+加权准确率和其他的性能度量值之间的关系汇总在下表中:度量 w2 w3 w4召回率110精度101F值2+1 B21000准确率11115.7.2接受者操作特征曲线接受者操作特征(receiver operating characteristic,C)曲线是显示分类器真正率和假正率之间折中的一种图形化方法。在一个ROC曲线中,真正率(TPR)沿y轴绘制,而假正率(FPR)显示在x轴上。沿着曲线的每个点对应于一个分类器归纳的模型。图5-41显示了一对分类器M1和M2的ROC曲线。ROC曲线上有几个关键点,它们都有公认的解释。(TPR=0,FPR=0):把每个实例都预测为负类的模型。(TPR=1,FPR=1):把每个实例都预测为正类的模型。(R=1,FPR=0):理想模型。 PDG
0.90.80.70.6试 M0.50.4030.20.10.10.20.30.40.50.60.70.80.9假正率图5-41两个不同分类器的ROC曲线一个好的分类模型应该尽可能靠近图的左上角,而一个随机猜测的模型应位于连接点(TPR=,P=0)和(TPR=1,FPR=1)的主对角线上。随机猜测是指以固定的概率p把记录分为正类,而不考虑它的属性集。例如,考虑一个包含n个正实例和n个负实例的数据集随机分类器期望正确地分类pn+个正实例,而误分pn_个负实例,因此,分类器的TPR是(pn)n+=p,而它的FPR是(pn)n=p。由于TPR和FPR相等,因此随机预测分类器的ROC曲线总是位于主对角线上。ROC曲线有助于比较不同分类器的相对性能。在图5-41中,当FPR小于0.36时,M1要好于M2,而FPR大于0.36时M2较好。很明显,这两个分类器各有各的长处。ROC曲线下方的面积(AUC)提供了评价模型的平均性能的另一种办法。如果模型是完美的,则它在ROC曲线下方的面积等于1。如果模型仅仅是简单地随机猜测,则ROC曲线下方的面积等于0.5。如果一个模型好于另一个,则它的ROC曲线下方面积较大。产生ROC曲线为了绘制ROC曲线,分类器应当能够产生连续值输出,可以用来从最有可能到最不可能分为正类的记录,对它的预测排序。这些输出可能对应于贝叶斯分类器产生的后验概率或人工神经网络产生的数值输出。下面给出产生ROC曲线的过程。(1)假定为正类定义了连续值输出,对检验记录按它们的输出值递增排序。(2)选择秩最低的检验记录(即输出值最低的记录),把选择的记录以及那些秩高于它的记录指派为正类。这种方法等价于把所有的检验实例都分为正类。因为所有的正检验实例都被正确分类,而所有的负测试实例都被误分,因此TPR=FPR=1(3)从排序列表中选择下一个检验记录,把选择的记录以及那些秩高于它的记录指派为正类,而把那些秩低于它的记录指派为负类通过考察前面选择的记录的实际类标号来更新TP和FP计数。如果前面选择的记录为正类,则TP计数减少而FP计数不变。如果前面选择的记录为负类,则FP计数减少而TP计数不变。(4)重复步骤3并相应地更新TP和FP计数,直到最高秩的记录被选择。 PDG(5)根据分类器的FPR画出TPR曲线。
图5-42显示了一个如何计算ROC曲线的例子。检验集中有5个正实例和5个负实例。检验记录的类标号显示在表的第一行。第二行对应于每个记录排序后的输出值,例如,它们可能对应于朴素贝叶斯分类器产生的后验概率P(+|x)。接下来的六行包括T计数、FP计数、计数和FN计数,以及它们对应的TPR和FPR。于是从左到右填表。开始时,所有的记录都被预测为正类,因此TP=FP=5,TPR=FPR=1。然后,指派有最低输出值的检验实例为负类。因为选择的记录实际上是正类,因此TP计数从5减到4,而FP计数不变,并相应地更新TPR和FPR值。重复这个过程直至到达列表的末尾,这时TPR=,FPR=0.这个例子的ROC曲线如图5-43所示。类+0250.430.530.760.850.850850.870.930.951.00t544333322105544311000t01234455522223345tpr10.80.80.60.60.60.60.40.40.2fpr110.80.80.60.40.20.2图5-42构造ROC曲线0.90.80.70.60.50.4030.2100.10.20.30.40.50.60.70.80.9图5-43图5-42所示数据集的ROC曲线5.7.3代价敏感学习代价矩阵对将一个类的记录分类到另一个类的惩罚进行编码。令C(i表示预测一个类记录为j类的代价。使用这种记号,C(+,-)是犯一个假负错误的代价,而C(-+是产生一个假警告的代价。代价矩阵中的一个负项表示对正确分类的奖励。给定一个N个记录的检验集,模型M的总代价是: C:(M) =TPXC(+, +)+FPxC-, +)+FNXC(+, -) TNXC(-,-)(5-79)在0/1代价矩阵中,即C(+,+)=C(-,-)=0而C(,-)=C(-,+)=1,可以证明总代价等价于误分类的数目。 PDG C.(M) =Ox(TP+TN)+ 1x(FP+FN)=NXErr(5-80)其中,Err是分类器的误差率。
例5.9考虑表5-7所示的代价矩阵。犯假负错误的代价是犯假警告的100倍。换句话说,漏检测出任何1个正样本与犯100个假警告一样糟糕给定具有表58所示的混淆矩阵的分类模型,每一个模型的总代价是:C(M)=150×(-1)+60×1+40×100=3910C(M2)=250×(-1)+5×1+45×100=4255注意,尽管模型M2同时改善了它的真正计数和假正计数,但是仍然较差,因为这些改善是建立在增加代价更高的假负错误之上。而标准的准确率度量更趋向于M2优于M1表5-7例5.9的代价矩阵预测的类类=+类=实际的类类=+1100类=-10表5-8两个分类模型的混淆矩阵预测的类预测的类模型M1模型M类+类类+类实际的类类+15040类+25045实际的类类60250类-5200代价敏感分类技术在构建模型的过程中考虑代价矩阵,并产生代价最低的模型。例如,如果假负错误代价最高,则学习算法将通过向负类扩展它的决策边界来减少这些错误,如图5-44所示。这种方法产生的模型覆盖更多的正类样本尽管其代价是产生了一些额外的假警告。图5-44修改决策边界(从B1到B2)以减少分类器的假负错误有许多办法将代价信息加入分类算法中。例如,在决策树归纳过程中,代价信息可以用来:(1)选择用以分裂数据的最好的属性;(2)决定子树是否需要剪枝;(3)处理训练记录的权值,使得学习算法收敛到代价最低的决策树;(4)修改每个叶结点上的决策规则。为了解释最后一种方法,令p(it)表示属于叶结点t的类i的训练记录所占的比例。如果下面的条件成立,一个典型的二元
分类问题的决策规则将正类指派到结点tp(+t)>p(-t)p(+>(1-p(+t→2p(+)>1→p(+>0.5(5-81)前面的决策规则表明,叶结点的类标号取决于到达该结点的训练记录的多数类。注意,这个规则假定对于正样本和负样本,误分的代价都是相同的。这个决策规则等价于4.3.5节的公式(4-48)给出的表达式。代价敏感的学习算法不是采用多数表决,而是赋予结点t类标号i如果它最小化如下表达式: c()=()c(j,i)(5-82)在C(+,+)=C(-,-)=0的情况下,叶结点t被指派为正类,如果:p(+)c(+,-)>p()c(-,+)→p(+t)c(+,-)>(1-p(+t(-,+)→p(+)>C(-,+(5-83)(-+)+c(+,-)这个表达式说明,可以把决策规则的阈值从0.修改为C(-,+)(C(+,-)+C(-,+)),得到一个代价敏感的分类器。如果C(-,+)<C(+,-),则阈值将小于0.5。这个结果是有意义的,因为一个假负错误比一个假警告代价高。降低阈值将向负类扩展决策边界,如图5-44所示5.7.4基于抽样的方法抽样是处理不平衡类问题的另一种广泛使用的方法。抽样的主要思想是改变实例的分布,从而帮助稀有类在训练数据集中得到很好的表示。用于抽样的一些现有的技术包括不充分抽样(undersampling)、过分抽样(oversampling)和两种技术的混合。为了解释这些技术,考虑一个包含100个正样本和1000个负样本的数据集。在不充分抽样的情况下,取100个负样本的一个随机抽样,与所有的正样本一起形成训练集。这种方法的一个潜在的问题是,一些有用的负样本可能没有选出来用于训练,因此会生成一个不太优的模型。克服这个问题的一个可行的方法是多次执行不充分抽样,并归纳类似于组合学习方法的多分类器。也可以使用聚焦的不充分抽样( focused undersampling),这时抽样程序精明地确定应该被排除的负样本,如那些远离决策边界的样本。过分抽样复制正样本,直到训练集中正样本和负样本一样多。图5-45说明了使用分类法(如决策树)构建决策边界时过分抽样对其产生的影响。不使用过分抽样,只有在图5-45a中左下角的那些正样本被正确分类,位于图中间的正样本没有被正确分类,因为没有足够的样本来确定分离正样本和负样本的新决策边界。过分抽样提供了需要的额外样本确保围绕该正样本的决策边界不被剪除,如图545b所示。
(a)不使用过分抽样(b)使用过分抽样图5-45解释稀有类过分抽样的影响然而,对于噪声数据,过分抽样可能导致模型过分拟合,因为一些噪声样本也可能被复制多次。原则上,过分抽样没有向训练集中添加任何新的信息。对正样本的复制仅仅是阻止学习算法剪掉模型中描述包含很少训练样本的区域的那部分(即小的不相连的部分)。增加的正样本有可能增加建立模型的计算时间。混合方法使用二者的组合,对多数类进行不充分抽样,而对稀有类进行过分抽样来获得均匀的类分布。不充分抽样可以采用随机或聚焦的子抽样。另一方面,过分抽样可以通过复制已有的正样本,或在已有的正样本的邻域中产生新的正样本来实现。在后一种方法中,必须首先确定每一个已有的正样本的k最近邻,然后,在连接正样本和一个k最近邻的线段上的某个随机点产生一个新的正样本。重复该过程,直到正样本的数目达到要求。不像数据复制方法,新的样本能够向外扩展正类的决策边界,与图5-44中的方法类似。然而,这种方法仍然可能受模型过分拟合的影响。5.8多类问题本章描述的一些分类技术(如支持向量机和 AdaBoost)原先是为二元分类问题设计的。然而在解决许多现实世界的问题(如特征识别、人脸识别和文本分类等)时,输入数据都被划分为多于两个类。本节给出扩展二元分类器以处理多类问题的方法。为了说明这些方法,令Y={y1,y2,y}是输入数据的类标号的集合。第一种方法将多类问题分解成K个二类问题。为每一个类y∈Y创建一个二类问题,其中所有属于y的样本都被看作正类,而其他样本作为负类。然后,构建一个二元分类器,将属于y的样本从其他类中分离出来。这种方法称为一对其他(1-r)方法。第二种方法称为一对一(1-1)方法,它构建K(K-1)/2个二类分类器,每一个分类器用来区分一对类(yy)当为类(yy)构建二类分类器时,不属于y或y的样本被忽略掉。不论1-1还是1-r方法,都是通过组合所有二元分类器的预测对检验实例分类。组合预测的典型做法是使用投票表决,将检验样本指派到得票最多的类。在1-r方法中,如果一个样本被分为负类,则除正类之外的所有类都得到一票。然而,这种方法可能导致不同类的平局。另一种可能是将二元分类器的输出变换成概率估计,然后将检验实例指派到具有最高概率的类。例5.10考虑一个多类问题,其中={y1,2,y3,y4}.假设根据1-r方法将一个检验实例分类为(+,-,-,-)。换言之,当y1作为正类时它被分为正类,而当y2,y3,y4作为正类时它被分为
负类。使用简单的多数表决,既然y1得到最高的投票数4,而其他类仅仅得到3票,因此检验实例被分类为y1假定使用1-1方法将检验实例分类如下:二类分+:y1+:y1+:y1+:y2+:y2+:y3类器类对-:y2-:y3-:y4-:y3:y4 - y分类++表的上面两行对应选来构建分类器的类对(y,),而最后一行表示检验实例的预测类。在组合预测后,y1和y4都得到2票,而y2和y3仅仅得到1票。依赖于平局处理策略,检验实例被分为y1或y4口纠错输出编码前面介绍的两种方法的一个问题是,它们对二元分类的错误太敏感。对于例5.10中给出的1-r方法,如果有一个二元分类器作出了错误的预测,则组合分类器可能就以平局或一个错误的预测结束。例如,假设由于第三个分类器的误分,检验实例被分为(+,-,+,)。这时,除非考虑与每个类预测相关联的概率,否则很难决定样本应分为y1类还是y3类。纠错输出编码(error-correcting output coding,ECOC)方法提供了一种处理多类问题的更鲁棒的方法。这种方法受信息理论中通过噪声信道发送信息的启发。其基本思想是借助于代码字向传输信息中增加一些冗余,从而使得接收方能发现接收信息中的一些错误,而且如果错误量很少,还可能恢复原始信息。对于多类学习,每个类y用一个长度为n的唯一的位串来表示,称为它的代码字。然后训练n个二元分类器,预测代码字串的每个二进位。检验实例的预测类由这样的代码字给出,该代码字到二元分类器产生的代码字海明距离最近。注意,两个位串之间的海明距离是它们的不同的二进位的数目。例5.11考虑一个多类问题,其中Y={y1,y2,y3y4假定使用下面的7位代码字对类进行编码:类y代码字111111y200001y3001101y40101010代码字的每个二进位用来训练一个二元分类器。如果一个检验实例被二类分类器分类为(0,1,1,11,1,1),则该代码字与y1之间的海明距离为1,而与其他类之间的海明距离为3因此,该检验实例被分类为y1口纠错码的一个有趣的性质是,如果任意代码字对之间的最小海明距离为d,则输出代码任意L(d-1)/2个错误可以使用离它最近的代码字纠正在例5.11中,因为任意代码字对之间的最小海明距离为4,因此组合分类器可以容忍7个二类分类器中的1个出错。如果出错的分类器超过一个,则组合分类器将不能校正这些错误。一个很重要的问题是如何为不同的类设计合适的代码字集合。从编码理论来说,目前已经开
发出了大量的能够产生具有有限海明距离的n位代码字的算法。然而,这些算法的讨论已经超出本书范围。值得一提的是,为通信任务设计纠错码明显不同于多类学习的纠错码。对通信任务,代码字应该最大化各行之间的海明距离,使得纠错可以进行。然而,多类学习要求将代码字列向和行向的距离很好地分开。较大的列向距离可以确保二元分类器是相互独立的,而这正是组合学习算法的一个重要要求。文献注释 Mitchell[208]从机器学习的角度极好地介绍了许多分类技术。对分类的更广泛的论述还可以从下面文献中找到:Duda等[180]、ebb[219] Fukunaga[187]、 Bishop[159]、 Hastie等192]CherkassyMuler[167}WitenFrank[221《Hand[90]anambcr[189]和Mulier[167]、itten和Frnk[22]、Hand等[190]、Han和Kamber[189]以及 Dunham[181].基于规则分类器的直接方法采用顺序覆盖模式来归纳分类规则。Holt的1R[195]是最简单的基于规则的分类器,因为它的规则集只包含单个的规则。尽管简单,但是Holt发现,对于一些属性和类标号显示出很强的一对一联系的数据集,1R的性能和其他分类器相当。基于规则的分类器的其他例子包括IREP184]、 RIPPER17]、cn2[168,169]、AQ207]、rise[76]和 ITRULE[214]。表5-9显示了其中4个分类器的特征对比。表5-9各种基于规则分类器的对比 RIPPERCN2(无序的)CN2(有序的) AQR规则增长策略一般到特殊一般到特殊一般到特殊一般到特殊(以一个正样本作种子)评估度量FOIL信息增益拉普拉斯熵和似然率真正类的个数停止规则增所有样本都属于同无性能提高无性能提高规则只覆盖正类长条件一个类规则剪枝减少错误无无无实例删除正的和负的正的正的正的和负的停止增加规 Error>50%或基于无性能提高无性能提高所有正样本都被则条件 MDL覆盖规则集剪枝替换或修改规则统计检验无无搜索策略贪心定向搜索定向搜索定向搜索对基于规则的分类器,规则的前件可以推广到包含任意命题或一阶逻辑表达式(例如,Horn子句)。对基于一阶逻辑规则分类器感兴趣的读者可以参阅相关文献,如[208]或关于归纳逻辑程序设计的大量文献[209] Quinlan[211]给出了C4.5算法,从决策树中提取分类规则。 Andrews等[157]给出了从人工神经网络中提取规则的间接的方法。 Cover和Hart[172]从贝叶斯定理的角度给出了最近邻分类方法的综述。Aha在[155]中提供了基于实例方法的理论和实验评价。 PEBLS是Cost和 Salzberg[171提出的一种最近邻分类算法,它能处理包含标称属性的数据集。在 PEBLS中,赋予每个训练实例一个权重因子,而因子取决于实例帮助作出正确预测的次数。Han等188]给出了一个调整权重的最近邻算法,使用一种贪心的、爬山式的优化算法来学习特征的权重。朴素贝叶斯分类器有许多作者绍,ly[203】 amoni Sebastiani2122 Lewis[204],以及 Domingos和 Pazzani[178]。尽管朴素贝叶斯分类器中使用的独立性假设看上去很
不现实,但是在诸如文本分类等应用领域,该方法的性能却出奇地好。通过允许某些属性相互依赖,贝叶斯信念网络提供了一种更灵活的方法。 Heckerman在[194]中给出了关于贝叶斯信念网络的很好的指南。 Vapnik[217,218]已经写了两本关于支持向量机(sM)的权威书籍。关于SVM和核方法的其他一些有用的资源包括 Cristianini Shawe-Taylor-[173]以及 Scholkopf和 Smola[213]的书。另外还有一些关于SVM的评论文章,包括 Burges[164], Bennet等[158], Hearst[93] Mangasarian[205] Dietterich[174]给出了机器学习中组合方法的概述。 Breiman[161]提出了装袋方法。 Freund和 Schapire[186]提出 AdaBoost算法 Arcing是自适应再抽样和组合(adaptive resampling and combining)的缩写,它是 Breiman[162]提出的提升算法的一个变形。它对训练实例赋予不一致的权重来对数据进行再抽样,从而建立训练数据集的组合分类器。不像 AdaBoost在决定测试样本的类标号时,基分类器的投票是不加权的。随机森林方法在Breiman[163]中有所介绍关于挖掘稀有类和不平衡数据集的工作可以参看 Chawla等[166]和 Weiss[220]写的综述。许多作者都介绍过挖掘不平衡数据集的基于样本的方法,如 Kubat和 Matwin[202]、 Japkowitz[196]以及 DrummondHolte和[179]. Joshi等[199]讨论了提升算法对稀有类建模的局限性。挖掘稀有类的其他一些算法包括 SMOTE[165]、 PNrule[98]和CREDOS200存在一些适合分不平衡类问题的可选度量。精度、召回率和F1度量是信息检索中广泛使用的度量[216]。ROC分析原先用于信号检测理论。 Bradley[160]研究了以ROC曲线下方面积作为机器学习算法的性能度量的应用。 Provost和 Fawcett210]中提出了使用ROC曲线的凸起来比较分类器性能的方法。 Ferri等[185开发了一种在决策树分类器上进行ROC分析的方法。他们还提出了在树增长过程中使用ROC曲线下方面积(AUC)作为分裂标准。 Joshi[197]从分析稀有类的角度考察了这些度量的性能。关于代价敏感学习的大量文献可以在ICML2000关于代价敏感学习研讨会的联机论文集中找到。 Elkan[182]研究了代价矩阵的特征。 MargineantuDietterich和[206]考察了将代价信息合并到C4.5学习算法中的多种方法,包括包装的方法,基于类分布的方法和基于损失(loss- based)的方法。其他一些独立于算法的代价敏感学习方法包括 AdaCost[183], Meta Cost[177]costing[222]关于多类学习,也存在大量文献。这包括Hstie和astieibshirani[191《AIwein[156]Kong[191]、Allwein等[156]、Kong和 Dietterich[201]以及Tax和Duin[215]的著作。 DietterichBakiri和[175]提出了纠错输出编码(ECOC)方法。他们也介绍了适合解决多类问题的代码设计技术。参考文献 [155] D. W. Aha. A study of instance-based algorithms for supervised learning tasks: mathematical, empirical, and psychological evaluations. PhD thesis University of California, Irvine, 1990. [156] E.. Allwein,. E. Schapire, and Y. Singer Reducing Multiclass to Binary: Unifying Approach to Margin Classifiers. Joumal of Machine Learning Research, 1: 113-141, 2000. [157] R. Andrews, J. Diederich, and A. Tickle A Survey and Critique of Techniques For Extracting Rules From Trained Artificial Neural Networks Knowledge Based Systems, 8(6): 373-389, 1995. [158] K. Bennett and C. Campbell. Support Vector Machines: Hype or Hallelujah. SIGKDD Explorations,2(2):1-13,2000. [159] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, U.K.,1995.
 [160] A. P. Bradley. The use of the area under the ROC curve in the Evaluation of Machine Learning Algorithms. Pattern Recognition, 30(7): 1145-1149, 1997. [161] L. Breiman. Bagging Predictors. Machine Learning, 24(2): 123-140, 1996. [162] L. Breiman. Bias, Variance, and Arcing Classifiers. Technical Report 486, University of California Berkeley, CA, 1996 [163] L. Breiman. Random Forests. Machine Learning, 45(1):5-32,2001 [164) C. J. C. Burges. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2(2):121-167,1998 [165] N. V. Chawla, K. W. Bowyer, L. Hall, and W. P. Kegelmeyer. SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16: 321-357, 2002. [166] N. V. Chawla, N. Japkowicz, and A. Kolcz Editorial: Special Issue on Learning from Imbalanced Data Sets. SIGKDD Explorations, 6(1): 1-6, 2004. [167] V. Cherkassky and F. Mulier. Learning from Data: Concepts, Theory, and Methods. Wiley Interscience, 1998. [168] P. Clark and. Boswell. Rule Induction with CN2: Some Recent Improvements In Machine Learning: Proc. of the 5th European Conf. (EWSL-91), pages 151-163,1991 [169] P. Clark and T. Niblett. The CN2 Induction Algorithm. Machine Learning, 3(4): 261-283,1989 [170] W.W. Cohen. Fast Effective Rule Induction. In Proc. of the 12th Intl. Conf on Machine Learning, pages 115-123, Tahoe City, CA, July 1995 [171] S. Cost and S. Salzberg. A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. Machine Learning, 10:57-78, 1993 [172] T. M. Cover and P. E. Hart. Nearest Neighbor Pattern Classification. Knowledge Based System8(6):373-389,1995 [173] N. Cristianini and J. Shawe-Taylor An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000. [174] T. G. Dietterich. Ensemble Methods in Machine Learning. In First Intl. Workshop on Multiple Classifier Systems, Cagliari, Italy, 2000. [175] T. G. Dietterich and G. Bakiri. Solving Multiclass Learning Problems via Error-Correcting Output Codes. Journal of Artificial Intelligence Research, 2: 263-286,1995 [176] P. Domingos. The RISE system: Conquering without separating. In Proc. of the 6th IEEE Intl. Conf. on Tools with Artificial Intelligence, pages 704-707, New Orleans, LA, 1994. [177] P. Domingos. MetaCost: A General Method for Making Classifiers Cost-Sensitive. In Proc. of the Sth Intl. Conf. on Knowledge Discovery and Data Mining, pages 155-164, San Diego, CA, August1999 [178] P. Domingos and M. Pazzani. On the Optimality of the Simple Bayesian Classifier under Zero-One Loss. Machine Learning, 29(2-3):103-130, 1997. [179] C. Drummond and R. C. Holte. C4.5, Class imbalance, and Cost sensitivity: Why under-sampling beats over-sampling. In ICML'2004 Workshop on Leaming from Imbalanced Data Sets II Washington, DC, August 2003. [180] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley& Sons, Inc., New York, 2nd edition, 2001. [181] M.. Dunham. Data Mining: Introductory and Advanced Topics. Prentice Hall, 2002 [182] C. Elkan. The Foundations of Cost-Sensitive Learning. In Proc. of the 17th Intl. Joint Conf. on Artificial Intelligence, pages 973-978, Seattle, WA, August 2001 [183] W. Fan, S. J. Stolfo, J. Zhang, and P.. Chan. AdaCost: misclassification costsensitive boosting. In Proc. of the 16th Intl. Conf. on Machine Learing, pages 97-105, Bled, Slovenia, June 1999. [184] J. Furnkranz and G. Widmer. Incremental reduced error pruning. In Proc. of the 11th Intl. Conf. on Machine Learning, pages 70-77, New Brunswick, NJ, July 1994 [185] C. Ferri,. Flach, and J. Hernandez-Orallo. Learning Decision Trees Using the Area Under the ROC Curve. In Proc. of the 19th Intl. Conf. on Machine Leaming, pages 139-146, Sydney, Australia, July2002.
 [186] Y. Freund and R. E. Schapire. decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1): 119-139, 1997 [187] K. Fukunaga. Introduction to Statistical Paitern Recognition. Academic Press, New York, 1990. [188] E.-H. Han, G. Karypis, and V. Kumar. Text Categorization Using Weight Adjusted k-Nearest Neighbor Classification. In Proc. of the 5th Pacific-Asia Conf. on Knowledge Discovery and Data Mining, Lyon, France, 2001. [189] J. Han and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers, San Francisco, 2001 [190] D. J. Hand, H. Mannila, and P. Smyth Principles of Data Mining. MIT Press, 2001. [191] T. Hastie and R. Tibshirani. Classification by pairwise coupling. Annals of Statistics, 26(2):451-471,1998. [192] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, Prediction. Springer, New York, 2001. [193] M. Hearst. Trends Controversies: Support Vector Machines. IEEE Intelligent Systems, 13(4): 18-28,1998. [194] D. Heckerman. Bayesian Networks for Data Mining. Data Mining and Knowledge Discovery, 1(1):79-119,1997. [195] R. C. Holte. Very Simple Classification Rules Perform Well on Most Commonly Used Data sets. Machine Leaming, 11: 63-91, 1993. [196] N. Japkowicz. The Class Imbalance Problem: Significance and Strategies. In Proc. of the 2000 Intl. Conf. on Artificial Intelligence: Special Track on Inductive Learning, volume 1, pages 111-117, Las Vegas, NV, June 2000. [197] M.. Joshi. On Evaluating Performance of Classifiers for Rare Classes. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, Maebashi City, Japan, December 2002 [198] M. V. Joshi, R. C. Agarwal, and V. Kumar. Mining Needles in a Haystack: Classifying Rare Classes via Two-Phase Rule Induction. In Proc of 2001 ACM-SIGMOD Intl. Conf. on Management of Data, pages 91-102, Santa Barbara, CA, June 2001. [199] M. V. Joshi, R. C. Agarwal, and V. Kumar. Predicting rare classes: can boosting make any weak learner strong? In Proc. of the 8th Intl. Conf. on Knowledge Discovery and Data Mining, pages 297- 306, Edmonton, Canada, July 2002. [200] M. V. Joshi and V. Kumar. CREDOS: Classification Using Ripple Down Structure (A Case for Rare Classes). In Proc. of the SIAM Intl. Conf. on Data Mining, pages 321-332, Orlando, FL, April 2004 [201] E. B. Kong and T. G. Dietterich. Error-Correcting Output Coding Corrects Bias and Variance. In Proc. of the 12th Intl. Conf. on Machine Learning, pages 313-321, Tahoe City, CA, July 1995. [202] M. Kubat and S. Matwin. Addressing the Curse of Imbalanced Training Sets: One Sided Selection. In Proc. of the 14th Intl. Conf. on Machine Learning, pages 179-186, Nashville, TN, July 1997. [203] P. Langley, W. Iba, and K. Thompson. An analysis of Bayesian classifiers. In Proc. of the 10th National Conf. on Artificial Intelligence, pages 223-228, 1992. [204]. D. Lewis. Naive Bayes at Forty: The Independence Assumption in Information Retrieval. In Proc. of the 10th European Conf. on Machine Learning(ECML 1998), pages 4-15, 1998. [205] O. Mangasarian. Data Mining via Support Vector Machines. Technical Report Technical Report 01-05, Data Mining Institute, May 2001 [206] D. D. Margineantu and T. G. Dietterich Learning Decision Trees for Loss Minimization in Multi-Class Problems. Technical Report 99-30-03, Oregon State University, 1999. [207]. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. The Multi-Purpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains. In Proc. of 5th National Conf. on Artificial Intelligence, Orlando, August 1986. [208] T. Mitchell. Machine Learning. McGraw-Hill, Boston, MA, 1997 [209] S. Muggleton. Foundations of Inductive Logic Programming. Prentice Hall, Englewood Cliffs, NJ,1995 [210] F. J. Provost and T. Fawcett. Analysis and Visualization of Classifier Performance: Comparison under
 Imprecise Class and Cost Distributions. In Proc of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 43-48, Newport Beach, CA, August 1997. [2111 J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan-Kaufmann Publishers, San Mateo, CA,1993 [212] M. Ramoni and P. Sebastiani. Robust Bayes classifiers. Artificial Intelligence, 125: 209-226, 2001. (213] B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2001 [214] P. Smyth and R.. Goodman. An Information Theoretic Approach to Rule Induction from Databases. IEEE Trans. on Knowledge and Data Engineering, 4(4): 301-316, 1992. [215] D. M. J. Tax and R.. W. Duin. Using Two-Class Classifiers for Multiclass Classification. In Proc. of the 16th Intl. Conf. on Pattern Recognition (ICPR 2002), pages 124-127, Quebec, Canada, August2002 (2161 C. J. van Rijsbergen. Information Retrieval. Butterworth-Hei, Newton, MA, 1978. [217] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995. [218]. Vapnik. Statistical Learning Theory John Wiley& Sons, New York, 1998. [219] A. R. Webb. Statistical Pattern Recognition. John Wiley Sons, 2nd edition, 2002. [220] G. M. Weiss. Mining with Rarity: A Unifying Framework. SIGKDD Explorations, 6(1):7-19, 2004. [221] I. H. Witten and E. Frank. Data Mining Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, 1999. [222] B. Zadrozny,. C. Langford, and N. Abe. Cost-Sensitive Learning by Cost-Proportionate Example Weighting. In Proc. of the 2003 IEEE Intl. Conf. on Data Mining, pages 435-442, Melbourne, FL, August 2003.习题1.考虑一个二值分类问题,属性集和属性值如下。空调={可用,不可用}引擎={好,差}。行车里程={高,中,低}生锈={是,否}假设一个基于规则的分类器产生的规则集如下。行车里程=高→价值=低行车里程=低→价值=高空调=可用,引擎好价值=高空调=可用,引擎=差→价值=低空调=不可用→价值=低(a)这些规则是互斥的吗?(b)这些规则集是完全的吗?(c)规则需要排序吗?(d)规则集需要默认类吗?2. RIPPER算法(Cohen[170])是早期算法ire( Furnkranz和 Widmer[184])的扩展。两个算法都使用减少误差剪枝(reduced-error- pruning)方法来确定一个规则是否需要剪枝。减少误差剪枝方法使用一个确认集来估计分类器的泛化误差。考虑下面两个规则:
R1:A→CR2:AB→CR2是由R1的左边添加合取项B得到的。现在的问题是,从规则增长和规则剪枝的角度来确定R2是否比R1好。为了确定规则是否应该剪枝,IREP计算下面的度量: VIREP= P+(N-n) P+N其中,P是确认集中正例的总数,N是确认集中反例的总数,p是确认集中被规则覆盖的正例数,而n是确认集中被规则覆盖的反例数。实际上, VIREP类似于确认集的分类准确率。IREP偏向于 VIREP值较高的规则。另一方面, RIPPER使用下面的度量来确定规则是否应该剪枝: VRIPPER= p+n(a)假设R1覆盖350个正例和150个反例,而R2覆盖300个正例和50个反例计算R2相对于R1的FOIL信息增益。(b)考虑一个确认集,包含500个正例和50个反例。假设R1覆盖200个正例和50个反例,R2覆盖100个正例和5个反例。计算R1和R2的Vep,IREP偏向于哪个规则?(c)计算(b)问题中的VRIPPER, RIPPER偏向于哪个规则?3.C4.5规则是从决策树生成规则的间接方法的一个实现,而RIPPER是从数据中生成规则的直接方法的一个实现。(a)讨论两种方法的优缺点。(b)考虑一个数据集,其中类的大小差别很大即有些类比其他类大得多)在为较小的类寻找高准确率规则方面,哪一种方法(c4.5规则和 RIPPER)更好?4.考虑一个训练集,包含100个正例和40个反例。对于下面的候选规则:R1:A→+(覆盖4个正例和1个反例)R2:B→+(覆盖个30个正例和10个反例)R3:C→+(覆盖100个正例和90个反例)根据下面的度量,确定最好规则和最差规则。(a)规则准确率。(b)FO信息增益(c)似然比统计量。(d)拉普拉斯度量。(e)m度量(k=2且p+=0.2)5.图5-4给出了分类规则R1、R2和R3的覆盖率。根据以下度量确定最好规则和最差规则。(a)似然比统计量。(b)拉普拉斯度量。(c)m度量(k=2且p+=0.58) PDG(d)发现规则R1后的准确率,这里不删除R1覆盖的任何样例。(e)发现规则R1后的准确率,这里仅删除R1覆盖的正例。
(f)发现规则R1后的准确率,这里删除R1覆盖的所有正例和反例。6.(a)假设本科生中抽烟的比例是15%,研究生中抽烟的比例是23%。如果大学生中研究生占1/5,其余是本科生,那么抽烟的学生是研究生的概率是多少?(b)根据(a)中的信息,随机选择一个大学里的学生,那么,该生是研究生或本科生的可能性哪个大?(c)同(b),假设学生是个抽烟者。(d)假设30%的研究生住学生宿舍,只有10%的本科生住学生宿舍。如果一个学生抽烟又住宿舍,那么他(她)是研究生或本科生的可能性哪个大?可以假设住宿舍的学生和抽烟的学生相互独立。7.考虑表5-10中的数据集。表5-10习题7的数据集记录ABC类000+1-1234567890001111001100001(a)估计条件概率P(A+),P(B|+),P(C+),P(A-),(b-)和P(C-)(b)根据(a)中的条件概率,使用朴素贝叶斯方法预测测试样本(A=0,B=1,C=0)的类标号。(c)使用m估计方法(p=1/2且m=4)估计条件概率。(d)同(b),使用(c)中的条件概率。(e)比较估计概率的两种方法。哪一种更好?为什么?8.考虑表5-11中的数据集。表5-11习题8的数据集实例ABC类12345678901011010000100010111100110001 POG101
(a)估计条件概率P(A=1|+),P(B=1|+),Pc=1+),p(=1-),(b=1-)和P(C=1-)(b)根据(a)中的条件概率,使用朴素贝叶斯方法预测测试样本(A=1,B=1,C=1)的类标号。(c)比较P(A=1),P(B=1)和P(A=1,B=1)。陈述A、B之间的关系(d)对P(A=1),P(B=0)和P(A=1,B=0)重复(c)的分析。(e)比较P(A=1,B=1类=+)与P(A=1类=+和P(B=1类=+)给定类+,变量A、B条件独立吗?9.(a)解释朴素贝叶斯分类器在图5-46数据集上的工作过程。(b)如果每个类进一步分割,得到四个类(A1A2,B1,B2),朴素贝叶斯会工作得更好吗?(c)决策树在该数据集上怎样工作(两类问题)?四个类呢?属性区分属性噪声属性1A1类A A2记录 B1类B B2图5-46习题9的数据集10.使用下面的信息,重复例5.3的分析,寻找决策边界位置。(a)先验概率P(鳄鱼)=2xP(美洲鳄)。(b)先验概率P(美洲鳄)=2xP(鳄鱼)(c)先验概率相同,但标准差不同,例如,(鳄鱼)=4,o(美洲鳄)=211.图5-47给出了表5-12中的数据集对应的贝叶斯信念网络(假设所有属性都是二元的)(a)画出网络中每个结点对应的概率表。(b)使用贝叶斯网络计算P(引擎=差,空调=不可用)行车里程引擎空调车的价值图5-47贝叶斯信念网络
表5-12习题11的数据集行车里程引擎空调车的价值=高的记录数车的价值=低的记录数好可用3高高高高低低低低好不可用差可用1差不可用0好可用好不可用差可用42540122差不可用12.给定图5-48所示的贝叶斯网络,计算下面概率。(a)P(B=好,F=空,G=空,S=是(b)P(B=差,F=空,G=非空,S=否)(c)如果电池是差的,计算车发动起来的概率。P(B=差)=0.1P(F=空)=0.2电池燃料仪表P(G=空B=好,F=非空)=0.1P(G=空B=好,F=空)=0.8P(G=空B=差,F=非空)=0.2P(G=空B=差,F=空)=0.9发动P(S=否B=好,F=非空)=0.1P(S=否B=好,F=空)=0.8P(S=否B=差,F=非空)=0.9P(S=否|B=差,F=空)=1.0图5-48习题12的贝叶斯信念网络13.考虑表5-13中的一维数据集。(a)根据1-最近邻、3-最近邻、5-最近邻及9最近邻,对数据点x=5.0分类(使用多数表决)。(b)使用5.2.1节中描述的距离加权表决方法重复前面的分析。表5-13习题13的数据集x0.53.04.54.64.95.25.35.57.09.5y+14.5.2节中描述的最近邻算法可以扩充以便处理标称属性Cost和 Salzberg[171]提出了一个最近邻算法的变形,称作 PEBLS(并行的基于实例的学习系统, Parallel Examplar-Based- Learning System),它使用改进的值差度量(odified Value Difference Metric,mvdm①B表示电池,F表示燃料,G表示仪表,表示发动译者注
来度量一个标称属性的两个值之间的距离。给定一对标称属性值V和V2,二者之间的距离定义为:d(V,v2)=m-n2(5-84) nn2I其中,n是类i中具有属性值V的样例数,n是具有属性值V的样例总数。考虑图5-9中贷款分类问题的训练集。使用MVDM来计算属性是否有房和婚姻状况的每一对属性值之间的距离。15.对下面的每一个布尔函数,说出问题是否线性可分。 (a)A AND B AND C (b)NOT A AND B (c)(A OR B)AND(A OR C) (d)(A XOR B)AND(A OR B)16.(a)说明感知器模型怎样表示两个布尔变量之间的AND和OR函数(b)评论使用线性函数作为多层神经网络的激活函数的缺点。17.请评价两个分类模型M1和M2的性能。所选择的测试集包含26个二值属性,记作A到Z。表5-14是模型应用到测试集时得到的后验概率(图中只显示正类的后验概率)因为这是二类问题,所以P(-)=1-P(+),P(-A,,Z=1-(,假设需要从正类中检测实例。(a)画出M1和M2的ROC曲线(画在同一幅图中)。哪个模型更好?给出理由。(b)对模型M1,假设截止阈值t=0.5。换句话说,任何后验概率大于t的测试实例都被看作正例。计算模型在此阈值下的精度、召回率和F度量。(c)对模型M2使用相同的截止阈值重复(b)的分析。比较两个模型的F度量值,哪个模型更好?所得结果和从ROC曲线中得到的结论一致吗?(d)使用阈值t=0.1对模型M1重复(b)的分析t=0.5和t=0.1哪一个阈值更好?该结果和你从ROC曲线中得到的一致吗?表5-14习题17的后验概率实例真实类P(+Z,M)P(+Z,M2)+0.730.61+0.690.031234567890.440.680.550.31+0.670.450.470.090.080.380.150.05+0.450.01100.350.0418.下面的数据集包含两个属性X和Y,两个类标号“+”和“-”。每个属性取三个不同的值:0,1或2“+”类的概念是Y=1,“-”类的概念是X=0vX=2
 XY实例数1000120120120001112200000000010010001001000100(a)建立该数据集的决策树。该决策树能捕捉到“+”和“-”的概念吗?(b)决策树的准确率、精度、召回率和F1度量各是多少?(注意,精度、召回率和F1度量均是对“+”类定义。)(c)使用下面的代价函数建立新的决策树:0如果i=j1如果i=+,j=C(i)=-实例个数如果i=-,j=++实例个数(提示:只需改变原决策树的叶结点。)新决策树能捕捉到“+”的概念吗?(d)新决策树的准确率、精度、召回率和F1度量各是多少?19.(a)考虑两类问题的代价矩阵。设C(+,+)=c(-,-)=p,(+,-)=(-,+)=q,且q>p证明:最小化代价函数等价于最大化分类器准确率。(b)证明:代价矩阵是比例不变量(scale-invariant-)例如,如果代价矩阵的大小进行伸缩C(i,)→BC(i),B是伸缩因子,则决策阈值(公式(5-82))保持不变(c)证明:代价矩阵是平移不变量( translation-invariant-)即代价矩阵的每一个元素都加上一个常量,不会影响决策阈值(公式(5-82))。20.考虑任务:为随机数据建立分类器,其中属性值随机产生,与类标号无关。假设数据集包含两个类“+”和“-”的记录。数据集的一半用于训练,而剩下的一半用于测试。(a)假设数据集中正例和反例的数目相等,决策树分类器把所有测试记录预测为正类。则分类器在测试数据上的期望误差率是多少?(b)假设分类器把每个测试记录预测为正类的概率是0.8,预测为负类的概率是0.2,重复前面的分析。(c)假设23的数据属于正类,1/3的数据属于负类。分类器把每个测试记录预测为正类的期望误差是多少?(d)假设分类器把每个测试记录预测为正类的概率是2/3,预测为负类的概率是1/3,重复前面的分析。21.导出不可分数据的线性SVM的对偶拉格朗日函数(dual Lagrangian),其中目标函数是: f()=+C5 POG22.考虑XOR问题,其中有四个训练点:(1,1,-),(1,0,+),(0,1,+),(0,0,-)
把数据转化为下面的特征空间:=(1,2x,2x2,2x2,x2,x2)找出转化后空间的最大边缘线性决策边界。23.给定图5-49所示的数据集,解释在此数据集上,决策树、朴素贝叶斯和k最近邻分类器是怎样工作的。属性属性识别属性噪声属性识别属性噪声属性】1类A类A记录记录111类B类B1(a)人造数据集1(b)人造数据集2属性识别属识别属性集1性集2噪声属性类A类B类A类B类A60%填140%填1类A>类B类A类B类A类B世腻记录类A类B类A类B类A类B类B类类B类A类B40%填160%填1属性(c)人造数据集3(d)人造数据集4类A类B类A世类B类B属性属性(e)人造数据集5人造数据集6图5-49习题23的数据集
第章关联分析:基本概念和算法许多商业企业在日复一日的运营中积聚了大量的数据。例如,食品商店的收银台每天都收集大量的顾客购物数据。表6-1给出一个这种数据的例子,通常称作购物篮事务(market basket tran- saction)表中每一行对应一个事务,包含一个唯一标识TID和给定顾客购买的商品的集合。零售商对分析这些数据很感兴趣,以便了解他们的顾客的购买行为。可以使用这种有价值的信息来支持各种商务应用,如市场促销,库存管理和顾客关系管理等表6-1购物篮事务的例子 TID项集面包,牛奶12345{面包,尿布,啤酒,鸡蛋}牛奶,尿布,啤酒,可乐}面包,牛奶,尿布,啤酒}面包,牛奶,尿布,可乐本章主要是介绍一种称作关联分析(association analysis)的方法,用于发现隐藏在大型数据集中的有意义的联系。所发现的联系可以用关联规则( association rule)或频繁项集的形式表示。例如,从表6-1所示的数据中可以提取出如下规则:{尿布}→{啤酒该规则表明尿布和啤酒的销售之间存在着很强的联系,因为许多购买尿布的顾客也购买啤酒。零售商们可以使用这类规则,帮助他们发现新的交叉销售商机。除了购物篮数据外,关联分析也可以应用于其他领域,如生物信息学、医疗诊断、网页挖掘和科学数据分析等。例如,在地球科学数据分析中,关联模式可以揭示海洋、陆地和大气过程之间的有趣联系。这样的信息能够帮助地球科学家更好地理解地球系统中不同的自然力之间的相互作用。尽管这里提供的技术一般可以都用于更广泛的数据集,但是为了便于解释,讨论将主要集中在购物篮数据上。在对购物篮数据进行关联分析时,需要处理两个关键的问题:第一,从大型事务数据集中发现模式可能在计算上要付出很高的代价;第二,所发现的某些模式可能是虚假的,因为它们可能是偶然发生的。本章的其余部分主要是围绕这两个问题组织。本章的第一部分解释关联分析的基本概念和用来有效地挖掘这种模式的算法。第二部分处理发现模式的评估问题,以避免产生虚假结果。 PDG
6.1问题定义这一节讲述关联分析中使用的基本术语,并提供该任务的形式化描述。二元表示购物篮数据可以用表6-2所示的二元形式来表示,其中每行对应一个事务,而每列对应一个项。项可以用二元变量表示,如果项在事务中出现,则它的值为1,否则为0因为通常认为项在事务中出现比不出现更重要,因此项是非对称(asymmetric)二元变量或许这种表示是实际购物篮数据极其简单的展现,因为这种表示忽略数据的某些重要的方面,如所购商品的数量和价格等。处理这种非二元数据的方法将在第7章讨论。表6-2购物篮数据的二元0/1表示TID面包牛奶尿布啤酒鸡蛋可乐110111011101110111001000001015项集和支持度计数令I={i,i2,ia}是购物篮数据中所有项的集合,而T={t1,t2,tn}是所有事务的集合。每个事务t包含的项集都是的子集。在关联分析中,包含0个或多个项的集合被称为项集(itemset)。如果一个项集包含个项,则称它为k项集例如,{啤酒,尿布,牛奶}是一个3项集。空集是指不包含任何项的项集。事务的宽度定义为事务中出现项的个数。如果项集X是事务t的子集,则称事务t包括项集X。例如,在表6-2中第二个事务包括项集{面包,尿布},但不包括项集{面包,牛奶}。项集的一个重要性质是它的支持度计数,即包含特定项集的事务个数。数学上,项集X的支持度计数a(可以表示为: (X)= XCtisLiETH其中,符号表示集合中元素的个数。在表62显示的数据集中,项集{啤酒,尿布,牛奶}的支持度计数为2,因为只有2个事务同时包含这3个项。关联规则(association rule)关联规则是形如→的蕴涵表达式,其中X和Y是不相交的项集,即Y=。关联规则的强度可以用它的支持度( support)和置信度(confidence)度量。支持度确定规则可以用于给定数据集的频繁程度,而置信度确定Y在包含X的事务中出现的频繁程度。支持度(s)和置信度(c)这两种度量的形式定义如下:s(x→y)=(x(6-1)Nc(x→y)=o(xuy)(6-2) o(X)而事务的总数是5,所以规则的支持度为2/5=0.4。规则的置信度是项集(牛奶,尿布,啤酒}的支PDG例6.1考虑规则{牛奶,尿布}→{啤酒}。由于项集牛奶,尿布,啤酒}的支持度计数是2,持度计数与项集{牛奶,尿布}支持度计数的商。由于存在3个事务同时包含牛奶和尿布,所以该规则的置信度为2/3=0.67口
为什么使用支持度和置信度?支持度是一种重要度量,因为支持度很低的规则可能只是偶然出现。从商务角度来看,低支持度的规则多半也是无意义的,因为对顾客很少同时购买的商品进行促销可能并无益处(6.8节讨论的情况则是例外)因此,支持度通常用来删去那些无意义的规则。此外,正如6.2.1节所示,支持度还具有一种期望的性质,可以用于关联规则的有效发现。另一方面,置信度度量通过规则进行推理具有可靠性。对于给定的规则X→Y,置信度越高,Y在包含X的事务中出现的可能性就越大。置信度也可以估计Y在给定X下的条件概率。应当小心解释关联分析的结果。由关联规则作出的推论并不必然蕴涵因果关系。它只表示规则前件和后件中的项明显地同时出现。另一方面,因果关系需要关于数据中原因和结果属性的知识,并且通常涉及长期出现的联系(例如,臭氧损耗导致全球变暖)关联规则挖掘问题的形式描述关联规则的挖掘问题可以形式地描述如下:定义6.1关联规则发现给定事务的集合T,关联规则发现是指找出支持度大于等于 minsup并且置信度大于等于 minconf的所有规则,其中 minsup和 minconf是对应的支持度和置信度阈值。挖掘关联规则的一种原始方法是:计算每个可能规则的支持度和置信度。但是这种方法的代价很高,令人望而却步,因为可以从数据集提取的规则的数目达指数级。更具体地说,从包含d个项的数据集提取的可能规则的总数为:R=34-2+1+1(6-3)此式的证明作为习题留给读者(见本章习题5)。即使对于表6-1所示的小数据集,这种方法也需要计算36-27+1=602条规则的支持度和置信度。使用minsup=20%和 minconf=50%,80%以上的规则将被丢弃,使得大部分计算是无用的开销。为了避免进行不必要的计算,事先对规则剪枝,而无须计算它们的支持度和置信度的值将是有益的。提高关联规则挖掘算法性能的第一步是拆分支持度和置信度要求。由公式(6-1)可以看出,规则x→Y的支持度仅依赖于其对应项集XY的支持度。例如,下面的规则有相同的支持度,因为它们涉及的项都源自同一个项集{啤酒,尿布,牛奶}:{啤酒,尿布}→{牛奶},{啤酒,牛奶}→{尿布},{尿布,牛奶}→{啤酒},{啤酒}→{尿布,牛奶}{牛奶}→{啤酒,尿布}尿布}→{啤酒,牛奶}如果项集{啤酒,尿布,牛奶}是非频繁的,则可以立即剪掉这6个候选规则,而不必计算它们的置信度值。因此,大多数关联规则挖掘算法通常采用的一种策略是,将关联规则挖掘任务分解为如下两个主要的子任务。(1)频繁项集产生:其目标是发现满足最小支持度阈值的所有项集,这些项集称作频繁项集( frequent itemset)(2)规则的产生:其目标是从上一步发现的频繁项集中提取所有高置信度的规则,这些规则称作强规则( strong rule)通常,频繁项集产生所需的计算开销远大于产生规则所需的计算开销。频繁项集和关联规产
生的有效技术将分别在6.2节和6.3节讨论。6.2频繁项集的产生格结构(lattice structure)常常被用来枚举所有可能的项集。图6-1显示I={a,b,c,d,e}的项集格。一般来说,一个包含k个项的数据集可能产生2-1个频繁项集,不包括空集在内。由于在许多实际应用中k的值可能非常大,需要探查的项集搜索空间可能是指数规模的。 (null ab()( ad(ae )()( be )()(ce )(de (abc)abd(abe)(acd)(ace )ade))(bcebdecde (abcd)abce )abde (acdebcde (abcde图6-1项集的格发现频繁项集的一种原始方法是确定格结构中每个候选项集(candidate itemset)的支持度计数。为了完成这一任务,必须将每个候选项集与每个事务进行比较,如图6-2所示。如果候选项集包含在事务中,则候选项集的支持度计数增加。例如,由于项集{面包,牛奶}出现在事务1,4和5中,其支持度计数将增加3次。这种方法的开销可能非常大,因为它需要进行O(NMw)次比较,其中N是事务数,M=2-1是候选项集数,而w是事务的最大宽度。候选事务 TID项1面包,牛奶 N32面包,尿布,啤酒,鸡蛋牛奶,尿布,啤酒,可乐4面包,牛奶,尿布,啤酒↓5面包,牛奶,尿布,可乐 POG图6-2计算候选项集的支持度
有几种方法可以降低产生频繁项集的计算复杂度。(1)减少候选项集的数目(M)下一节介绍的先验(apriori)原理,是一种不用计算支持度值而删除某些候选项集的有效方法。(2)减少比较次数。替代将每个候选项集与每个事务相匹配,可以使用更高级的数据结构,或者存储候选项集或者压缩数据集,来减少比较次数。这些策略将在6.2.4节和6.6节讨论。6.2.1先验原理本节描述如何使用支持度度量,帮助减少频繁项集产生时需要探查的候选项集个数。使用支持度对候选项集剪枝基于如下原理。定理6.1先验原理如果一个项集是频繁的则它的所有子集一定也是频繁的。为了解释先验原理的基本思想,考虑图6-3所示的项集格。假定{c,d,e}是频繁项集。显而易见,任何包含项集{c,d,e}的事务一定包含它的子集{c,d},{c,e},{d,e},{c},{d}和{e}这样,如果{c,d,e}是频繁的,则它的所有子集(图6-3中的阴影项集)一定也是频繁的。a ab )ac)(ad )()(b becdcedeabcabdabeacdaceadebcde频繁项集 (abcde图6-3先验原理的图示。如果{c,d,e}是频繁的,则它的所有子集也是频繁的相反,如果项集{a,b}是非频繁的,则它的所有超集也一定是非频繁的。如图64所示,一发现{a,b}是非频繁的,则整个包含{a,b}超集的子图可以被立即剪枝。这种基于支持度度量修剪指数搜索空间的策略称为基于支持度的剪枝support-based- pruning)这种剪枝策略依赖于支持度度量的一个关键性质,即一个项集的支持度决不会超过它的子集的支持度。这个性质也称支持度度量的反单调性(anti-monotone) PDG
 (null非频繁项集ab ac( ad( ae bc)bd (be )cdcede Cabeaod acdaceadeb abce abde acde bcde被剪枝的超集 (abede图6-4基于支持度的剪枝的图示。如果{a,b}是非频繁的,则它的所有超集也是非频繁的定义6.2单调性令1是项的集合,J=2是1的幂集。度量f是单调的(或向上封闭的),如果vx,y∈j(xc)→f(x)≤f(Y)这表明如果X是Y的子集,则f(X)一定不超过f(Y)。另一方面,f是反单调的(或向下封闭的),如果vxy∈j:(xcy)→f(r)≤f(x)表示如果X是Y的子集,则f(Y)一定不超过f(X)正如下节所述,任何具有反单调性的度量都能够直接结合到挖掘算法中,可以对候选项集的指数搜索空间进行有效地剪枝。6.2.2 Apriori算法的频繁项集产生 Apriori算法是第一个关联规则挖掘算法,它开创性地使用基于支持度的剪枝技术,系统地控制候选项集指数增长。对于表6-1中所示的事务,图6-5给出 Apriori算法频繁项集产生部分的一个高层实例。假定支持度阈值是60%,相当于最小支持度计数为3初始时每个项都被看作候选1项集。对它们的支持度计数之后,候选项集{可乐}和{鸡蛋}被丢弃,因为它们出现的事务少于3个。在下一次迭代,仅使用频繁1-项集来产生候选2项集,因为先验原理保证所有非频繁的1项集的超集都是非频繁的。由于只有4个频繁1项集,因此算法产生的候选2项集的数目为C2=6。计算它们的支持度值之后,发现这6个候选项集中的2个,啤酒,面包}和{啤酒,牛奶}是非频繁的。剩下的4个候选项集是频繁的,因此用来产生候选3项集。不使
用基于支持度的剪枝,使用该例给定的6个项,将形成C=20个候选3项集。依据先验原理,只需要保留其子集都频繁的候选3项集。具有这种性质的唯一候选是{面包,尿布,牛奶}候选1项集项计数啤酒最小支持度计数=3面包可乐尿布34244候选2项集牛奶鸡蛋项集计数啤酒,面包}{啤酒,尿布(啤酒,牛奶{面包,尿布{面包,牛奶232333因支持度低而被删除的项集(尿布,牛奶候选3项集项集计数面包,尿布牛奶}3图6-5使 Apriori用算法产生频繁项集的例子通过计算产生的候选项集数目,可以看出先验剪枝策略的有效性。枚举所有项集(到3项集)的蛮力策略将产生C+C2+C=6+15+20=4个候选;而使用先验原理,将减少为C+2+1=6+6+1=13个候选。甚至在这个简单的例子中,候选项集的数目也降低了68%算法6.1中给出了 Apriori算法产生频繁项集部分的伪代码令C为候选k项集的集合,而Fk为频繁k项集的集合:·该算法初始通过单遍扫描数据集,确定每个项的支持度。一旦完成这一步,就得到所有频繁1-项集的集合F1(步骤1和步骤2)接下来,该算法将使用上一次迭代发现的频繁(k-1)项集,产生新的候选k项集(步骤5)候选的产生使用 apriori-gen函数实现,将在6.2.3节介绍。为了对候选项的支持度计数,算法需要再次扫描一遍数据集(步骤6~10)。使用子集函数确定包含在每一个事务t中的Ck中的所有候选k项集。子集函数的实现在6.2.4节介绍。计算候选项的支持度计数之后,算法将删去支持度计数小于 minsup的所有候选项集(步骤12)。当没有新的频繁项集产生,即F=时,算法结束(步骤13) Apriori算法的频繁项集产生的部分有两个重要的特点:第一,它是一个逐层(level--wise)算法,即从频繁1项集到最长的频繁项集,它每次遍历项集格中的一层;第二,它使用产生测试generate--and-test)策略来发现频繁项集在每次迭代之后,新的候选项集都由前一次迭代发现的频繁项集产生,然后对每个候选的支持度进行计数,并与最小支持度阈值进行比较。该算法需要的总迭代次数是kmax+1,其中kma是频繁项集的最大长度。 PDG
算法6.1 Apriori算法的频繁项集产生1:k=12:Fk={i| ie(i)≥ minsup}发现所有的频繁1-项集} 3: repeat4:k=k+15:ck= apriori--gen(F-)产生候选项集}6:for每个事务 Tdo7:C= subset(t)识别属于t的所有候选}8:for每个候选项集 cEC do9:o(c)=a(c)+1支持度计数增值 10: end for 11: end for12:Fk={ CE(c)≥Nx minsup)【提取频繁k-项集} 13: until Fk=14: Result=uFk6.2.3候选的产生与剪枝算法6.1步骤5的 apriori--gen函数通过如下两个操作产生候选项集。(1)候选项集的产生。该操作由前一次迭代发现的频繁(k-1)项集产生新的候选k项集。(2)候选项集的剪枝。该操作采用基于支持度的剪枝策略,删除一些候选k项集。为了解释候选项集剪枝操作,考虑候选k项集X={i1,i2,}。算法必须确定它的所有真子集X-{}(Vj=1,2,,k)是否都是频繁的,如果其中一个是非频繁的,则X将会被立即剪枝这种方法能够有效地减少支持度计数过程中所考虑的候选项集的数量。对于每一个候选k项集,该操作的复杂度是O然而,随后我们将明白,并不需要检查给定候选项集的所有k个子集。如果k个子集中的m个用来产生候选项集,则在候选项集剪枝时只需要检查剩下的k-m个子集。理论上,存在许多产生候选项集的方法。下面列出了对有效的候选项集产生过程的要求。(1)它应当避免产生太多不必要的候选。一个候选项集是不必要的,如果它至少有一个子集是非频繁的。根据支持度的反单调属性,这样的候选项集肯定是非频繁的。(2)它必须确保候选项集的集合是完全的,即候选项集产生过程没有遗漏任何频繁项集。为了确保完全性,候选项集的集合必须包含所有频繁项集的集合,即k: Cko(3)它应该不会产生重复候选项集。例如:候选项集{a,b,c,d}可能会通过多种方法产生,如合并{a,b,c}和{d},合并{b,d}和{a,c}合并{c}和{a,b,d}等。候选项集的重复产生将会导致计算的浪费,因此为了效率应该避免。接下来,将简要地介绍几种候选产生过程,其中包括 apriori-g-gen函数使用的方法蛮力方法蛮力方法把所有的k项集都看作可能的候选,然后使用候选剪枝除去不必要的候选(见图6-6)。第k层产生的候选项集的数目为C,其中d是项的总数。虽然候选产生是相当简单的,但是候选剪枝的开销极大,因为必须考察的项集数量太大。设每一个候选项集所需的计算量为O(k),这种方法的总复杂度为ka)od2 PDG
候选产生项集{啤酒,面包,可乐}啤酒,面包,尿布啤酒,面包,牛奶项啤酒,面包,鸡蛋}啤酒,可乐,尿布项啤酒,可乐,牛奶啤酒{啤酒,可乐,鸡蛋面包候选剪枝可乐啤酒,尿布,牛奶啤酒,尿布,鸡蛋}项集尿布啤酒,牛奶,鸡蛋}面包,尿布,牛奶牛奶面包,可乐,尿布}鸡蛋面包,可乐,牛奶面包,可乐,鸡蛋}面包,尿布,牛奶面包,尿布,鸡蛋{面包,牛奶,鸡蛋{可乐,尿布,牛奶{可乐,尿布,鸡蛋可乐,牛奶,鸡蛋尿布,牛奶,鸡蛋图6-6产生候选3项集的蛮力方法Fk-1F1方法另一种的产生候选项集的方法是用其他频繁项来扩展每个频繁(k-1)项集。图6-7显示了如何用频繁项(如面包)扩展频繁2-项集{啤酒,尿布}产生候选3项集{啤酒,尿布,面包}。这种方法将产生O(FF个候选项集,其中表示频繁j项集的个数。这种方法总复杂度是O(FF)频繁2项集项集啤酒,尿布面包,尿布面包,牛奶}尿布,牛奶候选产生项集候选剪枝啤酒,尿布,面包项集啤酒,尿布,牛奶面包,尿布,牛奶频繁1项集面包,尿布,牛奶项面包,牛奶,啤酒啤酒面包尿布牛奶图6-7通过合并频繁(k-1)项集和频繁1-项集生成和剪枝候选k项集。注意:某些候选是不必要,因为它们的子集是非频繁的这种方法是完备的,因为每一个频繁k项集都是由一个频繁(k-1)-项集和一个频繁1项集组成的。因此,所有的频繁k项集是这种方法所产生的候选k项集的一部分然而,这种方法很难避免重复地产生候选项集。例如,项集{面包,尿布,牛奶不仅可以由合并项集{面包,尿布}和{牛奶得到,而且还可以由合并(面包,牛奶}和{尿布}得到,或者由合并{尿布,牛奶和面包}得到。避免产生重复的候选项集的一种方法是确保每个频繁项集中的项以字典序存储,每个
频繁(k-1)-项集X只用字典序比X中所有的项都大的频繁项进行扩展。例如,项集{面包,尿布}可以用项集{牛奶}扩展,因为“牛奶”(Milk)在字典序下比“面包”( Bread)和“尿布”(Diapers)都大。然而,不应当用{面包}扩展{尿布,牛奶}或用{尿布}扩展{面包,牛奶},因为它们违反了字典序条件。尽管这种方法比蛮力方法有明显改进,但是仍会产生大量不必要的候选。例如,通过合并{啤酒,尿布}和{牛奶}而得到的候选是不必要的,因为它的一个子集{啤酒,牛奶}是非频繁的。有几种启发式方法能够减少不必要的候选数量。例如,对于每一个幸免于剪枝的候选k项集,它的每一个项必须至少在k-1个(k-1)项集中出现否则,该候选就是非频繁的。例如,项集{啤酒,尿布,牛奶}是一个可行的候选3项集,仅当它的每一个项(包括“啤酒”)都必须在两个频繁2项集中出现。由于只有一个频繁2项集包含“啤酒”,因此所有包含“啤酒”的候选都是非频繁的。Fk-1xfk-1方法 apriori-g函数-gen的候选产生过程合并一对频繁(k-1)项集,仅当它们的前k-2个项都相同。令A={a1,a2,,ak-1}和B={b1,b2,bk-}是一对频繁(k-1)项集,合并A和B,如果它们满足如下条件:a=b(i=1,2,,k-2)并且ak-1≠bk-1在图6-8中,频繁项集{面包,尿布}和{面包,牛奶合并,形成了候选3项集{面包,尿布,牛奶}。算法不会合并项集{啤酒,尿布}和{尿布,牛奶},因为它们的第一个项不相同。实际上,如果啤酒,尿布,牛奶}是可行的候选,则它应当由{啤酒,尿布}和{啤酒,牛奶}合并而得到。这个例子表明了候选项产生过程的完全性和使用字典序避免重复的候选的优点。然而,由于每个候选都由一对频繁(k-1)项集合并而成,因此需要附加的候选剪枝步骤来确保该候选的其余k-2个子集是频繁的。频繁2项集项集啤酒,尿布面包,尿布{面包,牛奶尿布,牛奶候选产生候选剪枝项集项集(面包,尿布,牛奶}面包,尿布,牛奶频繁2项集项集啤酒,尿布{面包,尿布{面包,牛奶{尿布,牛奶}图6-8通过合并一对频繁(k-1)项集生成和剪枝候选k-项集6.2.4支持度计数支持度计数过程确定在 apriori-g-enm数的候选项剪枝步骤保留下来的每个候选项集出现的频DG繁程度。支持度计数在算法6.1的第6步到第11步实现。支持度计数的一种方法是,将每个事
务与所有的候选项集进行比较(见图6-2),并且更新包含在事务中的候选项集的支持度计数。这种方法是计算昂贵的,尤其当事务和候选项集的数目都很大时。另一种方法是枚举每个事务所包含的项集,并且利用它们更新对应的候选项集的支持度。例如,考虑事务t,它包含5个项{1,2,3,5,6}该事务包含C3=10个大小为3的项集,其中的某些项集可能对应于所考察的候选3项集,在这种情况下,增加它们的支持度。那些不与任何候选项集对应的事务t的子集可以忽略。图6-9显示了枚举事务t中所有3项集的系统的方法。假定每个项集中的项都以递增的字典序排列,则项集可以这样枚举:先指定最小项,其后跟随较大的项。例如,给定t={1,2,3,5,6}它的所有3项集一定以项1、2或3开始。不必构造以5或6开始的3项集,因为事务t中只有两个项的标号大于等于5.图69中第一层的前缀结构描述了指定包含在事务t中的3项集的第一项的方法。例如,12356表示这样的3项集它以1开始,后随两个取自集合{2,3,5,6}的项。事务t12356第一层123562356356第二层1235613561562356256356123125135136156235236256356126第三层包含3个项的子集图6-9枚举事务t的所有包含3个项的子集确定了第一项之后,第二层的前缀结构表示选择第二项的方法。例如:12356表示以1,2}为前缀,后随项3、5或6的项集。最后,第三层的前缀结构显示了事务t包含的所有的3项集。例如,以{1,2}为前的3-项集是(1,2,3},{1,2,}和{1,2,6};而以2,3}为前缀的3-项集是{2,3,5}和{2,3,6}图6-9中所示的前缀结构演示了如何系统地枚举事务所包含的项集,即通过从最左项到最右项依次指定项集的项。然而还必须确定每一个枚举的3项集是否对应于一个候选项集,如果它与一个候选匹配,则相应候选项集的支持度计数增值。下面,将解释如何使用Hash树来有效地进行匹配操作。使用Hash树进行支持度计数在 Apriori算法中,候选项集划分为不同的桶,并存放在Hash树中在支持度计数期间,包含在事务中的项集也散列到相应的桶中。这种方法不是将事务中的每个项集与所有的候选项集进行比较,而是将它与同一桶内候选项集进行匹配,如图6-10所示。
hash树包含候选2项{啤酒,面包}集的叶结点啤酒,尿布(面包,尿布}尿布,牛奶}啤酒,牛奶}{面包,牛奶}不不事务 TID项1面包,牛奶2面包,尿布,啤酒,鸡蛋3牛奶,尿布,啤酒,可乐4面包,牛奶,尿布,啤酒5面包,牛奶,尿布,可乐图6-10使用Hash树结构的项集支持度计数图6-11是一棵Hash树结构的例子。树的每个内部结点都使用Hash函数hp)=pmod3来确定应当沿着当前结点的哪个分支向下。例如,项1,4和7应当散列到相同的分支(即最左分支),因为除以3之后它们都具有相同的余数。所有的候选项集都存放在Hash树的叶结点中。图6-11中显示的Hash树包含15个候选3项集,分布在9个叶结点中Hash函数36925.81+2356事务2+356123563+56候选Hash树234567145136345356367357368689124125159457458图6-11在Hash树的根结点散列一个事务考虑一个事务t=(1,,356为了更新候选项集的支持度计数,必须这样遍历Hash树:所有包含属于事务t的候选3项集的叶结点至少访问一次。注意,包含在t中的候选3项集必须
以项1,2或3开始,如图6-9中第一层前缀结构所示。这样,在Hash树的根结点,事务中的项1,2和3将分别散列。项1被散列到根结点的左子女,项2被散列到中间子女,而项3被散列到右子女。在树的下一层,事务根据图6-中的第二层结构列出的第二项进行散列。例如,在根结点散列项1之后,散列事务的项2、3和5。项2和5散列到中间子女,而3散列到右子女,如图6-12所示。继续该过程,直至到达Hah树的叶结点。存放在被访问的叶结点中的候选项集与事务进行比较,如果候选项集是该事务的子集,则增加它的支持度计数。在这个例子中,访问了9个叶结点中的5个,15个项集中的9个与事务进行比较。1+2356事务2+356123563+56候选Hash树12+356-13+56--23415+6-567145136345356367357368689124125159457458图6-12在候选项集Hash树根的最左子树上的子集操作6.2.5计算复杂度 Apriori算法的计算复杂度受如下因素影响。支持度阈值降低支持度阈值通常将导致更多的频繁项集。这给算法的计算复杂度带来不利影响,因为必须产生更多候选项集并对其计数,如图6-13所示。随着支持度阈值的降低,频繁项集的最大长度将增加。而随着频繁项集最大长度的增加,算法需要扫描数据集的次数也将增多。项数(维度)随着项数的增加,需要更多的空间来存储项的支持度计数。如果频繁项集的数目也随着数据维度增加而增长,则由于算法产生的候选项集更多,计算量和IO开销将增加。事务数由于 Apriori算法反复扫描数据集,因此它的运行时间随着事务数增加而增加。事务的平均宽度对于密集数据集,事务的平均宽度可能很大,这将在两个方面影响 Apriori算法的复杂度。首先,频繁项集的最大长度随事务平均宽度增加而增加,因而,在候选项产生和支持度计数时必须考察更多候选项集,如图6-14所示其次,随着事务宽度的增加,事务中将包含更多的项集,这将增加支持度计数时Hash树的遍历次数。
4×105一支持度=0.1%支持度0.1%3.5支持度=0.2%35支持度=0.2%支持度0.5%支持度=0.5%35←5212※←22肾报坚1.50.50.52010项集的大小项集的大小(a)候选项集的个数(b)频繁项集的个数图6-13支持度阈值对候选项集和频繁项集的数量的影响1010510105宽度=5宽度=5--+宽度=18宽度=15←098765宽度=10宽度=15102510早守15项集的大小项集的大小a)候选项集的个数(b)频繁项集的个数图6-14事务的平均宽度对候选项集和频繁项集的数量的影响下面,详细分析 Apriori算法的时间复杂度。频繁1项集的产生对于每个事务,需要更新事务中出现的每个项的支持度计数。假定为事务的平均宽度,则该操作需要的时间为O(Nw),其中N为事务的总数。候选的产生为了产生候选k项集,需要合并一对频繁(k-1)-项集,确定它们是否至少有k-2个项相同。每次合并操作最多需要k-2次相等比较。在最好情况下,每次合并都产生一个可行的候选k项集;在最坏的情况下,算法必须合并上次迭代发现的每对频繁(k-1)项集。因此,合并频繁项集的总开销为: PDG(k-2)1<合并开销<(k-2)F
Hash树在候选产生时构造,以存放候选项集。由于Hash树的最大深度为k,将候选项集散列到Hash树的开销为O(kCD。在候选项剪枝的过程中,需要检验每个候选k项集的k-2k=2个子集是否频繁。由于在Hash树上查找一个候选的花费是O(k),因此候选剪枝需要的时间是 o(k(k-2)C:.支持度计数每个长度为|t|的事务将产生个k项集。这也是每个事务遍历Hash树的有效次数。支持度计数的开销为O(NCa2),其中w是事务的最大宽度,是更新Hash树中一k个候选k项集的支持度计数的开销。6.3规则产生本节介绍如何有效地从给定的频繁项集中提取关联规则。忽略那些前件或后件为空的规则(→Y或Y→),每个频繁k-项集能够产生多达22个关联规则。关联规则可以这样提取:将项集Y划分成两个非空的子集X和Y-X,使得X→Y-X满足置信度阈值。注意:这样的规则必然已经满足支持度阈值,因为它们是由频繁项集产生的。例6.2设X={1,2,3}是频繁项集。可以由X产生6个候选关联规则:{1,2}→{3},{1,3→{2},{2,3}→{1},{1}→{2,3},2}→{1,3}和3}→{1,2}由于它们的支持度都等于X的支持度,这些规则一定满足支持度阈值。口计算关联规则的置信度并不需要再次扫描事务数据集。考虑规则{1,2}→{3},它是由频繁项集X={1,2,3}产生的。该规则的置信度为({1,2,3})(1,2})。因为{1,2,3}是频繁的,支持度的反单调性确保项集{1,2}一定也是频繁的由于这两个项集的支持度计数已经在频繁项集产生时得到,因此不必再扫描整个数据集。6.3.1基于置信度的剪枝不像支持度度量,置信度不具有任何单调性。例如:规则X→Y的置信度可能大于、小于或等于规则→的置信度,其中X且Yy(见本章习题3)尽管如此,当比较由频繁项集Y产生的规则时,下面的定理对置信度度量成立。定理6.2如果规则→y-不满足置信度阈值,则形如X→-X的规则一定也不满足置信度阈值,其中X是X的子集。为了证明该定理,考虑如下两个规则:x→y-和→y-x,其中XX。这两个规则的置信度分别为o(Y)/o()和o(Y)o(X)。由于X是X的子集,所以o(X)≥a(X)。因此,前一个规则的置信度不可能大于后一个规则。 Apriori算法使用一种逐层方法来产生关联规则,其中每层对应于规则后件中的项数。初始,DG6.3.2 Apriori算法中规则的产生提取规则后件只含一个项的所有高置信度规则,然后,使用这些规则来产生新的候选规则。例如,
如果{acd}→{b}和{abd}→{c}是两个高置信度的规则,则通过合并这两个规则的后件产生候选规则{ad}→{bc}.图6-15显示了由频繁项集{ab,c,d}产生关联规则的格结构。如果格中的任意结点具有低置信度,则根据定理6.2,可以立即剪掉该结点生成的整个子图。假设规则{bcd}→{a}具有低置信度,则可以丢弃后件包含a的所有规则,包括{cd}→{ab},{bd}→{ac},{bc}→{ad}和{d}→ {abc}.低置信度规则 Cabcd=} (bed=>a acd=>)(abd=>)abc=od cd=>ab (bd bd=>ac )bc=>>=>bd ab=dabccabdb>bcd剪过的规则图6-15使用置信度度量对关联规则进行剪枝算法6.2和算法6.3给出了关联规则产生的伪代码。注意,算法6.3中的ap- -genrules过程与算法6.1中的频繁项集产生的过程类似。二者唯一的不同是,在规则产生时,不必再次扫描数据集来计算候选规则的置信度,而是使用在频繁项集产生时计算的支持度计数来确定每个规则的置信度。算法6.2 Apriori算法中的规则产生1:for每一个频繁k项集f,k≥2do:H1={if}规则的1项后件23 call ap-genrules(, H1) 4: end for算法6.3过程ap- -genrules(Hm)1:k=|f频繁项集的大小2:m=规则后件的大小 3: if k>m+ then 4: Hm+= apriori-gen(Hm)5:for每个hm+1∈Hm+1do 6: conf= o ()/(f-hm+1)7: if confminconf≥ then8: output:规则(-hm+)→hm+19: else10:从Hm+1 delete hm+ 11: end if 12: end for 13: call ap-genrules(,. Hm+1) 14: end if
6.3.3例:美国国会投票记录本节演示对美国众议院议员投票记录应用关联分析的结果。这些数据来自1984年美国国会投票数据库,可以在UCI机器学习库中找到。每一个事务包含议员的党派信息,以及他/她对16个关键问题的投票记录。数据集中共有435个事务和34个项。表6-3列出了所有的项。表6-31984年美国国会投票记录的二元属性列表。信息源:UC机器学习库 1. Republican18. aid to Nicaragua=no 2.Democrat 19. MX-missile= yes 3.handicapped-infants=yes20.mx- -missile=no 4. handicapped-infants no 21. immigration=yes 5. water project cost sharing yes 22. immigration =no 6. water project cost sharing =no23. synfuel corporation cutback=yes 7.budget-resolution =yes24. synfuel corporation cutback=no 8. budget-resolution =no 25. education spending =yes 9. physician fee freeze =yes 26. education spending =no 10. physician fee freeze =no27. right-to-sue=yes 11. aid to EI Salvador =yes28. right-to-sue=no12. aid to El Salvador=no 29.crime=yes13. religious groups in schools=yes 30.crime=no14. religious groups in schools=no 31.duty-free-exports=yes15.anti- -satellite test ban=yes 32. duty-free-exports =no 16. anti-satellite test ban =no 33. export administration act=yes I7. aid to Nicaragua yes 34. export administration act=no设定 minsup=30%和 minconf=90%,对数据集使用 Apriori算法表6-4列举了算法产生的一些高置信度的规则。前两个规则暗示大部分同时投“援助萨尔瓦多”(aid to EI Salvador)赞成票、投“预算决议案”( budget resolution)和“M导弹决议案”(MX missile)反对票的是共和党人而同时投“援助萨尔瓦多”反对票、投“预算决议案”和“MX导弹决议案”赞成票的是民主党人。这些高置信度的规则示出关键的问题可以将国会成员分为两个政党。如果降低最小置信度,将会发现很难找到区分政党的特定问题。例如当最小置信度为40%时,这些规则暗示对于一个问题两个政党的投票差不多投反对票的成员中52.3%是共和党人,47.7%的是民主党人。表6-4从1984年美国国会投票记录中提取的关联规则关联规则置信度 {budget resolution no, MX-missile no, aid to El Salvador =yes)(Republican)91.0% budget resolution yes, MX-missile =yes, aid to El Salvador no){Democrat)97.5% (crime= yes, right-to-sue yes, physician fee freeze yes}- Republican}93.5% (crime no, right-to-sue no, physician fee freeze =no[ Democrat100%6.4频繁项集的紧凑表示实践中,由事务数据集产生的频繁项集的数量可能非常大。因此,从中识别出可以推导出其他所有的频繁项集的、较小的、具有代表性的项集是有用的。本节将介绍两种具有代表性的项集:极大频繁项集和闭频繁项集。6.4.1极大频繁项集 PDG定义6.3极大频繁项集(maximal frequent itemset)极大频繁项集是这样的频繁项集,
它的直接超集都不是频繁的。为了解释这一概念,考虑图6-16所示的项集格。格中的项集分为两组:频繁项集和非频繁项集。图中虚线表示频繁项集的边界。位于边界上方的每个项集都是频繁的,而位于边界下方的项集(阴影结点)都是非频繁的。在边界附近的结点中,{a,d},{a,c,e}和{b,c,d,e}都是极大频繁项集,因为它们的直接超集都是非频繁的。例如,项集{a,d}是极大频繁的,因为它的所有直接超集{a,b,d},{a,c,d}和{a,d,e}都是非频繁的;相反,项集{a,c}不是极大的,因为它的一个直接超集{a,c,e}是频繁的。 (null极大频繁项集b ab)ac )()(ae )bc )bd)()cd (ce)de (abc)abd( (abcdabce abdeacde bede频繁的 abcde频繁项集边界非频繁的图6-16极大频繁项集极大频繁项集有效地提供了频繁项集的紧凑表示。换句话说,极大频繁项集形成了可以导出所有频繁项集的最小的项集的集合。例如,图6-16中的频繁项集可以分为以下两组。以项a开始、可能包含项c,d和e的频繁项集。这一组包含的项集有{a},{a,c},{a,d},{a,e和{a,c,e}·以项b,c,d或e开始的频繁项集。这一组包含的项集有{b},{b,c},{c,d,{b,c,d,e等。属于第一组的频繁项集是{a,c,e}或{a,d}的子集,而属于第二组的频繁项集都是{b,c,d,e}的子集因此,极大频繁项集{a,c,e},{a,d}和{b,c,d,}提供了图6-16中显示的频繁项集的紧凑表示。对于可能产生长频繁项集的数据集,极大频繁项集提供了颇有价值的表示,因为这种数据集中的频繁项集可能有指数多个。尽管如此,仅当存在一种有效的算法,可以直截了当地发现极大频繁项集而不需要枚举它们的所有子集时,这种方法才是实用的。将在6.5节中简略介绍一种这样的方法。尽管提供了一种紧凑表示,但是极大频繁项集却不包含它们子集的支持度信息。例如,极大频繁项集{a,c,e},{a,d}和{b,c,d,e}不能够提供它们子集的支持度的任何信息。因此,这就需要再一遍扫描数据集,来确定那些非极大的频繁项集的支持度计数。在某些情况下,可能需要得到保持支持度信息的频繁项集的最小表示。下一节将介绍一种这样的表示。
6.4.2闭频繁项集闭项集提供了频繁项集的一种最小表示,该表示不丢失支持度信息下面给出闭项集的形式定义。定义6.4闭项集(closed itemset)项集X是闭的如果它的直接超集都不具有和它相同的支持度计数。换句话说,如果至少存在一个X的直接超集,其支持度计数与X相同,X就不是闭的。闭项集的例子显示在图6-17中。为了更好地解释每个项集的支持度计数,格中每个结点(项集)都标出了与它相关联的事务的ID。例如,由于结点{b,c)与事务ID1,2和3相关联,因此它的支持度计数为3。从图中给定的事务可以看出包含b的每个事务也包含c,因此,由于{b}和{b,c}的支持度是相同的,所以{b}不是闭项集。同样,由于c出现在所有包含a和d的事务中,所以项集{a,d}不是闭的。另一方面,{b,c}是闭项集,因为它的支持度计数与它的任何超集都不同。TID项 1abc minsup =40% 2abcd null3bce1,24 4acde12.31,2.3424.534.5 5de121242422323.4de ab)(ac adaebo (bd (be cd4.512224abcabdabe(acd2 (ace)(ade (bed)bce)(bde)cde2 abcdabce abde abde)(acde(bede闭频繁项集 Cabede图6-17闭频繁项集的例子(最小支持度为40%)定义6.5闭频繁项集(closed frequent itemset)一个项集是闭频繁项集,如果它是闭的,并且它的支持度大于或等于最小支持度阈值。在前面的例子中,假定支持度阈值为40%,则项集{b,c}是闭频繁项集,因为它的支持度是60%。图6-17中其他闭频繁项集用带阴影结点指示出。有一些算法可以直接从给定的数据集中提取闭频繁项集。对于这些算法的进一步讨论,感兴趣的读者可以查阅本章的文献注释。可以使用闭频繁项集的支持度来确定那些非闭的频繁项集定与它的某个直接超集相同,关键是确定哪个超集({a,b,)a,)ad)恰好与lad/PDG的支持度。例如,考虑图6-17所示的频繁项集{a,d}。因为该项集不是闭的,所以它的支持度一具有相同的支持度计数。 Apriori原理表明任何包含{a,d超集的事务一定包含{a,d},然而,包
含{a,d}的事务不一定要包含{a,d}的超集。由于这个原因,{a,d}的支持度一定等于它的超集的最大支持度。由于{a,c,d}的支持度大于{a,,d}和{a,d,e}的支持度,因此{a,d}的支持度一定等于{a,c,d}的支持度。使用这种方法,可以开发一个算法,用以计算非闭频繁项集的支持度。算法6.4显示了这个算法的伪代码。该算法以从特殊到一般的方式进行处理,即从最大的频繁项集到最小的频繁项集。这是因为,为了找出非闭频繁项集的支持度,必须要知道它的所有超集的支持度。算法6.4使用闭频繁项集进行支持度计数1:设C是闭频繁项集的集合2:设kmax是闭频繁项集的最大长度3:F=ff∈C,f=kax}找出长度为kmax的所有频繁项集} 4: for= kmax-1 downto 1 do5:Fk={fc Fk+1,f=k}找出长度为k的所有频繁项集}6:for每个f∈Fkdo 7: iffe Cthen 8: f. support max{ supportf'E,fcf? 9: end if 10: end for 11: end for为了举例说明使用闭频繁项集的优点,考虑表6-5中的数据集,它包含10个事务和15个项。数据集中的这些项大致可以分为3组:()组A,包含项a1到a5;(2)组B,包含项b1到b5;(3)组C,包含项c1到c5。注意,每一组的项与同组的项有极好的关联,并且不与其他组中的项同时出现。假定支持度阈值为20%,频繁项集的总数为3×(25-1)=93。然而,该数据只有3个闭频繁项集:{a1,a2,a3,a4,as},{b1,b2,b3,b4,b}和{c1,c2,3,c4,c5}。对于分析,仅提供这些闭频繁项集,而不是所有的频繁项集就足够了。表6-5挖掘闭项集的事务数据集序23号 as as as b b2 b3 bs Ct C2 C3 C4csa1110000011111b00011111C0000G000000000000000000000000000001100000000000000000000000011100000111000100011101001110000011100010000000000111000011190000000111110000000000011111对于删除冗余的关联规则,闭频繁项集是非常有用的。关联规则→是余的,如果存在另一个关联规则X→使得两个规则的支持度和置信度都相同,其中,X是X的子集,并且Y是Y的子集。在图6-17显示的例子中,{b}不是闭频繁项集,而{b,c}是闭的。由于关联规则{b}→{d,e}与{b,c}→{d,e}具有相同的支持度和置信度,所以规则{b}→{d,e}是冗余的。如果使用闭频繁项集产生规则,则不会产生这样的冗余规则。最后,极大频繁项集都是闭的,因为任何极大频繁项集都不可能与它的直接超集具有相同的支持度计数。频繁项集、极大频繁项集和闭频繁项集之间的关系显示在图6-18中
频繁项集闭频繁项集极大频紫项集图6-18频繁项集、极大频繁项集和闭频繁项集之间的关系6.5产生频繁项集的其他方法 Apriori算法是在早期成功地处理频繁项集产生的组合爆炸问题的算法之一它通过使用先验原理对指数搜索空间进行剪枝,成功地处理了组合爆炸问题。尽管显著地提高了性能,但是该算法还是会导致不可低估的IO开销,因为它需要多次扫描事务数据集。此外,正如6.2.5节所提到的,对于稠密数据集,由于事务数据宽度的增加, Apriori算法的性能显著降低。为了克服这些局限性和提高 Apriori算法的效率,已经开发了一些替代方法。下面是这些方法的简略描述。项集格遍历概念上,可以把频繁项集的搜索看作遍历图6-1中的项集格。算法使用的搜索策略指明了频繁项集产生过程中如何遍历格结构。根据频繁项集在格中的布局,某些搜索策略优于其他策略。下面概述这些策略。一般到特殊与特殊到一般:Apriori算法使用了“一般到特殊”的搜索策略,合并两个频繁(k-1)-项集得到候选k项集。只要频繁项集的最大长度不是太长,这种“一般到特殊的搜索策略是有效的。使用这种策略效果最好的频繁项集的布局显示在图6-19a中,其中较黑的结点代表非频繁项集。相反,“特殊到一般”的搜索策略在发现更一般的频繁项集之前,先寻找更特殊的频繁项集。这种策略对于发现稠密事务中的极大频繁项集是有用的。稠密事务中频繁项集的边界靠近格的底部,如图6-19b所示。可以使用先验原理剪掉极大频繁项集的所有子集。具体说来,如果候选k项集是极大频繁项集,则不必考察它的任意k-1项子集。然而,如果候选k项集是非频繁的,则必须在下一迭代考察它所有k-1项子集。另外一种策略是结合“一般到特殊和“特殊到一般”的搜索策略,尽管这种双向搜索方法需要更多的空间存储候选项集,但是给定图6-19c所示的布局,该方法有助于加快确定频繁项集边界。频繁项 null集边界频繁项集边界 9ooo 000d频繁项an2a(a,a2a集边界ana(a)一般到特殊(b)特殊到一般(c)双向图6-19一般到特殊、特殊到一般和双向搜索
·等价类:另外一种遍历的方法是先将格划分为两个不相交的结点组(或等价类)。频繁项集产生算法依次在每个等价类内搜索频繁项集。例如, Apriori算法采用的逐层策略可以看作根据项集的大小划分格,即在处理较大项集之前,首先找出所有的频繁1项集。等价类也可以根据项集的前缀或后缀来定义在这种情况下,两个项集属于同一个等价类,如果它们共享长度为k的相同前缀或后缀。在基于前缀的方法中,算法首先搜索以前缀a开始的频繁项集,然后是以前缀b等开始的频繁项集,然后是c,如此下去。基于前缀和基于后缀的等价类都可以使用图6-20所示的类似于树的结构来演示。 (null (null a). (b) (c. (d(ab)(ac) (ad!(bc)(bd)(cd (ab)) (bc)(ad)( (abc)(abd)() abc(abd(acd) (abcd abcd(a)前缀树(b)后缀树图6-20基于项集前缀和后缀的等价类·宽度优先与深度优先: Apriori算法采用宽度优先的方法遍历格,如图6-21a所示。它首先发现所有频繁1-项集,接下来是频繁2项集,如此下去直到没有新的频繁项集产生为止。也可以以深度优先的方式遍历项集格,如图6-21b和图6-22所示。比如说,算法可以从图6-22中的结点a开始,计算其支持度计数并判断它是否频繁。如果是,算法渐增地扩展下层结点,即ab,abc,等等,直到到达一个非频繁结点,如abcd。然后,回溯到下一个分支,比如说abce,并且继续搜索。(a)宽度优先(b)深度优先图6-21宽度优先和深度优先遍历通常,深度优先搜索方法用于发现极大频繁项集的算法。这种方法比宽度优先方法更快地检测到频繁项集边界。一旦发现一个极大频繁项集,就可以在它的子集上进行剪枝。例如,如果图6-22中的结点bcde是极大频繁项集,则算法就不必访问以bd,be,c,d和e为根的子树,因为它们不可能包含任何极大频繁项集。然而,如果abc是极大频繁项集,则只有ac和bc这样的结点
不是极大频繁项集,但以它们为根的子树还可能包含极大频繁项集。深度优先方法还允许使用不同的基于项集支持度的剪枝方法。例如,假定项集{a,b,c}和{a,b}具有相同的支持度,则可以跳过以abd和abe为根的子树,因为可以确保它们不包含任何极大频繁项集。该问题的证明作为习题留给读者。 ab ac ad bded ae be ce de abcabd acdo bedoo \abe ace ade bce bde cde abedo abce abde acde bcde abcde图6-22使用深度优先方法产生候选项集事务数据集的表示事务数据集的表示方法有多种表示方法的选择可能影响计算候选项集支持度的IO开销。图6-23显示了两种表示购物篮事务的不同方法。左边的表示法称作水平(horizontal)数据布局,许多关联规则挖掘算法(包括 Apriori)都采用这种表示法;另一种可能的方法是存储与每一个项相关联的事务标识符列表(TID列表),这种表示法称作垂直(vertical)数据布局。候选项集的支持度通过取其子项集TD列表的交得到。随着不断进展,处理较大的项集,TID列表的长度不断收缩。然而,这种方法存在一个问题:TID列表的初始集合可能太大,以致无法放进内存,因此就需要特殊的巧妙技术来压缩TID列表。下一节,将介绍另外一种表示数据的有效方法。水平数据布局垂直数据布局TD项 abcd e 1 a,b,e11221 2 b,c,d42343 3c,e55456 4 a,c,d6789 5 a,b,c,d 6 a,e 7 a,b78989810 a,b,c 9 a,c,d10b图6-23水平和垂直数据形式66FP增长算法 PDG本节介绍另一种称作FP增长的算法。该算法采用完全不同的方法来发现频繁项集。该算法
不同于 Apriori算法的“产生一测试”范型,而是使用一种称作FP树的紧凑数据结构组织数据,并直接从该结构中提取频繁项集。下面详细说明该方法。6.6.1FP树表示法FP树是一种输入数据的压缩表示,它通过逐个读入事务,并把每个事务映射到FP树中的一条路径来构造。由于不同的事务可能会有若干个相同的项,因此它们的路径可能部分重叠。路径相互重叠越多,使用FP树结构获得的压缩的效果越好。如果FP树足够小,能够存放在内存中,就可以直接从这个内存中的结构提取频繁项集,而不必重复地扫描存放在硬盘上的数据。图6-24显示了一个数据集,它包含10个事务和5个项。图中还绘制了读入前3个事务之后FP树的结构。树中每一个结点都包括一个项的标记和一个计数,计数显示映射到给定路径的事务个数。初始,FP树仅包含一个根结点,用符号ull标记随后,用如下方法扩充FP树:(1)扫描一次数据集,确定每个项的支持度计数。丢弃非频繁项,而将频繁项按照支持度的递减排序。对于图6-24中的数据集,a是最频繁的项,接下来依次是b,c,d和e(2)算法第二次扫描数据集,构建FP树。读入第一个事务{a,b}之后,创建标记为a和b的结点。然后形成null→a→b路径,对该事务编码。该路径上的所有结点的频度计数为1(3)读入第二个事务b,c,d}之后,为项b,和d创建新的结点集。然后,连接结点null→b→c→d,形成一条代表该事务的路径。该路径上的每个结点的频度计数也等于1。尽管前两个事务具有一个共同项b,但是它们的路径不相交,因为这两个事务没有共同的前缀。(4)第三个事务{a,c,d,e}与第一个事务共享一个共同前缀项a,所以第三个事务的路径null→a→c→d→e与第一个事务的路径 null-a→b部分重叠。因为它们的路径重叠,所以结点a的频度计数增加为2,而新创建的结点c,d和e的频度计数等于1(5)继续该过程,直到每个事务都映射到FP树的一条路径。读入所有的事务后形成的FP树显示在图6-24的底部。通常,FP树的大小比未压缩的数据小,因为购物篮数据的事务常常共享一些共同项。在最好情况下,所有的事务都具有相同的项集FP树只包含一条结点路径。当每个事务都具有唯一项集时,导致最坏情况发生,由于事务不包含任何共同项,FP树的大小实际上与原数据的大小一样,然而,由于需要附加的空间为每个项存放结点间的指针和计数,FP树的存储需求增大。FP树的大小也取决于项的排序方式。如果颠倒前面例子的序,即项按照支持度由小到大排列,则结果FP树显示在图6-25中。该树显得更加茂盛,因为根结点上的分支数由2增加到5并且包含了高支持度项a和b的结点数由3增加到12。尽管如此,支持度计数递减序并非总是导致最小的树。例如,假设加大图6-24给定的数据集,增加100个事务包含{e}、80个事务包含{d}、60个事务包含{c}、40个事务包含{b}现在,项e是最频繁的,接下来依次是d,c,b和a使用加大的事务数据,支持度计数递减序将导致类似于图6-25中的FP树,而基于支持度计数递增序将产生一棵类似于图6-24(iv)的较小的FP树。FP树还包含一个连接具有相同项的结点的指针列表。这些指针在图6-24和图6-25中用虚线表示,有助于方便快速地访问树中的项。下一节,解释如何使P树和它的相应指针产生频繁项集。
 null事务数据集 nullTD项a:1b:1a:11{a,bb:1 2b,c,dc:1 3{a,c,d,e)b:1d:1 4{a,d,e){a,b,)(i)读入TD=1之后(i)读入TID=2之后 6{a,b,c,d) nullO 7 {a) 8{a,b,c)a:2b:1 9{a,b,d)b:1 10{b,c,e)c:1d:1-----d:1e:1()读入TID=3之后 null a:8c: 2b:5c:2c:3d:1d:1Ld:1e:1e:1e:1(iv)读入TID=10之后图6-24构造FP树 nulle:3c:2b:1d:2c:2---a:1c:1c:1b-a:1------a:1a:2a:1a:1图6-25对图6-24所示数据集使用不同的项序方案的FP树表示6.6.2FP增长算法的频繁项集产生FP增长(f-growth-)是一种以自底向上方式探索树,由FP树产生频繁项集的算法。给定图6-24所示的树,算法首先查找以e结尾的频繁项集,接下来依次是d,c,b,最后是a。这种用于发现以某一个特定项结尾的频繁项集的自底向上策略等价于6.5节介绍的基于后缀的方法。由于每一个事务都映射到FP树中的一条路径,因而通过仅考察包含特定结点(例如e)的路径,就可以发现以e结尾的频繁项集。使用与结点e相关联的指针,可以快速访问这些路径。图6-26a显示了所提取的路径。稍后详细解释如何处理这些路径,以得到频繁项集。
 nullnula:8a:80○b:2b:50+ob:2c:1c:2d:1c:3c:2d:1---:1--e:1e:1e:1d:1d:1(a)包含结点e的路径(b)包含结点d的路径 null null null a:8Cb:2a8b:2a:8b:5c2c:1b:5c:3(c)包含结点c的路径(d)包含结点b的路径(e)包含结点a的路径图6-26将频繁项集产生的问题分解成多个子问题,其中每个子问题分别涉及发现以e,d,c,b和a结尾的频繁项集发现以e结尾的频繁项集之后,算法通过处理与结点d相关联的路径,进一步寻找以d结尾的频繁项集。图6-26b显示了对应的路径。继续该过程,直到处理了所有与结点c,b和a相关联的路径为止。图6-26c、图6-26d、图6-26e分别显示了这些项的路径,而它们对应的频繁项集汇总在表6-6中。表6-6依据相应的后缀排序的频繁项集后缀频繁项集 {e),{d,), {a,d,), (,),(a,)edcba (d),{c,d), {b,c,d), (a,c,d ) (,),(a,,) {a,) {, {b,c), {a,b,],{a,}{b),{ab{a}FP增长采用分治策略将一个问题分解为较小的子问题,从而发现以某个特定后缀结尾的所有频繁项集。例如,假设对发现所有以e结尾的频繁项集感兴趣。为了实现这个目的,必须首先检查项集{e}本身是否频繁。如果它是频繁的则考虑发现以de结尾的频繁项集子问题,接下来是ce和ae。依次,每一个子问题可以进一步划分为更小的子问题。通过合并这些子问题得到的结果,就可以找到所有以e结尾的频繁项集。这种分治策略是FP增长算法采用的关键策略。为了更具体地说明如何解决这些子问题,考虑发现所有以e结尾的频繁项集的任务。(1)第一步收集包含e结点的所有路径。这些初始的路径称为前缀路径(prefix path),如图6-27a所示。(2)由图6-27a中所显示的前缀路径,通过把与结点e相关联的支持度计数相加得到e的支持度计数。假定最小支持度为2,因为{e的支持度是3所以它是频繁项集。(3)由于{e}是频繁的,因此算法必须解决发现以e、ce、be和ae结尾的频繁项集的子问题。以某特定后缀结尾的频繁项集之外,条件FP树的结构与FP树类似。条件FP树通过以下步骤得到。PDG在解决这些子问题之前,必须先将前缀路径转化为条件FP树(conditional FP--tree)。除了用于发现(a)首先,必须更新前缀路径上的支持度计数,因为某些计数包括那些不含项e的事务。例
如,图6-27a中的最右边路径null→b:2→c:→e:1,包括并不含项e的事务{b,c}。因此,必须将该前缀路径上的计数调整为1,以反映包含b,c,e}的事务的实际个数(b)删除e的结点,修剪前缀路径。删除这些结点是因为,沿这些前缀路径的支持度计数已经更新,以反映包含e的那些事务,并且发现以de,ce,be和ae结尾的频繁项集的子问题不再需要结点e的信息。(c)更新沿前缀路径上的支持度计数之后,某些项可能不再是频繁的。例如,结点b只出现了1次,它的支持度计数等于1,这就意味着只有一个事务同时包含b和e。因为所有以be结尾的项集一定都是非频繁的,所以在其后的分析中可以安全地忽略be的条件FP树显示在图6-27b中。该树看上去与原来的前缀路径不同,因为频度计数已经更新,并且结点b和e已被删除。(4)FP增长使用e的条件FP树来解决发现以e,ce,be和ae结尾的频繁项集的子问题。为了发现以de结尾的频繁项集,从项e的条件FP树收集d的所有前缀路径(图6-27c)。通过将与结点d相关联的频度计数求和,得到项集{d,e}的支持度计数。因为项集{d,e}的支持度计数等于2,所以它是频繁项集。接下来,算法采用第3步介绍的方法构建de的条件FP树更新了支持度计数并删除了非频繁项c之后,de的条件FP树显示在图6-27d中。因为该条件FP树只包含一个支持度等于最小支持度的项a,算法提取出频繁项集{a,d,e}并转到下一个子问题,产生以ce结尾的频繁项集。处理c的前缀路径后,只发现项集{c,e是频繁的。接下来,算法继续解决下一个子问题并发现项集{a,e}是剩下唯一的频繁项集。 null Qnulla:8 Qb:2a:2c:1c:2d:1c:1-----c:1d:1e:----------e:1e:1d:1d:1(a)以e结尾的前缀路径(b)e的条件FP树 Onull nulla:2c:1d:1d1a:2(c)以de结尾的前缀路径(d)de的条件FP树 null null Oa:2c:1------+c:1a:2()以ce结尾的前缀路径(ae的条件FP树图6-27使用FP增长算法发现以e结尾的频繁项集的例子这个例子解释了FP增长算法中使用的分治方法每一次递归,都要通过更新前缀路径中的支持度计数和删除非频繁的项来构建条件FP树由于子问题是不相交的,因此FP增长不会产生任何重复的项集。此外,与结点相关联的支持度计数允许算法在产生相同的后缀项时进行支持度
计数。FP增长是一个有趣的算法,它展示了如何使用事务数据集的压缩表示来有效地产生频繁项集。此外,对于某些事务数据集,FP增长算法 Aprion比标准的算法要快几个数量级P增长算法的运行性能依赖于数据集的压缩因子(compaction factor)。如果生成的条件FP树非常茂盛(在最坏情形下,是一棵满前缀树),则算法的性能显著下降,因为算法必须产生大量的子问题,并且需要合并每个子问题返回的结果。6.7关联模式的评估关联分析算法具有产生大量模式的潜在能力例如,虽然表6-1中所显示的数据集只有6个项,但是在特定的支持度和置信度阈值下,它能够产生数以百计的关联规则。由于真正的商业数据库的数据量和维数都非常大,很容易产生数以千计、甚至是数以百万计的模式,而其中很大一部分可能是不感兴趣的。筛选这些模式,以识别最有趣的模式并非一项平凡的任务,因为“一个人的垃圾可能是另一个人的财富”。因此,建立一组广泛接受的评价关联模式质量的标准是非常重要的。第一组标准可以通过统计论据建立。涉及相互独立的项或覆盖少量事务的模式被认为是不令人感兴趣的,因为它们可能反映数据中的伪联系。这些模式可以使用客观兴趣度度量(objective interestingness measure)来排除,客观兴趣度度量使用从数据推导出的统计量来确定模式是否是有趣的。客观兴趣度度量的例子包括支持度、置信度和相关性。第二组标准可以通过主观论据建立,即模式被主观地认为是无趣的,除非它能够揭示料想不到的信息或提供导致有益的行动的有用信息例如,规则{黄油}→{面包}可能不是有趣的,尽管有很高的支持度和置信度,但是它表示的关系显而易见。另一方面,规则{尿布}→{啤酒}是有趣的,因为这种联系十分出乎意料,并且可能为零售商提供新的交叉销售机会。将主观知识加入到模式的评价中是一项困难的任务,因为需要来自领域专家的大量先验信息。下面是一些将主观信息加入到模式发现任务中的方法。·可视化( visualization)这种方法需要友好的环境,保持用户参与,允许领域专家解释和检验被发现的模式,与数据挖掘系统交互。·基于模板的方法(template-based- approach)这种方法允许用户限制挖掘算法提取的模式类型。只把满足用户指定的模板的规则提供给用户,而不是报告提取所有模式。主观兴趣度度量( subjective interestingness measure)主观度量可以基于领域信息来定义,如概念分层(将在7.3节讨论)或商品利润等。然后,使用这些度量来过滤那些显而易见和没有实际价值的模式。对主观兴趣度度量感兴趣的读者可以参阅本章文献注释中列举的文献。6.7.1兴趣度的客观度量客观度量是一种评估关联模式质量的数据驱动的方法。它不依赖于领域,只需要最小限度用户的输入信息,它不需要通过设置阈值来过滤低质量的模式。客观度量常常基于相依表contingency table列的频度计数来计算,表67显示了一对二元变量A和B的相依表使用记号A(B)表示A(B)不在事务中出现。在这个2x2的表中,每个f都代表一个频度计数。例如,fi1表示A和B同时出现在一个事务中的次数,fo表示包含B但不包含A的事务的个数。
行和fi表示A的支持度计数,而列和f1表示B的支持度计数。最后,尽管讨论主要关注非对称的二元变量,相依表也可以应用于其他属性类型,如对称的二元变量、标称变量和序数变量。表6-7变量A和B的2路相依表 BB Afir fio fi A for foo fo fro N支持度-置信度框架的局限性现有的关联规则的挖掘算法需要使用支持度和置信度来除去没有意义的模式。支持度的缺点在于许多潜在的有意义的模式由于包含支持度小的项而被删去,这一点将在随后的6.8节介绍。置信度的缺点更加微妙,用下面的例子最适于说明。例6.3假定希望分析爱喝咖啡和爱喝茶的人之间的关系收集一组人关于饮料偏爱的信息,并汇总在表6-8中表6-81000个人的饮料偏爱咖啡咖啡茶茶150502006501508008002001000可以使用表中给出的信息来评估关联规则{茶→{咖啡}猛一看,似乎喜欢喝茶的人也喜欢喝咖啡,因为该规则的支持度(15%)和置信度(75%)都相当的高。这个推论也许是可以接受的,但是所有的人中,不管他是否喝茶,喝咖啡的人的比例为80%,而喝咖啡的饮茶者却只占75%。也就是说,一个人如果喝茶,则他喝咖啡的可能性由80%减到了75%。因此,尽管规则茶}→{咖啡}有很高的置信度,但是它却是一个误导。口置信度的缺陷在于该度量忽略了规则后件中项集的支持度。的确,如果考虑喝咖啡者的支持度,则将毫不奇怪地发现许多喝茶的人也喝咖啡。更奇怪的是喝咖啡的饮茶者所占的比例实际少于所有喝咖啡的人所占的比例,这表明饮茶者和喝咖啡的人之间存在着一种逆关系。由于支持度-置信度框架的局限性,各种客观度量已经用来评估关联模式。下面,简略介绍这些度量并解释它们的优点和局限性。兴趣因子茶与咖啡的例子表明,由于置信度度量忽略了规则后件中出现的项集的支持度,高置信度的规则有时可能出现误导。解决这个问题的一种方法是使用称作提升度(lift)的度量: Lift() =(A -B)(6-4) s(B)它计算规则置信度和规则后件中项集的支持度之间的比率。对于二元变量,提升度等价于另一种称作兴趣因子(interest factor)的客观度量,其定义如下: s(A, B) I(A, B) =s(A)(B) fif(6-5)1
兴趣因子比较模式的频率与统计独立假定下计算的基线频率。对于相互独立的两个变量,基线频率为:血x,或等价地f=f(6-6) NNN该式从使用简单比例作为概率估计的标准方法得到分数fiN是联合概率P(A,B)的估计,而fin和fN分别是概率P(A)和P(B)的估计。如果A和B是相互独立的,则P(A,B)=P(A)×P(B),从而产生公式(6-6)。使用公式(6-5)和(6-6),该度量可以解释如下:=1如果A和B是独立的I(A,B)>1如果A和B是正相关的(6-7)1如果A和B是负相关的对于表6-8中所显示的例子,1=15=0.9375,这表明在饮茶者和喝咖啡的人之间稍微负相关。兴趣因子的局限性这里以一个文本挖掘领域的例子解释兴趣因子的局限性在文本挖掘领域,假定一对词之间的关联依赖于同时包含这两个词的文档的数量是合理的。例如,由于二者之间的较强关联,预计在计算机文献中词“数据”和“挖掘”同时出现的频率高于“编译”和“挖掘”同时出现的频率。表6-9显示了两对词{p,q}和{r,s}出现的频率使用公式(6-5),{p,q}和{r,s}的兴趣因子分别为1.02和4.08。由于下面的原因,这些结果多少有点问题:虽然p和q同时出现在88%的文档中,但是它们的兴趣因子接近于1,表明二者是相互独立的;另一方面,{r,s}的兴趣因子比{p,q}的高,尽管r和s很少同时出现在同一个文档中。在这种情况下,置信度可能是一个更好的选择,因为置信度表明p和q之间的关联(94.6%)远远强于r和s之间的关联(28.6%)表6-9词对p和s}的相依表pq8805093020507050207050880930930701000709301000相关分析相关分析是分析一对变量之间关系的基于统计学的技术。对于连续变量,相关度用皮尔森相关系数定义(参看2.4.5节公式(2-1))。对于二元变量,相关度可以用数度量。数定义如下:中= fufo- fofio(6-8)√ fuf fo相关度的值从-1(完全负相关)到+1(完全正相关。如果变量是统计独立的,则=0例如,在表6-8中给出的饮茶者和喝咖啡者之间的相关度为0.0625 PDG相关分析的局限性相关性的缺点通过表69所给出词的关联可以看出。虽然词p和q同时
出现的次数比r和s更多,但是它们的数是相同的,即中(p,q)=p(r,s)=0.232这是因为,中系数把项在事务中同时出现和同时不出现视为同等重要。因此,它更适合分析对称的二元变量。这种度量的另一个局限性是,当样本大小成比例变化时,它不能够保持不变。该问题将在稍后介绍客观度量的性质时更详细地讨论。IS度量IS是另一种度量,用于处理非对称二元变量。该度量定义如下: IS(A,)=(A,)xs(A,)(A, B)(6-9)√s(a)s()注意,当模式的兴趣因子和模式支持度都很大时,IS也很大。例如,表6-9中显示的词对{p,q和{r,s}的IS值分别是0.946和0.286。与兴趣因子和数给出的结果相反,IS度量暗示{p,q}之间的关联强于{rs},这与期望的文档中词的关联一致。可以证明IS数学上等价于二元变量的余弦度量(参见2.4.5节公式(2-7))在这一点上,将A和B看作一对位向量,A·B=s(A,B)表示两个向量的点积,A=s(A)表示向量A的大小因此, IS(A,B)= s(A,B)As(a)xs(b) Ax cosine(a,B)(6-10)IS度量也可以表示为从一对二元变量中提取出的关联规则的置信度的几何均值: IS(A,B) =s(A, B)(A, B)Vs()s(B)=c(a→b)xc(B→A(6-11)由于两个数的几何均值总是接近于较小的数,所以只要规则p→q或q→p中的一个具有较低的置信度,项集{p,q}的IS值就较低。S度量的局限性一对相互独立的项集A和B的IS值是: ISindep(A, B)=(A, B)=s(A)s(B)s(a)s(b)s(a)xs(B)==√s(a)xs(B)因为IS值取决于s(A)和s(B),所以IS存在与置信度度量类似的问题即使是不相关或负相关的模式,度量值也可能相当大。例如,尽管表6-10中所显示的项p和q之间的IS值相当大(0.889),当项统计独立时它仍小于期望值(ISindep=0.9)表6-10项p和q的相依表的例子p800100900100010090010010001.其他客观兴趣度度量除了迄今为止介绍的度量外,仍有另外一些分析二元变量之间联系的度量方法。这些度量可
以分为两类:对称的和非对称的度量。度量M是对称的,如果M(A→B)=M(B→A)例如,兴趣因子是对称的度量,因为规则A→B和B→的兴趣因子的值相等;相反,置信度是非对称度量,因为规则A→B和B→A的置信度可能不相等。对称度量常常用来评价项集,而非对称度量方法更适合于分析关联规则。表6-11和表6-12用2×2相依表的频度计数,给出了这些度量的部分定义。表6-11项集{AB的对称的客观度量度量(符号)定义相关性( Nfu-fuf.ffffo几率(a) (fivfo)/ (fiofor) K(k)+ Nfoo-fuf fofo兴趣因子(I (Nfu)/ (fif+1)余弦(IS)f) Piatetsky-Shapiro(PS)集体强度(S)+ foo N-ff -fo f o fif++fo-f-0 N-f-foo Jaccard () fuv/ (fi++firfn)全置信度(h表6-12规则A→B的非对称的客观度量度量(符号)定义 Goodman-Kruskal (2) ( max f-max f. (N-max,)互信息(M)J度量(J) loglogNGini指标(G拉普拉斯(L)f+)f+2信任度(V))/()可信度因子(E)( Added ValueAv) PDG2.客观度量的一致性给定各种各样的可用度量后,产生的一个合理问题是:当这些度量应用到一组关联模式时是
否会产生类似的有序结果。如果这些度量是一致的,那么就可以选择它们中的任意一个作为评估度量。否则的话,为了确定哪个度量更适合分析某个特定类型的模式,了解这些度量之间的不同点是非常重要的。假设使用对称和非对称度量确定表6-13中的10个相依表的秩。这些相依表用来解释已有度量之间的差异。这些度量产生的序分别显示在表6-14和表6-15中(1是最有趣的,10是最无趣的)。虽然某些度量值看上去是一致的,但是仍有某些度量产生十分不同的次序结果。例如,φ系数与和集体强度产生的秩是一致的,但是与兴趣因子和几率产生的秩有些不同。此外,相依表E10根据数具有最低秩,而根据兴趣因子却具有最高秩。表6-13相依表的例子实例fu fio for foo18123834241370833026221046EEEEEEE3954308052961288613631320443115002000500600040002000100030009481298127944000200020002007450248363 E10612483447452表6-14使用表6-11中的对称度量对相依表定秩a31EEE民E1232845678796K124365789167432598112357961847251364879S123465789523679518410E99410E101051010101010741表6-15使用表6-12中的非对称度量对相依表定秩M GL V F AVE11114225E222351132266EEEE549378612534649336855357448377E8678594977108810919941239718 PDGE10101081061010
3.客观度量的性质表6-14中的结果数据表明很多度量对同一个模式的质量提供了互相矛盾的信息。为了了解它们之间的差异,需要考察这些度量性质。反演性考虑图6-28中显示的位向量,每个列向量中的0/1位表示一个事务(行)是否包含某个特定的项(列)。例如,向量A表示项a属于第一个和最后一个事务,而向量B表示项b只在第五个事务中出现。事实上,向量C和E与向量A有一定的关系它们的位由(不出现)反转为1(出现),反之亦然。同理,向量D与向量B和也存在着同样的位反转关系。这种反转位向量的过程称为反演(inversion)。如果度量在反演操作下是不变的,则向量对(C,D)的度量值和向量对(A,B)的度量值应当相等。度量的反演性可以用如下方法检验。 A B C D E F00000010100001100(a)(b)(c)图6-28反演操作的结果。向量C和E是向量A的反演,而向量D是向量B和F的反演定义6.6反演性客观度量M在反演操作下是不变的,如果交换频度计数fi和foo、fio和f它的值保持不变。在反演操作下保持不变的度量有数、几率、K和集体强度。这些度量可能不适合分析非对称的二元数据。例如,向量C和D之间的数与向量A和B之间的数相等,尽管项c和d同时出现比项a和b同时出现更加频繁。此外,向量C和D之间的数小于向量E和F之间的数,虽然项e和f仅有一次同时出现。前面讨论系数的局限性时,已经提到该问题。对于非对称的二元数据,使用非反演不变的度量更可取。一些非反演不变的度量包括兴趣因子、IS、PS和 Jaccard系数。零加性假定对分析文档集中的一对词(如“数据”和“挖掘”)之间的联系感兴趣。如果向数据集中添加有关冰下捕鱼的文章,对分析词“数据”和“挖掘”之间的关联有影响吗?这种向数据集(在此情况下为文档)中添加不相关数据的过程就是所谓的零加(null addition)操作。定义6.7零加性客观度量M在零加操作下是不变的,如果增加f而保持相依表中所有其他的频度不变并不影响M的值。对文档分析或购物篮分析这样的应用,期望度量在零加操作下保持不变。否则的话,当添加足够多的不包含所分析词的文档时,被分析词语之间的联系可能会完全消失。满足零加性的度量包括余弦(IS)和 Jaccard()度量,而不满足该性质的量包括兴趣因子、 Piatetsky-Shapiro-(p几率(a)和系数。
缩放性表6-16显示了1993年和2004年注册某课程的学生的性别和成绩的相依表。表中的数据表明自1993年以来男生的数量翻了一番,而女生则是原来的3倍。然而,2004年的男生并不比1993年的表现得更好,因为高分和低分男同学的比率保持不变,即3:4。与之类似,2004年的女同学也并不比1993的表现得更好。尽管抽样分布发生了变化,但是成绩和性别之间的关联预期保持不变。表6-16成绩和性别例子男女男女高302050高6060120低401050低8030110703010014090230(a)1993年的样本数据(b)2004年的样本数据定义6.8缩放不变性客观度量M在行列缩放操作下是不变的,如果M(T)=M(T),其中,T是频度计数为 fio fo的相依表,T是频度计为[ kikfio kikafoo的相依表,而k1,k2,k3,k4是正常量。由表6-17可知,只有几率(a)在行和列缩放操作下是不变的。所有其他的度量,例如系数、K、IS、兴趣因子和集体强度(S),当相依表的行和列缩放时,它们的值也发生变化。虽然没有讨论非对称度量(如置信度、J度量、Gini指标和信任度)的性质,但很明显,在反演和行列缩放操作下,这些度量不可能保持相同的值,不过它们在零加操作下是不变的。表6-17对称度量的性质符号度量反演零加缩放中数 Yes No Noa几率 Yes No Yes Cohen度量 Yes No No兴趣因子N NoNoIS余弦 No Yes No PS Piatetsky-ShapiroYes-度量 No NoS集体强度 Yes NoNo Jaccard No Yes Noh全置信度 NoNo Nos支持度 No NoNo6.7.2多个二元变量的度量表6-11和表6-12显示的度量都是针对一对二元变量定义的,例如,2项集或关联规则。然而,其中某些也可以应用于较大的项集,如支持度和全置信度(al-confidence)其他度量(如兴趣因子、IS、PS和 Jaccard系数)使用多维相依表中的频率,可以扩展到多个变量。例如,表6-18显示了a,b和c的3维相依表。表中每个表目f都表示包含项a,b和c的某种组合的事务数。例如,fou表示包含a和c但不包含b的事务数。另一方面,边缘率f表示包含项a和cO而不管是否包含项b的事务数。
表6-18一个三维相关性表的例子 cbb b a fiut fior fi+ a firo fioo fiwo a fou foor forl foro foo fn f-orf fio给定一个k-项集{in,2,,},统计独立性条件可以定义如下:ff+ff(6-12)利用该定义,可以扩展基于背离统计独立性的客观度量(如兴趣因子(1)和PS)到多个变量: I= N-Ix fu.. .. .S=+ffNN另一种方法是,将客观度量定义为模式中项对之间关联的最大值、最小值或平均值。例如,给定k项集X={i1,i2,i},可以将X的数定义为X中所有项对(ii)之间的数的平均值。然而,由于该度量只考虑逐对之间的关联,所以它可能不能捕获模式中的隐含联系。由于数据中存在部分关联,多维相依表的分析更加复杂。例如,根据特定变量的值,某些关联可能出现或不出现。这个问题就是辛普森悖论(Simpson's' paradox),将在下一节中介绍。可以使用更复杂的统计技术(如对数线性模型)来分析这种联系,但是这些技术已经超出了本书的范围。6.7.3辛普森悖论解释变量之间的关联时要特别小心,因为观察到的联系可能受其他混淆因素的影响,这些因素,即没有包括在分析中的隐藏变量。在某些情况下,隐藏的变量可能会导致观察到的一对变量之间的联系消失或逆转方向,这种现象就是所谓的辛普森悖论。用下面的例子解释这种悖论的性质。考虑高清晰度电视(HDTV)销售和健身器销售之间的联系,如表6-19所示。规则(买TV=是}→{买健身器=是}的置信度是99/180=55%,而规则(买HDTV=否}→{买健身器=是}的置信度是54/120=45%。这些规则暗示,购买了高清晰度电视的顾客比那些没有购买高清晰度电视的顾客更有可能购买健身器。表6-19高清晰度电视和健身器销售之间的2路相依表奖买HDTV买健身器是否是否99811805466120153147300
然而,进一步深入分析表明这些商品的销售取决于顾客是大学生或还是在职人员。表6-20汇总了大学生和在职人员购买高清晰度电视和健身器之间的联系。注意,表中给出的大学生和在职人员的支持度计数的总和等于表6-19中显示的频度。而且,更多是在职人员而不是大学生购买了这些商品的。对于大学生:c(买HDTV=是}→买健身器=是)=1/10=10%c(买HDTV=}→(买健身器=是})=4/34=11.8%对于在职人员:c(买HDTV=是}→买健身器=是})=98/170=57.7%c(买HDTV=否}→{买健身器=是})=50/86=58.1%这些规则暗示,对于每一组顾客,不买高清晰度电视的顾客更可能购买健身器,这与先前由包含两组顾客的数据得到的结论恰好相反。即使采用其他度量(如相关性、几率或兴趣因子)仍然发现在组合数据情况下购买HDTV和健身器之间存在正相关,但是在分层数据情况下却存在负相关(参见本章习题20)。这种关联方向上的逆转就是辛普森悖论。表6-203路相依表的例子买健身器顾客组买HDTV总数是大学生是否是否14在职人员98否90261034170508这种悖论可以用下面的方法解释。注意,买高清晰度电视的顾客大部分都是在职人员,而且在职人员也是购买健身器的最大人群。由于接近85%的顾客是在职人员,所以在组合数据情况下观察到的HDTV和健身器之间的联系要强于分层情况下的联系。这也可以数学地解释如下。假设ab<cld并且plq<rls其中ab和plq是规则A→B在两个不同层下的置信度,cld和rls是规则A→B在这两个层中的置信度。当数据汇集在一起时,在组合数据集中这些规则的置信度值分别是(a+p)(b+q)和(c+r)(d+s)当(a+p)(b+q)>(c+r)/(d+s)时,辛普森悖论出现,从而导致变量间联系的错误结论。这里的教训是,需要适当的分层才能避免因辛普森悖论产生虚假的模式。例如,大型连锁超市的购物篮数据应该依据商店的位置分层,而不同病人的医疗记录应当按照混杂因素(如年龄和性别等)分层。6.8倾斜支持度分布的影响许多关联分析算法的性能受输入数据的性质的影响。例如, Apriori算法的计算复杂度取决于G数据中的项数和事务的平均长度等性质。本节讨论另一种重要性质,该性质对关联分析算法的性能和提取模式的质量具有重要影响。更具体地说,关注具有倾斜支持度分布的数据集,其中大多
数项具有较低或中等频率,但是少数项具有很高的频率。图6-29显示了一个呈现这种分布的实际数据集的例子。该数据取自UM(Public Use Micro data Sample)人口普查数据,它包含49046条记录和2113个非对称的二元变量。本节的剩余部分,把非对称二元变量作为项,把记录作为事务尽管数据集中超过80%的项的支持度小于1%,但是少数项的支持度大于90%。为了解释倾斜支持度分布对频繁项集挖掘的影响,将所有的项按照支持度分为3组,G1,G2和G3。表6-21显示了每一组中包含项的数量。104353251.50.55001000150020002500按支持度排序的项图6-29人口普查数据集中项的支持度分布表6-21按照项的支持度将人口普查数据集中的项分组组 G2 G3支持度<1%1%~90%>90%项的数量173535820选择合适的支持度阈值挖掘这样的数据集可能相当棘手。如果阈值太高(如20%),则可能遗漏涉及G1中较低支持度项的模式。在购物篮数据分析中,这种低支持度的项可能对应那些顾客很少买的昂贵商品(如珠宝),但是它们的模式仍然是零售商十分感兴趣的。相反,如果支持度阈值太低,由于下面的原因,挖掘关联模式将变得非常困难:首先,由于支持度阈值太低,已有的关联分析算法所需的计算量和内存需求都将显著增加;其次,由于支持度阈值太低,提取出的关联模式的数量大幅度增加;再次,可能会提取出大量的高频率项(如“牛奶”)与低频率项如“鱼子酱”)相关联的虚假模式,这样的模式就是所谓的交叉支持( cross-support)模式,由于它们的相关性往往太弱,这些模式多半是虚假的。例如,当支持度阈值等于0.05%时,将会挖掘出18847个涉及G1和G3中的项的频繁项对,其中93%的是交叉支持模式,即包含来自G1和G3的项的模式。从这些交叉支持模式得到的最大相关度是0.029,远远小于从同一个组中挖掘出的其他频繁模式(高达1.0)。对前面讨论的其他兴趣度度量也可以做出类似的结论。这个例子表明,当支持度值足够低时,可能产生大量弱相关的交叉支持模式。在介绍排除这些模式的方法G之前,首先形式地定义交叉支持模式定义6.9交叉支持模式交叉支持模式是一个项集X={i,i2,i},它的支持度比率
()_min[( )( i2),,()](6-13) max[s( ),().,( )小于用户指定的阈值h例6.4假设牛奶的支持度是70%,糖的支持度是10%,鱼子酱的是0.04%。给定h=0.01,频繁项集{牛奶,糖,鱼子酱}是一个交叉支持模式因为它的支持度比率为:min0..0.00040.000400058<0.01max[0.7,0.10.0004]0.7现有的度量(如支持度和置信度),都不足以消除交叉支持模式,如图6-30显示的数据集所示。假定h=0.3,项集{p,q}、{p,r}和{p,,r}是交叉支持模式,因为它们的支持度比率等于0.2,小于阈值0.3。虽然可以采用较高的支持度阈值(如20%)来消除交叉支持模式,但是,这样却损失了其他有趣的模式,如强关联项集{r},它的支持度为16.7%。0000000图6-30一个包含3个项p,q和r的事务数据集,其中p是高支持度项,q和r是低支持度项置信度剪裁也无济于事,因为由交叉支持模式提取的规则的置信度可能很高。例如,虽然{pq}是一个交叉支持模式,但是规则{q}→{p}置信度却是80%。交叉支持模式能够产生高置信度的规则并不奇怪,因为其中的项p在数据集中频繁出现。因此,p在许多包含q的事务中出现是意料之中的事。同时,即使{q,r}不是交叉支持模式,规则{q}→{r}也具有高置信度。这个例子表明,使用置信度度量很难区别从交叉支持模式或非交叉支持模式中提取的规则。回到前面的例子,注意到由于包含p的大部分事务不包含q,所以规则{p}→{q}的置信度很低。相反,由模式{g,r}导出的规则{r}→{q}却有很高的置信度。这一观察暗示,可以通过检查由给定项集提取的最低置信度规则来检测交叉支持模式。这一论断的证明可以从以下讨论中理解。
(1)回忆置信度的如下反单调性:conf({i,i2}→{i,i,i)≤conf({i2i3}→{i4,is,,i)该性质表明,把关联规则左边的项移到右边后不会增加规则的置信度。根据这一性质,由频繁项集提取的最低置信度规则的左边仅包含一个项。把左边只有一个项的所有规则的集合用R表示。(2)给定一个频繁项集[i,i2,i},如果s(i=max[(i)s(i,s(i),则规则{}→{i,i2-1,+1,,是R1中具有最小置信度的规则。这一结论直接由置信度是规则的支持度与规则前件支持度的比得到。(3)总结以上各点,可以从频繁项集{i,2,证}中得到的最低置信度为:s(i,i2,…,} max[ s ()(i2),.,( )这个表达式又称h置信度(h--confidence)或全置信度(allconfidence-)度量。由于支持度的反单调性,h置信度度量的分子受限于频繁项集所有项中最小的支持度。换句话说,项集X={i}的h置信度不超过下面表达式:h -confidence-(x)≤min[s(),s(2),…,s(i max[s(), s(i2),,()]注意h置信度的上界与公式(6-13)中支持度比率(的等价性因为交叉支持模式的支持度比率总是小于h,因此这类模式的h置信度也一定小于h因此,通过确保模式的h置信度值超过h就可以消除交叉支持模式。最后,值得一提的是,使用h置信度的好处不仅是消除交叉支持模式。这种度量也是反单调的,即 h-confidence({, i2,,])h-confidence( {i1, i2,, ik+1 }从而可以将它直接并入挖掘算法。此外,h置信度能够确保项集中的项之间是强关联的。例如,假定一个项集X的h置信度是80%。如果X中的一个项出现在某个事务中,则中其他的项至少有80%的几率属于同一个事务。这种强关联模式又称超团模式(hyperclique pattern)文献注释关联规则的挖掘首先是由 Agrawal等人在[228,229中提出,用来发现购物篮数据事务中各项之间的有趣联系。从那以后,人们进行了广泛的研究,以解决关联分析任务的概念、实现和应用问题。图6-31中汇总了该领域各种各样的研究活动。 POG
做回鞦四回回似燃米台吗图州兰导回输回你私尔回的E集什乏学繁回世归最歌起只妈你尔沓米世器!平新名如券回的整名名长应控会剛名图马⌒图回聚东 PDG
1.概念问题概念问题的研究主要集中在建立描述关联分析的理论基础的框架,扩展形式机制,以处理新的模式类型,以及扩展形式机制,以纳入非对称二元数据之外的属性类型。继 Agrawal等人的开创性工作之后,在发展关联分析问题的理论方面已有大量的研究成果。在[254]中, Gunopoulos等证明了挖掘极大频繁项集问题和超图遍历问题之间的关系,并推出关联分析任务复杂度的上界。Zaki等[334,336]和 Pasquier等[294]使用形式概念分析研究频繁项集产生问题。随后Zaki等人的工作提出了闭频繁项集的理论[336] Friedman等在多维空间凸点搜索(bump hunting)的背景下研究了关联分析的问题252]。更具体地说,他们把频繁项集产生看作在多维空间中寻找高概率的稠密区域。许多年来,已经定义了许多新类型的模式,如轮廓关联规则(profile association rule)[225]环关联规则(cyclic association rule)[290]、糊关联规则[273]、例外规则[316]、负关联规则[238,304]、加权的关联规则[240,300]、依赖规则308]、罕见规则(peculiar rule)[340]、事务间关联规则(inter-transaction- association rule)[250323]和局部分类规则[231,285]其他类型的模式包括闭项集[294,336]、极大项集[234]、超团模式[330]、支持包络(support envelope)[314]、显露模式[246]和对比集( contrast set)[233]。关联分析也成功地应用于序列数据[230,312]、空间数据[266]和基于图的数据[268,274,293,331,335]。交叉支持模式的概念首先由Hui等[330]提出。作者还提出了一种有效的自动删除交叉支持模式的算法,称为超团挖掘(Hyperclique Miner)已经做了大量的研究,将最初的关联规则分析扩展到了标称属性[311]、序数属性[281]、区间属性[284]和比率属性[253,255,311,325,339]一个关键性的问题是如何定义这些属性的支持度。 Steinbach等[315]提出了一种方法,将传统的支持度概念扩展到更一般的模式和属性类型。2.实现问题这一领域的研究活动主要涉及:(1)将挖掘能力集成到现有的数据库技术中;(2)产生高效的可伸缩的挖掘算法;(3)处理用户指定的或领域特定的约束;(4)提取模式的后处理。将关联分析集成到现有的数据库技术中有着许多优点。首先,可以利用数据库系统的索引和查询处理机制;其次,可以利用DBMS对可伸缩性、检查点和并行性的支持[301] Houtsma等[265]提出的SETM算法是早期通过SQL查询支持关联规则挖掘的算法之一。从那时起,产生了许多的算法,用以在数据库系统中提供关联规则挖掘能力。例如,DMQL[258]和M-SQL[267]查询语言用新的关联规则挖掘操作扩展了基本SQL.挖掘规则操作(Mine Rule operator)[283]是一种表达能力很强的SQL操作,可以处理聚集属性和项分层结构Tsur等[322]提出了称作查询群( query flock)的挖掘关联规则的产生测试的法。Chen等[241]提出了分布的、基于OLAP的挖掘多层关联规则的框架。 Dunkel和 Soparkar[248]研究了 Apriori算法的时间和存储复杂度。Han等[259]提出了FP增长算法。挖掘频繁项集的其他算法包括ark等[292]提出的DHP算法和 Savasere等[303]提出的划分算法 Toivonen[320]提出了基于抽样的频繁项集产生算法,这种算法只需要扫描一次数据集,但是它产生相对较多的候选项集。动态项集计数(dynamic itemset counting,DC)算法[239]只需要扫描数据集1.5次,并且它产生的候选项集少于基于抽样的算法。其他著名的算法包括树投影算法223-Mine[295关于频繁项集产生算法的综述可以在文献2262中找到。频繁项集挖掘实现库 fimi (http: /fimi.cs. helsinki.fi )提供了有用的数据集和挖掘算法。许多作者都提出了挖掘关联模式的并行算法[224,256,287,306,337]。这些算法的综述可以在[333]中找到。
 Hidber[260]和 Cheung等[242]还提出了挖掘关联规则算法的联机和增量版本。 Srikant等[313]考虑了在布尔约束下挖掘关联规则的问题。例如,给定诸如(饼干牛奶)(descendents(饼)ancestors(小麦面包))约束,算法寻找包含饼干和牛奶的规则,或包含饼干的后代而不包含小麦面包的祖先的规则。 Singh等[310]和Ng等[288]提出了另外一种基于约束的关联规则挖掘技术。也可以对不同项集的支持度施加约束。Wang等[324]、Liu等[279]和Seno等[305]研究了这个问题。关联分析的潜在问题是现在的算法可能产生大量的模式。为了解决这个问题,提出了模式定秩、汇总和模式过滤方法。 Toivonen等[321]提使用结构规则覆盖(structural rule cover)删除冗余规则、并使用聚类对剩下规则分组的思想Liu等[280使用统计x2检验排除虚假模式,并采用一种称作方向设置规则(direction setting rule)的模式子集汇总剩下的模式。许多研究者都考察了使用客观度量过滤模式的方法,包括Bri等[238]、 Bayardo和 Agrawal[235]、 Aggarwal和 Yu[227], DuMouchel Pregibon[247]. Piatetsky-Shapiro[297]. Kamber Singhal[270] Hilderman和 Hamilton[261]、Tan等[318]分析了这些度量的性质。“成绩性别”例子用于强调行、列缩放不变性的重要性,该例很大程度上是受 Mosteller在286]中的讨论的影响。同时,“喝茶喝咖啡”的例子用以解释置信度的局限性,该例是受Bin等[238]给出的例子的启发。由于置信度的局限性,Brin等[238]提出使用兴趣因子作为兴趣度度量的思想。 Omiecinski[289]提出了全置信度度量观点。 Xiong等[330]引进交叉支持性质,并表明全置信度度量可以用来删除交叉支持模式。使用支持度之外的客观度量的主要困难在于它们不具有单调性,这使得它们很难直接应用到挖掘算法中。 Xiong等[328]通过引进数的上界函数,提出了一种高效的挖掘相关性的方法。虽然数是非单调的,但是它有一个上界表达式,可以用来有效地挖掘强相关的项对。 Fabris和 Freitas[249]提出了一种方法,通过检测辛普森悖论[309]发现有趣的关联。 Megiddo和 Srikant[282]也介绍了一种方法,采用假设检验来验证提取的模式。为了避免因多重比较问题而产生虚假模式,提出了一种基于再抽样的技术。Bolton等[237]使用 Benjamini-Hochberg-[236]和 Bonferroni校正方法调整从购物篮数据中挖掘出的模式的p值。Webb[326]和 Zhang等[338]提出了另外一种多重比较问题的方法。许多研究者都研究了主观度量在关联分析中的应用 Silberschatz和 Tuzhilin[307提出了从主观角度判断一个规则是否是有趣的两条原则。Liu等[277]提出了非期望条件规则的概念。 Cooley等[243]使用 Dempster-Shafer-理论分析组合软置信集的思想,并使用这种方法识别Web数据中相反或新颖的关联模式。其他方法包括使用贝叶斯信念网络[269]和基于近邻的信息[245]识别主观上有趣的模式。可视化也有助于用户快速地掌握发现模式的基本结构许多商业数据挖掘工具把规则的完全集(满足支持度和置信度阈值)以二维图的形式显示,其中每个轴对应于这个规则的前件或后件项集。 Hofmann等[263]提出使用 Mosaic图和双层图显示关联规则。这种方法不仅仅能够显示一条特定的规则,而且还显示规则的前件项集和后件项集之间的相依表。然而,这种技术假定规则的后件只有一个属性。关联分析已经应用于各种各样的应用领域,如Web(296317、分析[264]、通信警G3.应用问题告分析[271]、网络入侵检测[232,244,275]和生物信息学[302,327。文献[298,299,319]考察了关联和相关模式分析在地球科学的研究中的应用。
关联模式也已经应用到其他学习问题,如分类[276,278]、回归[291]和聚类[257,329,332] Freitas在他的意见书[251]中对分类和关联规则挖掘进行了比较。许多作者研究了将关联模式应用于聚类,包括Han等[257]、 Kosters等[272]Yang等[332]和 Xiong等[329]参考文献 (223] R. C. Agarwal, C. C. Aggarwal, and V. V. V. Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets. Journal of Parallel and Distributed Computing (Special Issue on High Performance Data Mining), 61(3):350-371, 2001 [224] R. C. Agarwal and J. C. Shafer. Parallel Mining of Association Rules. IEEE Transactions on Knowledge and Data Engineering, 8(6): 962-969, March 1998. [225] C. C. Aggarwal, Z. Sun, and P. S. Yu. Online Generation of Profile Association Rules. In Proc. of the 4th Intl. Conf. on Knowledge Discovery and Data Mining, pages 129-133, New York, NY, August1996. [226] C. C. Aggarwal and P. S. Yu. Mining Large Itemsets for Association Rules. Data Engineering Bulletin, 21(1): 23-31, March 1998 [227] C. C. Aggarwal and P. S. Yu. Mining Associations with the Collective Strength Approach. IEEE Trans. on Knowledge and Data Engineering, 136):863-873, January/February 2001. [228] R. Agrawal, T. Imielinski, and A. Swami. Database mining: A performance perspective. IEEE Transactions on Knowledge and Data Engineering, 5: 914-925,1993.[229]. Agrawal,t. Imielinski,anda. Swami. Mining association rules between sets of items in large databases. In Proc. ACM SIGMOD Intl. Conf. Management of Data, pages 207-216, Washington,DC,1993. [230] R. Agrawal and R. Srikant. Mining Sequential Patterns. In Proc. of Intl. Conf. On Data Engineering, pages 3-14, Taipei, Taiwan, 1995. [231] K. Ali, S. Manganaris, and R. Srikant. Partial Classification using Association Rules. In Proc. of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 115-118, Newport Beach, CA, August 1997 [232] D. Barbara, J. Couto, S. Jajodia, and N. Wu. ADAM: A Testbed for Exploring the Use of Data Mining in Intrusion Detection. SIGMOD Record, 30(4):15-24, 2001 [233] S. D. Bay and M. Pazzani. Detecting Group Differences: Mining Contrast Sets. Data Mining and Knowledge Discovery, 5(3):213-246, 2001. [234] R. Bayardo. Efficiently Mining Long Patterns from Databases. In Proc. of 1998 ACM-SIGMOD Intl. Conf. on Management of Data, pages 85-93, Seattle, WA, June 1998 [235] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the 5th Intl. Conf. on Knowledge Discovery and Data Mining, pages 145-153, San Diego, CA, August 1999. [236] Y. Benjamini and Y. Hochberg. Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. Journal Royal Statistical Society B, 57(1): 289-300, 1995. [237] R. J. Bolton, D. J. Hand, and N. M. Adams. Determining Hit Rate in Pattern Search. In Proc. of the ESF Exploratory Workshop on Pattern Detection and Discovery in Data Mining, pages 36-48, London, UK, September 2002. [238] S. Brin, R. Motwani, and C. Silverstein. Beyond market baskets: Generalizing association rules to correlations. In Proc. ACM SIGMOD Intl. Conf. Management of Data, pages 265-276, Tucson, AZ,1997. [239] S. Brin, R. Motwani, J. Ullman, and S. Tsur. Dynamic Itemset Counting and Implication Rules for market basket data. In Proc. of 1997 ACM-SIGMOD Intl. Conf. on Management of Data, pages 255-264, Tucson, AZ, June 1997. [240] C. H. Cai, A. Fu,. H. Cheng, and W. W. Kwong. Mining Association Rules with Weighted Items. In Proc. of IEEE Intl. Database Engineering and Applications Symp., pages 68-77, Cardiff, Wales,1998
 [241] Q. Chen, U. Dayal, and M. Hsu. A Distributed OLAP infrastructure for E-Commerce. In Proc. of the 4th IFCIS Intl. Conf. on Cooperative Information Systems, pages 209-220, Edinburgh, Scotland1999[242]d.c. Cheung,s.d.lee,andb.kao. General Incremental Technique for Maintaining Discovered Association Rules. In Proc. of the 5th Intl. Conf. on Database Systems for Advanced Applications. ges 185-194, Melbourne, Australia, 1997. page [243] R. Cooley, P. N. Tan, and J. Srivastava. Discovery of Interesting Usage Patterns from Web Data. In M. Spiliopoulou and B. Masand, editors, Advances in Web Usage Analysis and User Profiling, volume 1836, pages 163-182. Lecture Notes in Computer Science, 2000. [244] P. Dokas, L. Ertoz, V. Kumar, A. Lazarevic, J. Srivastava, and P. N. Tan. Data Mining for Network Intrusion Detection. In Proc. NSF Workshop on Next Generation Data Mining, Baltimore, MD, 2002. [245] G. Dong and J. Li. Interestingness of discovered association rules in terms of neighborhood-based unexpectedness. In Proc. of the 2nd Pacific-Asia Conf. on Knowledge Discovery and Data Mining, pages 72-86, Melbourne e, Australia, April 1998 [246]. Dong and J. Li. Efficient Mining of Emerging Patterns: Discovering Trends and Differences. In Proc. of the 5th Intl. Conf. on Knowledge Discovery and Data Mining, pages 43-52, San Diego, CA, August 1999. [247] W. DuMouchel and D. Pregibon. Empirical Bayes Screening for Multi-Item Associations. In Proc. of the 7th Intl. Conf. on Knowledge Discovery and Data Mining, pages 67-76, San Francisco, CA, August 2001 [248] B. Dunkel and N. Soparkar. Data Organization and Access for Efficient Data Mining. In Proc. of the 15th Intl. Conf. on Data Engineering, pages 522-529Sydney, Australia, March 1999 [249] C.. Fabris and A. A. Freitas. Discovering surprising patterns by detecting occurrences of Simpson's paradox. In Proc. of the 19th SGES Intl. Conf on Knowledge-Based Systems and Applied Artificial Intelligence, pages 148-160, Cambridge, UK, December 1999. [250] L. Feng, H. J. Lu, J. X. Yu, and J. Han Mining inter-transaction associations with templates. In Proc. of the 8th Intl. Conf. on Information and Knowledge Management, pages 225-233, Kansas City, Missouri, Nov 1999 [251] A. A. Freitas. Understanding the crucial differences between classification and discovery of association rules a position paper. SIGKDD Explorations, 2(1): 65-69, 2000 [252] J. H. Friedman and N. I. Fisher. Bump hunting in high-dimensional data. Statistics and Computing,9(2):123-143, April1999 [253] T. Fukuda, Y. Morimoto, S. Morishita and T. Tokuyama. Mining Optimized Association Rules for Numeric Attributes. In Proc. of the 15th Symp. on Principles of Database Systems, pages 182-191, Montreal, Canada, June 1996 [254] D. Gunopulos, R. Khardon, H. Mannila, and H. Toivonen. Data Mining, Hypergraph Transversals and Machine Learning. In Proc. of the 16th Symp. on Principles of Database Systems, pages 209- 216, Tucson, AZ, May 1997. [255] E.-H. Han, G. Karypis, and V. Kumar. Min-Apriori An Algorithm for Finding Association Rules in data with  continuous attributes. http: //www.cs.umn.e/han, 1997 [256] E.-H. Han, G. Karypis, and V. Kumar. Scalable Parallel Data Mining for Association Rules. In Proc. of 1997 ACM-SIGMOD Intl. Conf. on Management of Data, pages 277-288, Tucson, AZ, May1997. [257] E.-H. Han, G. Karypis, V. Kumar, and B. Mobasher. Clustering Based on Association Rule Hypergraphs. In Proc. of the 1997 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, Tucson, AZ, 1997.[258].han,y.fu,k. Koperski,w.wang,ando.r. Zaiane.dmql: data mining query language for relational databases. In Proc. of the 1996 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, Montreal Canada, June 1996 [259] J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc ACM-SIGMOD Int. Conf. on Management of Data (SIGMOD'00), pages 1-12, Dallas, TX, May
2000. [260] C. Hidber. Online Association Rule Mining. In Proc. of 1999 ACM-SIGMOD Intl. Conf. on Management of Data, pages 145-156, Philadelphia, PA, 1999 [261] R. J. Hilderman and H. J. Hamilton. Knowledge Discovery and Measures of Interest. Kluwer Academic Publishers, 2001. [262] J. Hipp, U. Guntzer, and G. Nakhaeizadeh. Algorithms for Association Rule MiningA General Survey. SigKDD Explorations, 2(1): 58 -64, June 2000. [2631 H. Hofmann, A.. J. M. Siebes, and A. F. X. Wilhelm. Visualizing Association Rules with Interactive Mosaic Plots. In Proc. of the 6th Intl. Conf. on Knowledge Discovery and Data Mining, pages 227 -235, Boston, MA, August 2000. [264] J. D. Holt and S. M. Chung. Efficient Mining of Association Rules in Text Databases. In Proc. of the 8th Intl. Conf. on Information and Knowledge Management, pages 234-242, Kansas City, Missouri,1999 [265] M. Houtsma and A. Swami. Set-oriented Mining for Association Rules in Relational Databases. In Proc. of the 1lth Intl. Conf. on Data Engineering, pages 25-33, Taipei, Taiwan, 1995. [266] Y. Huang, S. Shekhar, and H. Xiong. Discovering Co-location Patterns from Spatial Datasets: A General Approach. IEEE Trans. on Knowledge and Data Engineering, 16(12): 1472-1485,4 December 200 [267] T. Imielinski, A. Virmani, and A. Abdulghani. DataMine: Application Programming Interface and Query Language for Database Mining. In Proc. of the 2nd Intl. Conf. on Knowledge Discovery and Data Mining, pages 256-262, Portland, Oregon, 1996 [268] A. Inokuchi, T. Washio, and H. Motoda. An Apriori-based Algorithm for Mining Frequent Substructures from Graph Data. In Proc. of the 4th European Conf. of Principles and Practice of Knowledge Discovery in Databases, pages 13-23, Lyon, France, 2000. [269] S. Jaroszewicz and D. Simovici. Interestingness of Frequent Itemsets Using Bayesian Networks as Background Knowledge. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 178-186, Seattle, WA, August 2004 [270] M. Kamber and R. Shinghal. Evaluating the Interestingness of Characteristic Rules. In Proc. of the 2nd Intl. Conf. on Knowledge Discovery and Data Mining, pages 263-266, Portland, Oregon, 1996. [271] M. Klemettinen. A Knowledge Discovery Methodology for Telecommunication Network Alarm Databases. PhD thesis, University of Helsinki, 1999 [272] W.A. Kosters, E. Marchiori, and A. Oerlemans. Mining Clusters with Association Rules. In The 3rd Symp. on Intelligent Data Analysis (IDA99), pages 39-50, Amsterdam, August 1999. [273] C. M. Kuok, A. Fu, and M. H. Wong. Mining Fuzzy Association Rules in Databases. ACM SIGMOD Record,27(1):41-46, March1998. [274] M. Kuramochi and G. Karypis. Frequent Subgraph Discovery. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining, pages 313-320, San Jose, CA, November 2001. [275]. Lee, S. J. Stolfo, and K. W. Mok. Adaptive Intrusion Detection: A Data Mining Approach Artificial Intelligence Review, 14(6): 533-567, 2000. [276] W. Li, J. Han, and J. Pei. CMAR Accurate and Efficient Classification Based on Multiple Class-association Rules. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining, pages 369-376,San Jose, CA, 2001. [277] B. Liu, W. Hsu, and S. Chen. Using General Impressions to Analyze Discovered Classification Rules. In Proc. of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 31-36, Newport Beach, CA, August 1997. [278] B. Liu, W. Hsu, and Y. Ma. Integrating Classification and Association Rule Mining. In Proc. of the 4th Intl. Conyf. on Knowledge Discovery and Data Mining, pages 80-86, New York, NY, August 1998. [279] B. Liu, W. Hsu, and Y. Ma. Mining association rules with multiple minimum supports. In Proc. of the Sth Intl. Conf. on Knowledge Discovery and Data Mining, pages 125-134, San Diego, CA, August61999 [280] B. Liu, W. Hsu, and Y. Ma. Pruning and Summarizing the Discovered Associations. In Proc. of the 5th
 Intl. Conf. on Knowledge Discovery and Data Mining, pages 125-134, San Diego, CA, August 1999. [281] A. Marcus, J. I. Maletic, and K.-I. Lin. Ordinal association rules for error identification in data sets. In Proc. of the 10th Intl. Conf. on Information and Knowledge Management, pages 589-591, Atlanta a, GA, October 2001 [282] N. Megiddo and R. Srikant. Discovering Predictive Association Rules. In Proc. of the 4th Intl. Conf. on Knowledge Discovery and Data Mining, pages 274-278, New York, August 1998 [283] R. Meo, G. Psaila, and S. Ceri. A New SQL-like Operator for Mining Association Rules. In Proc. of the 22nd VLDB Conf., pages 122 -133, Bombay, India, 1996. [284] R. J. Miller and Y. Yang. Association Rules over Interval Data. In Proc. of 1997 ACM-SIGMOD Intl. Conf. on Management of Data, pages 452-461 Tucson, AZ, May 1997 [285] Y. Morimoto, T. Fukuda, H. Matsuzawa, T. Tokuyama, and K. Yoda. Algorithms for mining association rules for binary segmentations of huge categorical databases. In Proc. of the 24th VLDB Conf., pages 380-391, New York, August 1998. [286] F. Mosteller. Association and Estimation in Contingency Tables. Journal of the American Statistical Association, 63:1-28, 1968 [287] A. Mueller. Fast sequential and parallel algorithms for association rule mining: A comparison. Technical Report CS-TR-3515, University of Maryland, August 1995 [288] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang. Exploratory Mining and Pruning Optimizations of Constrained Association Rules. In Proc. of 1998 ACM-SIGMOD Intl. Conf. on Management of Data, pages 13-24, Seattle, WA, June 1998 [289] E. Omiecinski. Alternative Interest Measures for Mining Associations in Databases. IEEE Trans. on Knowledge and Data Engineering, 15(1)57-69, January/February 2003 [290] B. Ozden, S. Ramaswamy, and A. Silberschatz. Cyclic Association Rules. In Proc. of the 14th Intl. Conf. on Data Eng., pages 412-421, Orlando, FL, February 1998. [291] A. Ozgur, P. N. Tan, and V. Kumar. RBA: An Integrated Framework for Regression based on Association Rules. In Proc. of the SIAM Intl. Conf. on Data Mining, pages 210-221, Orlando, FL, April 2004. [292] J. S. Park, M.-S. Chen, and P. S. Yu. An effective hash-based algorithm for mining association rules. SIGMOD Record,25(2):175-186,1995. [293) S. Parthasarathy and M. Coatney Efficient Discovery of Common Substructures in Macromolecules. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, pages 362-369, Maebashi City,Japan December 2002. [294] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules. In Proc. of the 7th Intl Conf. on Database Theory (ICDT'99), pages 398-416, Jerusalem, Israel, January 1999. [295] J. Pei, J. Han, H. J. Lu, S. Nishio, and S. Tang. H-Mine: Hyper-Structure Mining of Frequent Patterns in Large Databases. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining, pages 441-448, San Jose CA, November 2001. [296] J. Pei, J. Han, B. Mortazavi-Asl, and. Zhu. Mining Access Patterns Efficiently from Web Logs. In Proc. of the 4th Pacific-Asia Conf. on Knowledge Discovery and Data Mining, pages 396-407, Kyoto, Japan, April 2000. [297] G. Piatetsky-Shapiro. Discovery, Analysis and Presentation of Strong Rules. In G. Piatetsky-Shapiro and W. Frawley, editors, Knowledge Discovery in Databases, pages 229-248. MIT Press, Cambridge, MA, 1991. [298] C. Potter, S. Klooster, M. Steinbach, P. N. Tan, V. Kumar, S. Shekhar, and C. Carvalho. Understanding Global Teleconnections of Climate to Regional Model Estimates of Amazon Ecosystem Carbon Fluxes. Global Change Biology, 10(5): 693-703, 2004. [299] C. Potter, S. Klooster, M. Steinbach, P. N. Tan, V. Kumar, S. Shekhar, R. Myneni, and R. Nemani. Global Teleconnections of Ocean Climate to Terrestrial Carbon Flux. J Geophysical Research108(D17),2003 [300] G. D. Ramkumar, S. Ranka, and S. Tsur. Weighted Association Rules Model and Algorithm.
 http: //www.cs.ucla.edu/ czdemo/tsur/, 1997. [301] S. Sarawagi, S. Thomas, and R. Agrawal. Integrating Mining with Relational Database Systems: Alternatives and Implications. In Proc. of 1998 ACM-SIGMOD Intl. Conf. on Management of Data, pages 343-354, Seattle, WA, 1998 [302] K. Satou, G. Shibayama, T. Ono, Y. Yamamura, E. Furuichi, S. Kuhara and T. Takagi. Finding Association Rules on Heterogeneous Genome Data. In Proc. of the Pacific Symp. on Biocomputing, pages 397-408, Hawaii, January 1997 [303] A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association rules in large databases. In Proc. of the 21st Int. Conf on Very Large Databases (VLDB'95), pages 432-444, Zurich, Switzerland, September 1995. [304] A. Savasere, E. Omiecinski, and S. Navathe Mining for Strong Negative Associations in a Large Database of Customer Transactions. In Proc. of the 14th Intl. Conf. on Data Engineering, pages 494-502, Orlando, Florida, February 1998. [305] M. Seno and G. Karypis. LPMiner An Algorithm for Finding Frequent Itemsets Using Length-Decreasing Support Constraint. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining,pages 505-512, San Jose, CA, November 2001 [306] T. Shintani and M. Kitsuregawa. Hash based parallel algorithms for mining association rules. In Proc of the 4th Intl. Conf. on Parallel and Distributed Info. Systems, pages 19-30, Miami Beach, FL, December 1996 [307] A. Silberschatz and A. Tuzhilin. What makes patterns interesting in knowledge discovery systems. IEEE Trans. on Knowledge and Data Engineering, 8(6): 970-974, 1996 [308] C. Silverstein, S. Brin, and R. Motwani. Beyond market baskets: Generalizing association rules to dependence rules. Data Mining and Knowledge Discovery, 2(1): 39-68, 1998. [3091 E.-H. Simpson. The Interpretation of Interaction in Contingency Tables. Journal of the Royal Statistical Society, B(13): 238 -241, 1951. [310]. Singh,. Chen, R. Haight, and P. Scheuermann. An Algorithm for Constrained Association Rule Mining in Semi-structured Data. In Proc. of the 3rd Pacific-Asia Conf. on Knowledge Discovery and Data Mining, pages 148-158, Beijing, China, April 1999 [311] R. Srikant and R. Agrawal. Mining Quantitative Association Rules in Large Relational Tables. In Proc. of 1996 ACM-SIGMOD Inil. Conf. on Management of Data, pages 1-12, Montreal, Canada, 1996 [312] R. Srikant and R. Agrawal. Mining Sequential Patterns: Generalizations and Performance Improvements. In Proc. of the 5th Intl Conf. on Extending Database Technology (EDBT'96), pages 18-32, Avignon, France,1996. [313] R. Srikant, Q. Vu, and. Agrawal. Mining Association Rules with Item Constraints. In Proc. of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 67-73, Newport Beach, CA, August 1997 [314] M. Steinbach, P. N. Tan, and V. Kumar Support Envelopes: Technique for Exploring the Structure of Association Patterns. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 296-305, Seattle, WA, August 2004 [315] M. Steinbach, P. N. Tan, H. Xiong, and V. Kumar. Extending the Notion of Support. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 689-694, Seattle, WA, August 2004. [316] E. Suzuki. Autonomous Discovery of Reliable Exception Rules. In Proc. of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 259-262, Newport Beach, CA, August 1997. [317]. N. Tan and V. Kumar. Mining Association Patterns in Web Usage Data. In Proc. of the Intl. Conf. on Advances in Infrastructure for e-Business-,e -Education-,e- Science and e-Medicine- on the Internet, L' Aquila, Italy, January 2002 [318] P. N. Tan, V. Kumar, and J. Srivastava Selecting the Right Interestingness Measure for Association Patterns. In Proc. of the 8th Intl. Conf. on Knowledge Discovery and Data Mining, pages 32-41, Edmonton, Canada, July 2002. [319] P. N. Tan, M. Steinbach, V. Kumar, S. Klooster, C. Potter, and A. Torregrosa. Finding Spatio-Temporal Patterns in Earth Science Data In KDD 2001 Workshop on Temporal Data Mining,
 San Francisco, CA, 2001. [320] H. Toivonen. Sampling Large Databases for Association Rules. In Proc. of the 22nd VLDB Conf., pages 134-145, Bombay, India, 1996[321h. Toivonen,m. Klemettinen,p. Ronkainen,k. Hatonen,andh. Mannila. Pruning and Grouping Discovered Association Rules. In ECML-95 Workshop on Statistics, Machine Learning and Knowledge Discovery in Databases, pages 47-52, Heraklion, Greece, April 1995 [322] S. Tsur, J. Ullman, S. Abiteboul, C. Clifton R. Motwani, S. Nestorov, and A. Rosenthal.Query Flocks: A Generalization of Association Rule Mining. In Proc. of 1998 ACM-SIGMOD Intl. Conf. on Management of Data, pages 1 -12, Seattle, WA, June 1998. [323] A. Tung, H. J. Lu, J. Han, and L. Feng. Breaking the Barrier of Transactions: Mining Inter-Transaction Association Rules. In Proc. of the 5th Intl. Conf. on Knowledge Discovery and Data Mining, pages 297 -301, San Diego, CA, August 1999 [324] K. Wang, Y. He, and J. Han. Mining Frequent Itemsets Using Support Constraints. In Proc. of the 26th VLDB Conf., pages 43-52, Cairo, Egypt, September 2000325]k.wang,s.h.tay,andb.liu. Interestingness Interval Merger for Numeric Association Rules. In Proc. of the 4th Intl. Conf. on Knowledge Discovery and Data Mining, pages 121-128, New York, NY, August 1998 [326] G. I. Webb. Preliminary investigations into statistically valid exploratory rule discovery. In Proc. of he Australasian Data Mining Workshop (Aus DMO3), Canberra, Australia, December 2003. [327] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. R. Holbrook. Identification of Functional Modules in Protein Complexes via Hyperclique Pattern Discovery. In Proc. of the Pacific Symposium on Biocomputing, (PSB 2005), Maui, January 2005. [328] H. Xiong, S. Shekhar, P. N. Tan, and V. Kumar. Exploiting a Support-based Upper Bound of Pearson's Correlation Coefficient for Efficiently Identifying Strongly Correlated Pairs. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 334-343, Seattle, WA, August2004[329]h. Xiong,m. Steinbach,p.n.tan,andv. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of the SIAM Intl. Conf. on Data Mining, pages 279-290,Orlando, FL, April2004[330]h. Xiong,p.n.tan,andv. Kumar. Mining Strong Affinity Association Patterns in Data Sets with Skewed Support Distribution. In Proc. Of the 2003 IEEE Intl. Conf. on Data Mining, pages 387-394 Melbourne, FL, 2003 [331] X. Yan and J. Han. gSpan: Graph-based Substructure Pattern Mining. In Proc of the 2002 IEEE Intl. Conf. on Data Mining, pages 721-724, Maebashi City, Japan, December 2002. [332] C. Yang, U. M. Fayyad, and P. S. Bradley Efficient discovery of error-tolerant frequent itemsets in high dimensions. In Proc. of the 7th Intl. Conf. on Knowledge Discovery and Data Mining, pages 194-203, San Francisco, CA, August 2001. [333] M. J. Zaki. Parallel and Distributed Association Mining: A Survey. IEEE Concurrency, special issue on Parallel Mechanisms for Data Mining, 7(4): 14-25, December 1999. [334] M. J. Zaki. Generating Non-Redundant Association Rules. In Proc. of the 6th Intl. Conf. on Knowledge Discovery and Data Mining, pages 34-43, Boston, MA, August 2000 [335]. J. Zaki. Efficiently mining frequent trees in a forest. In Proc. of the 8th Intl. Conf. on Knowledge Discovery and Data Mining, pages 71-80, Edmonton, Canada, July 2002. [336] M. J. Zaki and M. Orihara. Theoretical foundations of association rules. In Proc. of the 1998 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, Seattle, WA, June1998 [337] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New Algorithms for Fast Discovery of Association Rules. In Proc. of the 3rd Intl. Conf. on Knowledge Discovery and Data Mining, pages 283-286, Newport Beach, CA, August 1997. [338] H. Zhang, B. Padmanabhan, and A. Tuzhilin. On the Discovery of Significant Statistical Quantitative Rules. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 374-383,
 Seattle, WA, August 2004. [339] Z. Zhang, Y. Lu, and B. Zhang. An Effective Partioning-Combining Algorithm for Discovering Quantitative Association Rules. In Proc. of the Ist Pacific-Asia Conf. on Knowledge Discovery and Data Mining, Singapore, 1997. [340] N. Zhong,. Y. Yao, and S. Ohsuga. Peculiarity Oriented Multi-database Mining. In Proc. of the 3rd European Conf. of Principles and Practice of Knowledge Discovery in Databases, pages 136 -146. Prague, Czech Republic, 1999.习题1.对于下面每一个问题,请在购物篮领域举出一个满足下面条件的关联规则的例子。此外,指出这些规则是否是主观上有趣的。(a)具有高支持度和高置信度的规则。(b)具有相当高的支持度却有较低置信度的规则。(c)具有低支持度和低置信度的规则。(d)具有低支持度和高置信度的规则。2.考虑表6-22中显示的数据集。表6-22购物篮事务的例子顾客ID事务ID购买项10001 {a,d,e)10024 (a,b,c,e)0012 {a,b,d,e)20031 {a,c,d,e)30015 {b,c,e]30022 {b,d,e)0029 {c,d)0040 {a,b,c)50033 {a,d,e)50038 {a,b,e}(a)将每个事务ID视为一个购物篮,计算项集{e},{b,d}和{b,d,e}的支持度(b)使用(a)的计算结果,计算关联规则{b,d}→{e}和{e}→{b,d}的置信度。置信度是对称的度量吗?(c)将每个顾客ID作为一个购物篮,重复(a应当将每个项看作一个二元变量(如果个项在顾客的购买事务中至少出现了一次,则为1;否则,为0)(d)使用(c)的计算结果,计算关联规则{,d}→{e}和{e}→{b,d}的置信度。(e)假定s1和1是将每个事务ID作为一个购物篮时关联规则r的支持度和置信度,而s2和C2是将每个顾客ID作为一个购物篮时关联规则r的支持度和置信度。讨论s1和S2或c1和c2之间是否存在某种关系?3.(a)规则→A和A→的置信度是多少?(b)令c1,c2和c3分别是规则{p}→{q},{p}→{q,r}和{p,r}→{q}的置信度。如果假定cc2和C3有不同的值,那么C1,C2和C3之间可能存在什么关系?哪个规则的置信度最低? PDG(c)假定(b)中的规则具有相同的置信度重复(b)的分析。哪个规则的置信度最高?(d)传递性:假定规则A→B和B→C的置信度都大于某个阅值 minconf。规则A→C可能具有
小于 minconf的置信度吗?4.对于下列每种度量,判断它是单调的、反单调的或非单调的(即既不是单调的,也不是反单调的)例如:支持度s=()T是反单调的,因为只要xcY,就有s(x)≥s((a)特征规则是形如{p}→{q1,92,n}的规则,中规则的前件只有一个项。一个大小为k的项集能够产生k个特征规则。令5是由给定项集产生的所有特征规则的最小置信度: 5((Pl,P2,",]) min[c({ ) {P2,,",: ) "",(( ) {P1,,"",Pk-1)I5是单调的、反单调的或非单调的?(b)区分规则是形如{p1,P2,n}→{q}的规则,其中规则的后件只有一个项。一个大小为k的项集能够产生k个区分规则。令n是由给定项集产生的所有区分规则的最小置信度: n((P1,P2,Pk min[c((P2. P3,"( )," c(,", Pk-1}-{)]是单调的、反单调的或非单调的?(c)将最小值函数改为最大值函数,重做(a)和(b)的分析。5.证明公式(6-3)。(提示:首先,计算创建形成规则左部项集的方法数;然后,对每个选定为规则左部的k项集,计算选择剩下的dk个项形成规则右部的方法数。)6.考虑表6-23中显示的购物篮事务。表6-23购物篮事务事务ID购买项牛奶,啤酒,尿布面包,黄油,牛奶123456789牛奶,尿布,饼干}{面包,黄油,饼干}{啤酒,饼干,尿布}牛奶,尿布,面包,黄油{面包,黄油,尿布}啤酒,尿布牛奶,尿布,面包,黄油}10啤酒,饼干(a)从这些数据中,能够提取出的关联规则的最大数量是多少(包括零支持度的规则)?(b)能够提取的频繁项集的最大长度是多少(假定最小支持度>0)?(c)写出从该数据集中能够提取的3项集的最大数量的表达式。(d)找出一个具有最大支持度的项集(长度为2或更大)(e)找出一对项a和b,使得规则{a}→{b和{b}→{a}具有相同的置信度。7.考虑下面的频繁3项集的集合:{1,2,3},{1,2,4},{1,2,5},{1,3,4},(1,3,5},(2,3,4},{2,3,5},{3,4,5假定数据集中只有5个项。 POG(a)列出采用Fk-1×F1合并策略,由候选产生过程得到的所有候选4项集。
(b)列出由Apriori算法的候选产生过程得到的所有候选4项集。(c)列出Apriori算法候选剪枝步骤后剩下的所有候选4-项集。8. Apriori算法使用产生-计数的策略找出频繁项集。通过合并一对大小为k的频繁项集得到一个大小为k+1的候选项集(称作候选产生步骤)在候选项集剪枝步骤中,如果一个候选项集的任何一个子集是不频繁的,则该候选项集将被丢弃。假定将Apriori算法用于表6-24所示数据集,最小支持度为30%,即任何一个项集在少于3个事务中出现就被认为是非频繁的。表6-24购物篮事务的例子事务购买项 {a, b,d,e)123456789 {b, c,d) {a, b,d,e) {a,c,d, e} {b, c,d,e) (b,d,e){c,d} {a, b, c] {a,d,e)10 {b,d)(a)画出表示表6-24所示数据集的项集格。用下面的字母标记格中每个结点。·N:如果该项集被 Apriori算法认为不是候选项集。一个项集不是候选项集有两种可能的原因:它没有在候选项集产生步骤产生,或它在候选项集产生步骤产生,但是由于它的一个子集是非频繁的而在候选项集剪枝步骤被丢掉。F:如果该候选项集被 Apriori算法认为是频繁的。I:如果经过支持度计数后,该候选项集被发现是非频繁的。(b)频繁项集的百分比是多少?(考虑格中所有的项集)(c)对于该数据集, Apriori算法的剪枝率是多少?(剪枝率定义为由于如下原因不认为是候选的项集所占的百分比:在候选项集产生时未被产生,或在候选剪枝步骤被丢掉。)(d)假警告率是多少?(假警告率是指经过支持度计算后被发现是非频繁的候选项集所占的百分比。)9. Apriori算法使用Hash树数据结构,有效地计算候选项集的支持度考虑图6-32所示的候选3项集的Hash树。1.4725836,91473,6,91.473692582,583.6,9 L1 L5 L6725118L9L12{145178168{246258{568346356{36727828937968914725.83.69678L23 L412715459457158)456 POG458)789图6-32Hash树结构的例子
(a)给定一个包含项{1,3,4,5,8}的事务,在寻找该事务的候选项集时,访问了Hash树的哪些叶结点?(b)使用(a)中访问的叶结点确定事务{1,3,4,5,8}包含的候选项集。10.考虑下面的候选3项集的集合:{1,2,},{1,2,6),{1,3,4},{2,3,4},{2,4,5},{3,4,6},{4,5,6}(a)构造以上候选3-项集的Hash树。假定Hash树使用这样一个Hash函数:所有的奇数项都被散列到结点的左子女,所有偶数项被散列到右子女。一个候选k项集按如下方法插入到Hash树中:散列候选项集中的每个相继项,然后再按照散列值到相应的分支。一旦到达叶结点,候选项集将按照下面的条件插入。条件1:如果该叶结点的深度等于k(假定根结点的深度为0),则不管该结点已经存储了多少个项集,将该候选插入该结点。条件2:如果该叶结点的深度小于k,则只要该结点存储的项集数不超过 maxsize,就把它插入到该叶结点。这里,假定 maxsize为2条件3:如果该叶结点的深度小于k且该结点已存储的项集数量等于 maxsize,则这个叶结点转变为内部结点,并创建新的叶结点作为老的叶结点的子女。先前老叶结点中存放的候选项集按照散列值分布到其子女中新的候选项集也按照散列值存储到相应的叶结点。(b)候选Hash树中共有多少个叶结点、多少个内部结点?(c)考虑一个包含项集{1,2,3,5,6}的事务使用(a)所创建的Hash树,则该事务要检查哪些叶结点?该事务包含哪些候选3项集?11.给定图6-33所示的格结构和表6-24给定的事务,用如下字母标记每一个结点。M:如果结点是极大频繁项集。C:如果结点是闭频繁项集。N:如果结点是频繁的,但既不是极大的也不是闭的。:如果结点是非频繁的。 (abac )ad)) )de (ade)((bce)(bde)cde (abcdCabce )abdeacdebode abcde POG图6-33项集的格
假定支持度阈值等于30%。12.传统的关联规则挖掘方法使用支持度和置信度度量来剪裁没有兴趣的规则。(a)使用表6-25中的事务数据,绘制出下面每个规则对应的相依表。规则:{b}→{c},{a}→{d},{b}→{d},{e}→{c},{c}→{a}表6-25购物篮事务示例事务I购买项1 {a,b, d, e) (b,c,d) la,b,d,e) {a,c, d, e) {b,c,d,e) {b,d,e} {c,d)89 [a,b,c} {a,d,e]10 {b,d)(b)利用(a)的相依表,按照下面的度量计算并依递减序确定规则的秩。i.支持度。i.置信度。 P(X,Y) iii. Interest(X-Y)(X)P(). iv. IS(X-)= P(X)P()'v. Klosgen(xy=p(x,y(p(yx)-p(y)),其中P()=(xY P(X) P(X, Y)P(,)几率(xyyp(x,)(13.给定习题12中得到的秩,计算置信度的秩与其他五种度量之间的相关性。哪种度量与置信度相关性最强?哪种最弱?14.使用图6-34中所显示的数据集回答下列问题注意,每个数据集包括1000个项和1000个事务。图中黑色单元表示项在事务中出现,白色表示不出现。假定使用 Apriori算法提取频繁项集,最小支持度为10%(即项集至少要包含在1000个事务中)(a)哪些数据集产生的频繁项集数量最多?(b)哪些数据集产生的频繁项集数量最少?(c)哪些数据集产生最长的频繁项集?(d)哪些数据集产生具有最大支持度的频繁项集?(e)哪些数据集产生的频繁项集包含更广泛支持度(即所包含项的支持度由小于20%到大于70%)的项?
项项2000■2000■40004000次6000■600080008000200400600800200400600800(a)(b)项项■■2000200040004000冷■6000600080008000■200400600800200400600800(c)(d)项项20002000400010%为14000冷-600090%为0均匀分布600080008000200400600800200400600800 (e)(图6-34习题14的图15.(a)证明:当且仅当fi=fi+=f+1时,数等于1(b)证明:如果A和B是相互独立的,则P(A,B)P(A,B)=P(A,B)P(A,B)(c)说明:ule的Q和系数是几率的规范化版本 Lfufoo+ fof or=- fo fo√ff+ff PDG(d)假定变量是统计独立的,写出表6-11和表6-12中所列出的各种度量值的简化表达式。
16.对于关联规则A→B,考虑兴趣度度量M=P(BA)-P(B)-p(b)(a)该度量的取值范围是什么?什么时候取最大值和最小值?(b)当P(A,B)增加,P(A)和P(B)保持不变时,M如何变化?(c)当P(A)增加,P(A,B)和P(B)保持不变时,M如何变化?(d)当P(B)增加,P(A,B)和P(A)保持不变时,M如何变化?(e)该度量在变量置换下对称吗?()若A和B是统计独立的,该度量的值是多少?(g)该度量是零加不变的吗?(h)在行或列缩放操作下,该度量保持不变吗?()在反演操作下,该度量如何变化?17.假定有一个购物篮数据集,包含100个事务和20个项。假设项a的支持度为25%,项b的支持度为90%,且项集{a,b}的支持度为20%。令最小支持度阈值和最小置信度阈值分别为10%和60%(a)计算关联规则{a}→{b}的置信度。根据置信度度量,这条规则是有趣的吗?(b)计算关联模式{a,b}的兴趣度度量。根据兴趣度度量,描述项a和项b之间联系的特点。(c)由(a)和(b)的结果,能得出什么结论?(d)证明:如果规则{a}→{b}的置信度小于{b}的支持度,则ic(a→{b)>c({a}→{biic(a}→{b})>s(b})。其中,c()表示规则置信度,s()表示项集的支持度。18.表6-26显示了二元变量A和B在控制变量C的不同值上的2×22的相依表。表6-26一个相依表101015 C=0B01530 C=1500015(a)分别计算当C=0,C=1和C=0或1时A和B的系数。注意:({A,B)= P(A,)-P(A)P(B)P(A)P(B)(1-P()(1-P(B))(b)由上面的结果可以得出什么结论?所19.考虑表6-27中显示的相依表。 PDG
表6-27习题19的相依表 BB BBA91891A189A19(a)表Ib)表Ⅱ(a)对于表,计算关联模式{A,B}的支持度、兴趣度和相关系数,并计算规则A→B和B→A的置信度。(b)对于表Ⅱ,计算关联模式(A,B}的支持度、兴趣度和相关系数,并计算规则A→B和B→A的置信度。(c)由(a)和(b)的结果可以得出什么结论?20.考虑表6-19和表6-20中显示的购买高清晰度电视和购买健身器的顾客之间的联系。(a)计算两个表的几率。(b)计算两个表的数。(c)计算两个表的兴趣因子。对于上述每一个度量,描述当汇总数据取代分层数据后,关联方向的变化。
家
第章关联分析:高级概念上一章介绍的关联规则挖掘假定输入数据由称作项的二元属性组成。还假定项在事务中出现比不出现更重要。这样,项被看作非对称的二元属性,并且只有频繁模式才被认为是有趣的。本章将这种表示扩展到具有对称二元属性、分类属性和连续属性的数据集。这种表示还将进一步扩充到包含诸如序列和图形的更复杂的实体。尽管关联分析算法的总体结构保持不变,但是算法的某些方面必须加以修改,以便处理非传统的实体。7.1处理分类属性许多应用包含对称二元属性和标称属性。表7-1显示的因特网调查数据包含对称二元属性,如性别、家庭计算机、网上聊天、网上购物和关注隐私;还包括标称属性,如文化程度和州。使用关联分析,我们可能发现关于因特网用户特征的有趣信息,如网上购物=是}→{关注隐私=是}这条规则暗示大部分在网上购物的因特网用户都关心个人隐私。表7-1具有分类属性的因特网调查数据性别文化程度州家庭计算机网上聊天网上购物关注隐私研究生伊利诺伊大学加利福尼亚女男男女女男男男女研究生密歇根大学弗吉尼亚研究生加利福尼亚大学明尼苏达大学阿拉斯加是否是否是是是是否是否是否否是是否是高中俄勒冈是否是是否是是否否…是否是是是是否否否…研究生得克萨斯…为了提取这样的模式,首先将分类属性和对称二元属性转换成“项”,目的是使用已有的关联规则挖掘算法。这种类型的变换可以通过为每个不同的属性值对创建一个新的项来实现。例如,标称属性文化程度可以用三个二元项取代文化程度=大学,文化程度=研究生,文化程度=高中。类似地,对称二元属性性别可以转换成一对二元项:男和女。表7-2显示因特网调查数据二元化后的结果。
表7-2二元化分类属性和对称二元属性后的因特网调查数据男女文化程度=研究生文化程度=大学关注隐私=是关注隐私=否001100111010011000110101010001010101100101111000110000将关联分析用于二元化后的数据时,需要考虑如下问题。(1)有些属性值可能不够频繁,不能成为频繁模式的一部分。对于具有许多可能属性值的标称属性(如州名),这个问题更为明显。降低支持度阈值不起作用,因为发现的频繁模式(许多可能是不真实的)将以指数增长,计算开销更高。更实际的做法是,将相关的属性值分组,形成少数类别。例如,每个州名都可以用对应的地理区域如中西部、太平洋西北部、西南部和东海岸取代。另一种可能性是,将不太频繁的属性值聚合成一个称作其他的类别,如图7-1所示。弗吉尼亚其他俄亥俄纽约伊利诺伊密歇根佛罗里达加利福尼亚明尼苏达马萨诸塞俄勒冈萨斯图7-1具有合并的“其他”类别的饼图(2)某些属性值的频率可能比其他属性高很多。例如,假定85%的被调查人都有家庭计算机。如果为每个频繁出现在数据中的属性值创建一个二元项,我们可能产生许多冗余模式,如下面的例子所示:{家庭计算机=是,网上购物=是}→{关注隐私=是}该规则是冗余的,因为它被归入本节开始给出的更一般的规则。由于高频繁度的项对应于属性的典型值,因此它们很少携带有助于更好地理解模式的新信息。因此,在使用标准的关联分析算法之前,删除这样的项可能是有好处的。另一种可能的做法是,使用68节提供的技术,处理具有宽支持度值域的数据集。(3)尽管每个事务的宽度与原始数据中属性的个数相同,但是计算时间可能增加,特别是当新创建的项变成频繁项时。这是因为需要更多时间处理由这些项产生的候选项集(见习题1)。
减少计算时间的一种方法是,避免产生包含多个来自同一属性的项的候选项集。例如,我们不必产生诸如{州=X,州=Y,…}的候选项集,因为该项集的支持度计数为零7.2处理连续属性上一节介绍的因特网调查数据可能还包含连续属性,如表7-3所示。挖掘连续属性可能揭示数据的内在联系,如“年收入超过$120K的用户属于45~60年龄组”,或“拥有超过3个e-mail账号并且每周上网超过15小时的用户通常关注个人隐私”包含连续属性的关联规则通常称作量化关联规则( quantitative association rule)表7-3具有连续属性的因特网调查数据性别年龄年收入每周上网小时数e-mail账号数关注隐私2690K2051135K10女男男女女男男男女2980K10…45120K153195K202555K2537100K10423355121是否是是是是否否否4165K82685K12本节介绍对连续数据进行关联分析的各种方法。具体地说,我们讨论三类方法:(1)基于离散化的方法,(2)基于统计学的方法,(3)离散化方法。使用这些方法导出的量化关联规则本质上差别很大。7.2.1基于离散化的方法离散化是处理连续属性最常用的方法。这种方法将连续属性的邻近值分组,形成有限个区间。例如,年龄属性可以划分成如下区间:年龄∈[12,16),年龄∈[16,20),年龄∈[20,24),…,年龄∈[56,60)其中,[a,b)代表包含a但不包含b的区间离散化可以使用2.3.6节介绍的任意技术(等区间宽度、等频率、基于熵或聚类)实现离散的区间可以映射到非对称的二元属性,使得可以使用已有的关联分析算法。表7-4显示离散化和二元化后的因特网调查数据。属性离散化的一个关键参数是用于划分每个属性的区间个数。通常,这个参数由用户提供,用区间宽度(对于等区间宽度方法)、每个区间的平均事务个数(对于等频率方法)或所希望的聚类数(对于基于聚类的方法)来表示。确定正确的区间数的困难性可以用表7-5中的数据解释。该表汇总参加调查的250个用户的回答数据中隐含两个强规则。R1:年龄∈[16,24)→网上聊天=是(s=8.8%,c=81.5%)R2:年龄∈[44,60)→网上聊天=否(s=16.8%,c=70%) PDG
表7-4二元化分类属性和连续属性后的因特网调查数据男女年龄<13年龄∈[13,21)年龄∈[21,30)关注隐私=是关注隐私=否0110010001011000000001010010100000000001000000011101…这些规则暗示16~24岁年龄组的大部分用户通常参加网上聊天,而44~60岁的多半不会参加网上聊天。在这个例子中,我们认为某个规则是有趣的,仅当它的支持度(s)超过5%,并且它的置信度(c)超过65%。当我们对年龄属性离散化时,遇到的问题之一是如何确定区间宽度。表7-5根据参加网上聊天的因特网用户的年龄组划分因特网用户年龄组网上聊天=是网上聊天=否[12,16)12[16,20)11[20,24)11323[24,28)1213[28,32)141232,36)1512[36,40)161440,44)1614[44,48)410[48,52)[52,56)5541110[56,60)11(1)如果区间太宽,则可能因为缺乏置信度而丢失某些模式。例如,当区间宽度为24岁时,R1和R2被如下规则所取代。R1:年龄∈[12,36)→网上聊天=是(s=30%,c=57.7%)R2:年龄∈[36,60)→网上聊天=否(s=28%,c=58.3%)尽管它们有较高的支持度,但是较宽的区间导致两个规则的置信度都低于最小置信度阈值。其结果是,离散化之后,两个模式都失去了。(2)如果区间太窄,则可能因为缺乏支持度而丢失某些模式。例如,如果区间宽度为4岁,则R被分裂成如下两个子规则。R:年龄∈[16,20)→网上聊天=是(s=4.4%,c=84.6%)R2:年龄∈[20,24)→网上聊天=是(s=4.4%,c=78.6%) PDG由于两个子规则的支持度都低于最小支持度阈值,离散化后R1丢失了。同理,规则R2被分
裂成4个子规则,也因4个子规则的支持度都低于最小支持度阈值而丢失。(3)如果区间宽度是8岁,则规则R2被分裂成如下两个子规则。R:年龄∈[44,52)→网上聊天=否(s=8.4%,c=70%)R:年龄∈[52,60)→网上聊天=否(s=8.4%,c=70%)由于R和R都有足够的支持度和置信度,R2可以通过聚合两个子规则而恢复与此同时,R1被分裂成如下两个子规则。R:年龄∈[12,20)→网上聊天=是(s=9.2%,c=60.5%)R:年龄∈[20,28)→网上聊天=是(s=9.2%,c=60.0%)不像R2,我们不能通过聚合这两个子规则来恢复R1,因为两个子规则的置信度都低于阈值。处理这些问题的一个方法是,考虑邻近区间的每种可能的分组例如,我们可以以宽度4岁开始,将近邻的区间合并成较宽的区间,年龄∈[12,16),年龄∈[12,20),…,年龄∈[12,60),年龄∈[16,20),年龄∈[16,24)等等。这种方法能够检测出R1和R2是强规则。然而,这也导致如下计算问题。(1)计算开销非常大。如果值域被划分成k个区间,则必须创建k(k-1)/2个二元项来代表所有可能的区间。此外,如果对应于区间[a,b)的项是频繁的,则包含[a,b)的区间对应的所有项也必然是频繁的。因此,这种方法可能产生过多的候选和频繁项集。为了处理这些问题,可以使用最大支持度阈值,防止创建对应于非常宽的区间的项,并减少项集的数量。(2)提取许多冗余规则。例如,考虑下面的规则对:R3:{年龄∈[16,20),性别=男}→{网上聊天=是}R4:{年龄∈[16,24),性别=男}→{网上聊天=是}R4是R3的泛化(R3是R4的特化),因为对于年龄属性,R4有更宽的区间。如果两个规则的置信度值相同,则R4应当更有趣,因为它涵盖了更多的例子一包括R3涵盖的那些。因此,R3是冗余的。7.2.2基于统计学的方法量化关联规则可以用来推断总体的统计性质。例如,假定我们希望根据表7-1和表7-3提供的数据,找出因特网用户特定组群的平均年龄。使用本节介绍的基于统计学的方法,可以提取如下形式的量化关联规则:{年收入>$100K,网上购物=是}→年龄:均值=38该规则表明年收入超过$100K并且定期在网上购物的因特网用户的平均年龄为38岁。1.规则产生为了产生基于统计学的量化关联规则,必须指定用于刻画有趣总体段特性的目标属性。保留目标属性,使用上一节介绍的方法对数据中的其余分类属性和连续属性二元化然后,可以使用已有的算法,如 Apriori算法或FP增长,从二元化数据中提取频繁项集。每个频繁项集确定一个有趣总体段。使用诸如均值、中位数、方差或绝对偏差等统计量,可以对目标属性在每个段内的
分布进行汇总。例如,前面的规则就是通过对支持频繁项集{年收入>$100K,网上购物=是}的因特网用户的年龄求平均值得到的。使用这个方法得到的量化关联规则的数量与提取的频繁项集相同。由于量化关联规则的定义方法,对于这种规则,不能使用置信度。确认关联规则的可选方法在下面给出。2.规则确认某个量化关联规则是有趣的,仅当由规则覆盖的事务计算的统计量不同于由未被规则覆盖的事务计算的统计量。例如,本节开始给出的规则是有趣的,仅当不支持频繁集{年收入>$100K,网上购物=是}的因特网用户的平均年龄显著地大于或小于38岁。为了确定该平均年龄差是否具有统计意义,应当使用统计假设检验方法进行检验。考虑量化关联规则A→t:从,其中A是频繁项集,t是连续的目标属性,而u是被A覆盖的事务的t的平均值。此外,设u是未被A覆盖的事务的t的平均值。目标是检验和之间的差是否大于用户指定的某个阈值△。在统计假设检验中,给定两个相反的假设分别称作原假设(null hypothesis)和备择假设(alternative hypothesis)根据从数据收集的证据,进行假设检验,确定两个假设中的哪一个被接受。在这种情况下,假定<μ',则原假设是Ho:u=u+△,而备择假设是H1:>μ+△。为了确定应当接受哪个假设,计算下面的Z统计量:z=--△(7-1)22+2+ n n2其中,n1是支持A的事务个数,n2是不支持A的事务个数,s1是支持A的事务的t的标准差,而2是不支持A的事务的t的标准差。在原假设下,具有标准正态分布,均值为0,方差为1然后,将使用公式(7-1)计算的Z值与临界值Z比较,其中Z是依赖于期望置信水平的阈值。如果Z>Za,则原假设被拒绝,并且我们可以断言该量化关联规则是有趣的。否则,数据中没有足够的证据证明均值之差具有统计意义。例7.1考虑量化关联规则{收入>100K,网上购物=是}→年龄:μ=38假定有50个因特网用户支持该规则的前件。他们年龄的标准差是3.5。另一方面,不支持该规则前件的200个用户的平均年龄是30,标准差是6.5。假定量化关联规则是有趣的,仅当与μ之间的差大于5岁。使用公式(7-1),我们得到=38-30-5==4.44143.526.5250200对于一个置信水平为95%的单侧假设检验,拒绝原假设的临界值是1.64由于Z>1.64,原假设被拒绝。因此,我们断言该量化关联规则是有趣的,因为支持和不支持规则前件的用户的平均年龄之差大于5岁。口 PDG
7.2.3非离散化方法有一些应用,令分析者更感兴趣的是发现连续属性之间的关联,而不是连续属性的离散区间之间的关联。例如,考虑如下问题:找出表7-所示文本文档中词的关联。文档词矩阵中的每个表值代表词在给定文档中出现的规范化频率。用每个词的频率除以所有文档词频之和对数据进行规范化。这种规范化的理由之一是确保所得到的支持度值是0和1之间的数。然而,更重要的理由是确保数据在相同的尺度上,以相同方式变化的词的集合可以具有相似的支持度值。表7-6规范化的文档词矩阵 word word00 word4 words word文山山小0.30.60.20.10.20.20.40.20.70.20.2000.3000010000100000.100.00.3在文本挖掘中,分析者更感兴趣的是发现词(例如,数据和挖掘)之间的关联,而不是词频区间(例如,数据∈[1,4],挖掘∈[2,3])之间的关联。一种做法是,将数据变换成0/1矩阵;其中,如果规范化词频超过某个阈值t,则值为1,否则为0。尽管该方法使得分析者可以对二元化数据集使用已有的频繁模式产生算法,但是为二元化找到合适的阈值却很棘手。如果阈值设得太大,则可能丢失有趣的关联。反之,如果阈值设得太小,则可能产生大量谬误的关联。本节提供另一种发现词关联的方法,称作min- -Apriorio类似于传统的关联分析,项集是词的汇集,而其支持度用来度量词之间的关联程度。项集的支持度可以根据对应词的规范化频率计算。例如,考虑表7-6中的文档d1词word1和word2的规范化频率分别为0.3和0.6。有人可能认为,计算这两个词之间关联的一个合理的方法是取它们的规范化频率的平均值,即(0.3+0.6)/2=0.45。然后,对所有文档的平均规范化频率求和,就可以计算项集的支持度:s(word1,word2})=0.3+0.60.1+0.20.4+0.20.2+0=12222这个结果一点也不意外。因为每个词频都规范化到1,对规范化频率取平均值使得每个项集的支持度等于1这样,使用该方法,所有的项集都是频繁的。该方法对于识别有趣的模式亳无用处。在min -Apriori-中,给定文档中词之间的关联通过取它们的规范化频率的最小值得到,即min(word,word2)=min(0.3,0.6)=0.3.项集的支持度通过在所有文档上聚集它的支持度得到。s(word,word2})=min(0.3,0.6)+min(0.,0.2)+min(0.4,0.2)+min(0.2,0)=0.6min -Apriori-中定义的支持度具有如下期望性质,使它适合用来发现文档中词的关联。(1)支持度随词的规范化频率增加而单调递增。(2)支持度随包含该词的文档个数增加而单调递增。(3)支持度具有反单调性。例如,考虑一对项集{A,B}和{A,B,C}由于min({a,b}≥min({AB,C}),从而S({A,B})≥s({A,B,C})。因此支持度随项集中词数的增加而单调递减。使用新的支持度定义,可以修改标准 Apriori算法,来发现词之间的关联。 PDG
7.3处理概念分层概念分层是定义在一个特定的域中的各种实体或概念的多层组织。例如,在购物篮分析中,概念分层具有如下形式:项的分类法描述商店销售的商品之间的“is-a”联系例如,牛奶是一种食品,而DVD是一种电子设备(见图7-)。通常,概念分层根据领域知识,或者基于特定组织的标准分类方案来定义(例如,国会图书馆的分类方案用来根据主题组织图书资料)。食品电子产品面包计算机牛奶家电便携机配件普通面包白面包脱脂牛奶2%台式机便携机电视DVDAC适配器底座图7-2商品分类的例子概念分层可以用有向无环图(directed acyclic graph)表示,如图7-2所示如果图7-2中存在一条从结点p到另一个结点c的边,则称p是c的父母,c是p的子女。例如,牛奶是脱脂牛奶的父母,因为从结点牛奶到结点脱脂牛奶存在一条有向边。文称作X的祖先(X是的后代),如果有向图中存在一条从到X的路径。在图7-2中食品是脱脂牛奶的祖先,而AC适配器是电子产品的后代。将概念分层纳入关联分析的主要优点如下。(1)位于层次结构较下层的项可能没有足够的支持度,从而不在任何频繁项集中出现。例如,尽管AC适配器和底座的销售量可能很低,但是,作为概念分层结构中它们的父母结点,便携机配件的销售量可能很高。不使用概念分层就可能丢失涉及便携机配件的有趣模式。(2)在概念分层的较低层发现的规则过于特殊,可能不如较高层的规则令人感兴趣。例如,诸如牛奶和面包等大宗商品趋向于产生许多低层规则,如,脱脂牛奶→普通面包,2%牛奶→普通面包,脱脂牛奶白面包。使用概念分层结构,它们可以汇总为一条规则:牛奶→面包。仅考虑分层结构顶部的商品可能也不好,因为这样的规则可能没有任何实际应用价值。例如,尽管规则电子产品一食品可能满足支持度和置信度阈值,但是它并不提供什么信息,因为顾客经常一起购买电子产品和食品是已知的事实。如果牛奶和电池才是经常同时销售的商品,则模式{食品,电子产品}可能过分泛化了这种情况。可以用以下方法扩充标准的关联分析,使其包括概念分层初始,每个事务t用它的扩展事务(extended transaction)t取代,其中,t包含t中所有项和它们的对应祖先。例如,事务{DVD普通面包}可以扩展为{DVD,普通面包,家电,电子产品,面包,食品},其中,家电和电子产品是DVD的祖先,而面包和食品是普通面包的祖先。使用这种方法,可以对扩展的数据库使用
诸如 Apriori等已有的算法来发现跨越多个概念层的规则。这种方法有一些明显的局限性。(1)处于较高层的项比处于较低层的项趋向于具有较高的支持度计数。这样,如果支持度阈值设得太高,则只能提取涉及较高层项的模式另一方面,如果阈值设得太低,则算法可能产生太多模式(其中大部分可能是不真实的),使得计算效率极低。(2)概念分层的引入增加了关联分析的计算时间,因为项的个数更多,事务宽度更大。算法产生的候选模式和频繁模式的个数可能随事务变宽而指数增加。(3)概念分层的引入可能产生余规则。规则→是冗余的,如果存在一个更一般的规则文→,其中文是X的祖先,是Y的祖先,并且两个规则具有非常相似的置信度。例如,假定{面包}→{牛奶},{白面包}→{2%牛奶}{白面包}→{脱脂牛奶}和{普通面包}→{脱脂牛奶}具有非常相似的置信度。涉及较低层中项的规则是冗余的,因为它们可以被涉及其祖先的规则所,,概括。诸如{脱脂牛奶,牛奶,食品}的项集也是冗余的,因为食品和牛奶都是脱脂牛奶的祖先。幸而,给定分层结构,在频繁模式产生时容易删除这类冗余项集。7.4序列模式购物篮数据常常包含关于商品何时被顾客购买的时间信息。可以使用这种信息,将顾客在一段时间内的购物拼接成事务序列。同样,从管理科学实验或对诸如通信网络、计算机网络和无线遥感网络等的物理系统中收集的基于事件的数据都具有固有的序列特征。也就是说在这种数据代表的事件之间存在某种序关系,通常基于时间或空间的先后次序。然而,迄今为止所讨论的关联模式概念都只强调同时出现关系,而忽略数据中的序列信息。对于识别动态系统的重现特征,或预测特定事件的未来发生,序列信息可能是非常有价值的。本节给出序列模式的基本概念和发现序列模式的算法。7.4.1问题描述发现序列模式的问题输入是一个序列数据集,如图7-3左部所示。每一行记录与一个特定的对象相关联的一些事件在给定时刻的出现。例如,第一行包含在时间戳t=10时出现的对象A的事件集。将与对象A有关的所有事件按时间戳增序排序,就得到对象A的一个序列(sequence),如图7-3右部所示。时间线110152025303序列数据库对象时间事件对象AA102,3,5的序列A206,1A231114.56BBBC172对象B27,81,22816的序列141.8.76对象C的序列 POG图7-3一个序列数据库的例子
一般地,序列是元素(element)的有序列表,可以记作s=ee2een)其中每个e是一个或多个事件的集族,即e={i,i2,i}下面是一些序列的例子。Web站点访问者访问的web页面序列:主页电子产品}照相机和摄像机}数码像机}购物车订购确认返回购物}〉导致三里岛核事故的事件序列:树脂堵塞出口阀关闭失去给水}冷凝器出口阀关闭增压泵跳闸主水泵跳闸主涡轮机跳闸反应堆压力上升》计算机科学主修课程序列:算法与数据结构,操作系统引论}{数据库系统计算机体系结构计算机网络,软件工程}计算机图形学,并行程序设计}序列可以用它的长度和出现事件的个数刻画。序列的长度对应于出现在序列中的元素个数,而k-序列是包含k个事件的序列。上面例子中的Web序列包含7个元素和7个事件;三里岛事件序列包含8个元素和8个事件;而课程序列包含4个元素和8个事件。图7-4提供了一些应用领域定义的序列、元素和事件的例子。除最后一行外,与前三个领域相关的序数属性对应于日历时间。对于最后一行,序数属性对应于基(A、C、G、T)在基因序列中的位置。尽管关于序列模式的讨论主要考虑时间事件,但是可以将它推广到事件具有空间次序的情况。序列数据库序列元素(事务)事件(项)顾客给定顾客的购物历史顾客在时刻t购买的商品的集合书、日常用品、CD等Web数据特定Web访问者的浏览活动一次鼠标点击后Web访问者观看主页索引页、的文件的集合联系信息等事件数据给定的传感器产生的事件历史传感器在时刻t触发的事件传感器产生的警报类型基因组序列一个特定物种的DNA序列DNA序列的元素基A、T、G、C元素(事务)事件(项) /E1E1 E2 E3 E2E3 E2E4序列序数属性图7-4序列数据集中元素和事件的例子子序列序列t是另一个序列s的子序列( subsequence),如果t中每个有序元素都是s中一个有序元素的子集。形式化表述为,序列t={ttm)序列s=(s12sn)的子序列,如果存在整数1≤j<j2<…<jm≤n,使得,2i2,,如果t是s的子序列,则称t包含在s中下表的例子解释了子序列的概念。序列s序列tt是s的子序列吗? PDG2,4}(3,5,6(82}(3,6}18}是
(续)序列s序列tt是s的子序列吗?2,4}{3,5,6}8}2}{8}是1,2}3,41}2}否2,4}{2,4){2,5}2}4是7.4.2序列模式发现设D是包含一个或多个数据序列( data sequence)的数据集。术语数据序列是指与单个数据对象相关联的事件的有序列表。例如,图7-3中显示的数据集包含三个数据序列,对象A、B和C各一个。序列s的支持度是包含s的所有数据序列所占的比例。如果序列s的支持度大于或等于用户指定的阈值 minsup,则称s是一个序列模式(或频繁序列)定义7.1序列模式发现给定序列数据集D和用户指定的最小支持度值minsup,序列模式发现的任务是找出支持度大于或等于 minsup的所有序列。图7-5给出了一个包含5个数据序列的数据集的例子。序列{1}2}的支持度等于80%,因为它出现在5个数据序列的4个中(除D之外的每个对象)。假定最小支持度阈值是50%,则至少出现在3个数据序列中的任何序列都被视为序列模式。从给定的数据集中提取的序列模式的例子包括({1}2},1,2}),《{2,3}),《1,2}2,3}等。对象时间戳事件1,2,4 Minsup= 50%AAABBCC2,35序列模式的例子:1,22,3,4<{1,2}>s=60%2123123121,2<{23s=60%2,3,4<24>=80%2,4,5<{3}{5}>s=80%s=80%CDDDEE2<1222>S=60%3,4<{1}{23>s=60%45<{2}2,3}s=60%1,3<{1,2}2,3s=602,4,5图7-5由包含5个数据序列的数据集导出的序列模式序列模式的发现是一项具有挑战性的计算任务,因为在给定的数据序列中的序列有指数多个。例如,数据序列({a,b}c,d,e}g,h,i}包含的序列有{ac,dg)c,d,e}),bg}等。容易证明,出现在具有n个事件的数据序列中的序列总数为C因此,具有9个事件的数据序列包含C+c2+…+c=2-1=511
个不同的序列。产生序列模式的一种蛮力方法是枚举所有可能的序列,并统计它们各自的支持度。给定n个事件的集族,首先产生候选1-序列,然后是候选2序列,候选3序列,等等。1-序列:<i>,<i2>…,<in2-序列:<{i,i2}>,<{i,i}>,<{in-,in}>,<ii><ii><in-in3-序列:<{i,2,i3}>,<{i,i2,i4}>,,<{i1,2}i}>, <{ i1, i2)>,, < ) 1)>,, <inHinHin>注意,候选序列的个数比候选项集的个数大得多。产生更多候选的原因有下面两个(1)一个项在项集中最多出现一次,但一个事件可以在序列中出现多次。给定两个项i和2只能产生一个候选2项集{i1,i2},但却可以产生许多候选2-序列,如({i,i2},{i}i2}),ii}和{i,i}。(2)次序在序列中是重要的,但在项集中不重要。例如,{1,2}和{2,1}表示同一个项集,而ii2}和{}{i}对应于不同的序列,因此必须分别产生。先验原理对序列数据成立,因为包含特定k-序列的任何数据序列必然包含该k序列的所有(k-1)-子序列。可以开发类Apriori算法,从序列数据集中提取序列模式。算法的基本结构在算法7.1中给出。算法7.1序列模式发现的类算法1:k=12:F=i)/n minsup.{出所有的频繁1序列。 3: repeat4:k=k+1 5:= apriori-gen(Fk-1).{产生候选k序列。}6:for每个数据序列t∈Tdo7:C= subsequence(Ck,t).{识别包含在t中的所有候选。8:for每个候选k序列c∈Cdo9:o(c)=(c)+1支持度计数增值。 10: end for 11: end for12:Fk={ cE(c)n minsupl.{提取频繁k-序列。} 13: until=. 14: Answer=U Fk.注意,该算法的结构几乎与算法6.1完全一样。该算法将迭代地产生新的候选k序列,剪掉那些其(k-1)序列非频繁的候选,然后对留下的候选计数,识别序列模式。这些步骤的细节在下面给出。候选产生一对频繁(k-1)-序列合并,产生候选k序列。为了避免重复产生候选,传统的 Apriori算法仅当前k-1项相同时才合并一对频繁k项集。类似的方法可以用于序列。序列合并的原则在以下过程中给出。图7-6给出了一个例子,通过合并成对的频繁3序列得到候选序列。第一个候选(1234)通过合并({12}(3)和{2}{34})得到。由于事件3和事件4属于第二个序列的不同元素,它们在合并后序列中也属于不同的元素。另一方面,将153)与5H3,4}合并产生候选4序列
1}5}3,4})。在这种情况下,事件3和事件4属于第二个序列的相同元素,4被合并到第一个序列的最后一个元素中。最后,序列{1}2}{3}125}不必合并,因为去掉第一个序列的第一个事件与去掉第二个序列的最后一个事件并不产生相同的子序列。尽管({1}25}3}是一个可行的候选,但是它是通过合并另外一对序列{1}2,5}和{2,5}3}产生的。该例表明序列合并过程是完备的;即,它不会丢失任何可行的候选,与此同时,它能避免产生重复的候选序列。频繁3序列<(1)(2)(3)><(1)(25)>候选产生<(1)(5)(3)><(2)(3)(4)><(1)(2)(3)(4)><(25)(3)><(1)(25)(3)>候选剪枝<(3)(4)(5)><(1)(5)(34)><(5)(34)><(2)(3)(4)(5)><(25)(34)><(1)(25)(3)>图7-6序列模式挖掘算法的候选产生和剪枝步骤的例子序列合并过程序列s(1)与另一个序列s(2合并,仅当从s中去掉第一个事件得到的子序列与从中去掉最后一个事件得到的子序列相同。结果候选是序列s)与s(2)的最后一个事件的连接。s2的最后一个事件可以作为最后一个事件合并到(1的最后一个元素中,也可以作为一个不同的元素,取决于如下条件。(1)如果s(2)的最后两个事件属于相同的元素,则s的最后一个事件在合并后的序列中是s(1)的最后一个元素的一部分。(2)如果s2的最后两个事件属于不同的元素,则s的最后一个事件在合并后的序列中成为连接到s1)的尾部的单独元素。候选剪枝如果候选k-序列的(k-1)-序列致少有一个是非频繁的,那么它将被剪掉。例如,假定{123}4}是一个候选4-序列。我们需要检查1}(24})和{13}4}是否是频繁3-序列。由于它们都不是频繁的,因此可以删除候选({1}2}34})读者可以验证,候选剪枝后,图7-6中剩下的唯一候选4序列是1}{2,5}3}支持度计数在支持度计数期间,算法将枚举属于特定数据序列的所有候选k序列。这些候选的支持度将增值。计数之后,算法将识别出频繁k序列,并可以丢弃其支持度计数小于最小支持度阈值 minsup的候选。7.4.3时限约束本节提出一种序列模式,其中模式的事件和元素都施加时限约束为了诱导对时限约束的需要,考虑如下被两个注册数据挖掘课程的学生选修的课程序列:学生A:统计学}数据库系统}数据挖掘}
学生B:《{数据库系统}{统计学}{数据挖掘}感兴趣的序列模式是{统计学,数据库系统}{据挖掘}》意思是说注册数据挖掘课程的学生必须先选修数据库系统和统计学方面的课程显然,该模式被这两个学生支持,尽管他们都没有同时选修统计学和数据库系统。相比之下不能认为某个10年之前选修了统计学课程的学生支持该模式,因为这些课程的时间间隔太长了。由于上一节提供的表示并未体现时限约束,因此需要定义新的序列模式。图7-7解释了可以施加在模式上的某些时限约束。这些约束的定义和它们对序列模式发现算法的影响将在下面讨论。注意,序列模式的每个元素都与一个时间窗口[]相关联,其中是该时间窗口内事件的最早发生时间,而u是该时间窗口内事件的最晚发生时间。 u(+1)-()<=maxgap()-u(s)>mingap窗口大小 wS2序列3245u(sn)-l(si)=maxspan每个元素的时间窗口(w)用[,u]刻画,其中l是w内事件的最早发生时间,u是w内事件的最晚发生时间图7-7序列模式的时限约束1.最大跨度约束最大跨度约束指定整个序列中所允许的事件的最晚和最早发生时间的最大时间差。例如,假定下面的数据序列包含的事件发生在相继的时间戳(1,2,3,…)假定最大时间跨度 maxspan=3,下面的表包含了给定的数据序列支持和不支持的序列模式。数据序列s序列模式ts支持t?1,3}3,4}4(5}6,78)341,3)3,4}4)5{6,7}8}3)6是是否1,3}{3,4}4(5}{6,7}8}1,3}6一般地, maxspan越长,在数据序列中检测到模式的可能性就能大。然而,较长的 maxspan也可能捕获不真实的模式,因为这增加了两个不相关的事件成为时间相关事件的可能性。此外,模式也可能涉及陈旧事件。最大跨度约束影响序列模式发现算法的支持度计数。如前面的例子所示,施加最大时间跨度约束之后,有些数据序列就不再支持候选模式。如果我们简单地使用算法7.1,则有些模式的支持度计数就可能过分估计。为了避免该问题,必须修改算法,忽略给定模式中事件的第一次和最后一次发生的时间间隔大于 maxspan的情况。2.最小间隔和最大间隔约束时限约束也可以通过限制序列中两个相继元素之间的时间差来指定。如果最大时间差
(maxgap)是一周,则元素中的事件必须在前一个元素的事件出现后的一周之内出现。如果最小时间差( mingap)是零,则元素中的事件必须在前一个元素的事件出现之后出现。假定 maxgap=3, mingap=1,下表给出了模式通过或未通过最大间隔和最小间隔约束的例子。数据序列s序列模式 maxgap mingap1,3}{3,4}4}5}6,783}(6通过通过1,3}{3,4}4}(5}6,7}{86(8通过未通过1,3}(3,4}(4}5}6,7}(8}1,3}6}未通过通过(1,3{3,4}4(5}671811}(3}(8}未通过未通过与最大跨度一样,这些约束也影响序列模式发现算法的支持度计数,因为当最小间隔和最大间隔约束存在时,有些数据序列就不再支持候选模式。必须修改算法,确保对模式进行支持度计数时不会违反时限约束。否则的话,可能将某些非频繁的序列误认为频繁序列。使用最大间隔约束的一个旁效是可能违反先验原理。为了解释这一点,考虑图7-5中的数据集。由于没有最小间隔或最大间隔约束,{25}和{23}5}的支持度都是60%然而,如果 mingap=0, maxgap=1,则{25}的支持度下降至40%,而({235)的支持度仍然是60%换句话说,当序列中的事件个数增加时,支持度增加了这与先验原理相违背。出现这种违背的原因是,事件2和事件5之间的时间间隔大于 maxgap,因而对象D不支持模式(25}使用邻接子序列的概念可以避免这一问题。定义7.2邻接子序列序列s是序列w=ee2e)的邻接子序列( contiguous subsequence),如果下列条件之一成立。(1)s是从e1或ek中删除一个事件后由w得到。(2)s是从至少包含两个事件的任意e∈w中删除一个事件后由w得到。(3)s是t的邻接子序列,而t是w的邻接子序列。下面的例子解释了邻接子序列概念:数据序列s序列模式tt是s的邻接子序列?1}2,3}1}{21,2}{2}(3}1){2}3,4}{1,2}{2,3){41}2}1}(3){212}是是否否1,2}{1}(3}(21(2}使用邻接子序列概念,可以用如下方法修改先验原理,来处理最大间隔约束。定义7.3修订的先验原理如果一个k序列是频繁的,则它的所有邻接(k-1)子序列也一定是频繁的。只需少量改动,就可以将修订的先验原理用于序列模式发现算法。在候选剪枝阶段,并非所有的k序列都需要检查,因为它们之中的一些可能违反最大间隔约束。例如,如果 maxgap=1,则不必检查候选({1}2,3}4H5}的子序列12,35})是否是频繁的,因为元素{2,3}和{5}之间的时间差大于一个时间单位。我们只需要考察(12,3H45})的邻接子序列,包括{12,
34),{2,3}45}),{1245})和{1}{3}{4}5})3.窗口大小约束最后,元素s中的事件不必同时出现。可以定义一个窗口大小阈值(ws)来指定序列模式的任意元素中事件最晚和最早出现之间的最大允许时间差窗口大小为0表明模式同一元素中的所有事件必须同时出现。下面的例子使用ws=2,确定数据序列是否支持给定的序列(假定 mingap=0, maxgap=3, maxspan∞数据序列s序列模式ts支持t?1,33,445)6,7)834)51,33,4)4)15)6,7)8)<4681,3}{3,4}(4}5){6,7{8}3,4,6){8}是是否否1,3){3,4}4}5(6,7}{8}1,3,46,7,8}在上一个例子中,尽管模式{1,3,4}{6,7,8}满足窗口大小约束,但是它违反最大间隔约束,因为两个元素中事件的最大时间差是5个时间单位。窗口大小约束也影响序列模式发现算法的支持度计数。如果直接使用算法7.1而不施加窗口大小约束,则某些候选模式的支持度计数可能过低估计,从而可能丢掉某些有趣的模式。7.4.4可选计数方案有一些方法可以用来由序列数据库对候选k序列的支持度计数为了解释,考虑序列pq的支持度计数问题,如图7-8所示。假定ws=, mingap=0, maxgap=1, maxspan=2对象的时线p pp序列:(p)(q) p pqqqqq(方法,计数)3456 COBJ 1 CWIN 6 CMINWIN 4 CDIST_ CDIST 5 POG图7-8比较不同的支持度计数方法
COBJ:每个对象出现一次。该方法在对象时线中查找给定序列的至少一次出现。在图7-8中,尽管序列{pq}在对象的时线中出现多次,但是只对它计数一次p出现在t=1,而q出现在t=3CWIN:每个滑动窗口出现一次。在该方法中,将一个固定长度(maxspan)的滑动时间窗口移过时线,一次移动一个时间单位。序列在滑动窗口中遇到一次,支持度计数就增值一次。在图7-8中,使用该方法,序列{pq})被观察到6次。 CMINWIN:最小出现窗口数。最小出现窗口是给定时限约束下序列出现的最小窗口。换言之,最小出现窗口是这样的时间间隔,使得序列在该时间间隔中出现,但不在其任何真子间隔中出现。该定义可以视为CWI的限制版本,因为其效果是收缩或坍塌被CWIN计数的某些窗口。例如,序列{pq}有4个最小出现窗口:(1)对(p:t=2,q:t=3),(2)对(p:t=3,q:t=4),(3)对(p:t=5,q:t=6,(4)对(p:t=6,qt=7)事件p在t=1且事件q在t=3的出现不是最小出现窗口,因为它包含一个更小的窗口(p:t=2,q:t=3),该子窗口才是最小出现窗口。 CDIST_O:允许事件时间戳重叠的不同出现( distinct occurrence)。序列的不同出现定义为事件时间戳对的集合,使得至少有一个新的事件时间戳对不同于以前统计过的出现。对这样的不同出现计数就产生了 CDIST方法。如果事件p和q的出现时间表示为元组(t(p),t(q),则该方法产生序列{p}q}的8个不同出现,分别在时间(1,3)、(2,3)、(2,4)、(3,4)、(3,5)、(5,6)、(5,7)和(6,7) CDIST:不允许事件时间戳重叠的不同出现。在上面的 CDIST中,允许序列的两次出现具有重叠的事件时间戳对,如(1,3)(2,3) CDIST方法不允许重叠当一个事件时间戳对在计数时用过之后,将它标记为已使用,并且在对相同的序列计数时不再使用。例如,在图7-8中,序列({p}9}的不同的、不重叠的出现有5次。这些出现的发生时间分别为(1,3)、(2,4)、(3,5)、(5,6)和(6,7)。可以看出,这些出现是 CDIST观察到的出现的子集。关于计数方法最后要说的是,需要确定计算支持度度量的基线。对于频繁项集挖掘,基线由事务总数给定。对于序列模式挖掘,基线依赖于计数方法。对于COBJ方法,可以用输入数据中对象的总数作为基线。对于CWIN和 CMINWIN方法,基线由所有对象中可能的时间窗口数之和给定。对于诸如 CDIST和 CDIST方法,基线由每个对象输入数据中出现的不同的时间戳个数之和确定。7.5子图模式本节将关联分析方法应用到远比项集和序列更加复杂实体。例子包括化学化合物、3-D蛋白质结构、网络拓扑和树结构的XML文档。这些实体可以用图形表示建模,如表7-7所示在这种类型的数据上进行数据挖掘的任务是,在图的集合中发现一组公共子结构。这样的任务称作频繁子图挖掘(frequent subgraph mining)。频繁子图挖掘的潜在应用可以在计算化学领域看到。每年,为了研制药物、农药、化肥等,都要构造新的化合物。尽管我们知道化合物的化学性质主要取决于其结构,但是建立它们之间的确切联系却很困难。通过识别与已知化合物的特定性质相关联的常见子结构,频繁子图挖掘可以为这项工作提供支持。这样的信息可以帮助科学家构造具有特定性质的新化学化合物。
表7-7不同应用领域中实体的图形表示应用图形顶点边Web挖掘Web浏览模式Web页面页面之间的超链接计算化学化学化合物的结构原子或离子原子或离子之间的键网络计算计算机网络计算机和服务器机器之间的互联语义WebXML文档的集合XML元素元素之间的父子联系生物信息学蛋白质结构氨基酸接触残基本节提供一些方法,将关联分析用于基于图的数据。首先,我们回顾一些与图有关的基本概念和定义;然后引入频繁子图挖掘问题;接下来介绍如何扩展传统的 Apriori算法来发现这些模式。7.5.1图与子图图是一种可以用来表示实体集之间联系的数据结构。从数学上讲,图由顶点集V和连接顶点对的边集E构成。每条边用顶点对(v,)表示,其中vvV可以给每个顶点v赋予一个标号(v),代表实体的名字。同理,每条边(v,v)也可以关联到一个标号l(v,,描述实体对之间的联系。表7-7显示了与不同类型的图相关联的顶点和边。例如,在一个Wb图中,顶点对应于Web页面,而边表示Web页面之间的超链接。定义7.4子图图G=(V,E是另一个图G=(VE)的子图,如果它的顶点集V是V的子集,并且它的边集E是E的子集。子图关系记作GsG图7-9显示了一个包含6个顶点和11条边的图,以及它的一个可能的子图。该子图显示在图7-9b中,只包含原图6个顶点中的4个,11条边中的4条。(a)标记的图(b)子图图7-9子图的例子定义7.5支持度给定图的集族,子图g的支持度定义为包含它的所有图所占的百分比,即s(g)= KG,8 cs,G, s}(7-2)s例7.2考虑5个图G1到G5,如图7-10所示右上角的图g1是G1、G3、G4和G的子图,因此s(g1)=4/5=80%。同理,我们有s(g2)=60,因为g2是G1、G2和G3的子图;而s(g3)=40因为83是G1和G3的子图。 PDG
e子图gada●e支持度=80%●c6子图g22a1dda支持度=60%ecb子图g3●eG5adC图数据集支持度=40%图7-10由图集计算子图的支持度7.5.2频繁子图挖掘本节给出频繁子图挖掘问题的定义,并解释该任务的复杂性。定义7.6频繁子图挖掘给定图的集合和支持 minsup度阈值,频繁子图挖掘的目标是找出所有使得s(g)≥ minsup的子图g尽管该定义适用于所有类型的图,但是本章主要关注无向连通图(undirected, connected graph)。这种图的定义如下。(1)一个图是连通的,如果图中每对顶点之间都存在一条路径;其中,路径是顶点的序列<V2V>,使得序列中每对相邻的顶点(v+)之间都有一条边(2)一个图是无向的,如果它只包含无向边。一条边(vi)是无向的,如果它与(vv无区别处理其他类型(有向的或不连通的)子图的方法留给读者作为习题(见本章习题15)挖掘频繁子图是一项计算量很大的任务,因为搜索空间是指数的。为了解释这项任务的复杂性,考虑包含d个实体的数据集。在频繁项集挖掘中,每个实体是一个项,待考察的搜索空间大小是2,这是可能产生的候选项集的个数在频繁子图挖掘中,每个实体是一个顶点,并且最多可以有d-1条到其他顶点的边。假定顶点的标号是唯一的,则子图的总数是Cx2-2其中,C是选择i个顶点形成子图的方法数,而2-1是子图的顶点之间边的最大值。表7-8对不同的d比较了项集和子图的个数。表7-8对于不同的维度d,项集和子图的个数比较实体个数d1项集个数子图个数222453884567816326412825681131450400692350602286192513
候选子图的个数实际上少得多,因为表7-8给出的个数包含了不连通的子图。不连通的子图通常被忽略,因为它们没有连通子图令人感兴趣。挖掘频繁子图的一种蛮力方法是,产生所有的连通子图作为候选,并计算它们各自的支持度。例如,考虑图7-11a中显示的图。假定顶点标号选自集合{a,b},而边的标号选自集合{p,q},则具有一个到三个顶点的连通子图列在图7-1b中。候选子图的个数比传统的关联规则挖掘中的候选项集的个数大得多,其原因如下。(1)项在某个项集中至多出现一次,而某个顶点标号可能在一个图中出现多次(2)相同的顶点标号对可以有多种边标号选择。给定大量候选子图,即使对于规模适度的图蛮力方法也可能垮掉。 pqp GI G2 G3 G4(a)图数据集的例子K=1apK=2aaaq (a)bp (bbbb一---------K=3papqbppq a)(aapqb)连通子图的列表图7-11挖掘频繁子图的蛮力方法7.5.3类 Apriori方法奖本节考察如何开发一种类 Apriori算法来找出频繁子图。1.数据变换一种可行的方法是将图变换成类似于事务的形式,使得我们可以使用诸如 Apriori等已有的算法。图7-12解释了如何将图的集族变换成等价的购物篮表示。在这种表示下,边标号l(e)与对
应的顶点标号((v),(v)组合被映射到一个“项”。“事务”的宽度由图的边数决定。尽管很简单,但是仅当图中每一条边都具有唯一的顶点和边标号组合时,该方法才可行。否则,就不能使用这种表示对图正确建模。qqppq bPb4bq G1 G2 G3 G4 (a,b,p) (a,,) (a,b,r) (b,c,) (b,c,)(,c,) ...(,e,r) G1G21G300000001000001000G40010000图7-12映射图的集族到购物篮事务2.频繁子图挖掘算法的一般结构挖掘频繁子图的类 Apriori算法由以下步骤组成。(1)候选产生:合并频繁(k-1)-子图对,得到候选k子图。(2)候选剪枝:丢弃包含非频繁的(k-1)子图的所有候选k-子图(3)支持度计数:统计s中包含每个候选的图的个数。(4)候选删除:丢弃支持度小于 minsup的所有候选子图。这些步骤的具体细节在本节的余下部分讨论。7.5.4候选产生在候选产生阶段,将一对频繁(k-1)子图合并成一个候选k子图。首要问题是如何定义子图的大小k。在图7-11显示的例子中,k是图中的顶点个数。通过添加一个顶点,迭代地扩展子图的方法称作顶点增长(vertex growing)。k也可以是图中边的个数。添加一条边到已有的子图中来扩展子图的方法称作边增长(edge growing)为了避免产生重复的候选,我们可以对合并施加附加的条件:两个(k-1)子图必须共享一个共同的(k-2)子图。共同的(k-2)子图称作核core)下面,我们简要描述顶点增长和边增长策略的候选产生过程。1.通过顶点增长产生候选顶点增长是通过添加一个新的顶点到一个已经存在的频繁子图上,产生新候选的过程。在介绍该方法之前,我们首先考虑图的邻接矩阵表示矩阵的每一项M(或者包含连接顶点v和v的边标号,或者为0(顶点之间没有边)。顶点增长方法可以看作合并一对(k-1)(k-1)的邻接矩阵,产生kxk邻接矩阵的过程,如图7-13所示。G1和G2是两个图,其邻接矩阵分别为M(G1)和M(G2)。图中虚线框指出了图的核。通过顶点增长产生候选子图的过程在下面给出。
qp+r⊙ GI G2 G3= merge(G1, G2) o ppo o ppq o o ppiqp0r00 MG1= rMoep= r o rp0 o0rq000q00000图7-13顶点增长策略通过顶点增长合并子图过程邻接矩阵M1)与另一个邻接矩阵M2合并,如果删除M和M2的最后一行和最后一列得到的子矩阵相同。结果矩阵是M(1),添加上M2)的最后一行和最后一列。新矩阵的其余项或者为0,或者用连接顶点对的合法的边标号替换。结果图包含的边比原来的图多一条或两条。在图7-13中,G1和G2都包含4个顶点和4条边。合并之后,结果图G3有5个顶点。G3中边的数目取决于顶点d和e是否相连。如果d和e是不相连的,则G3有5条边,并且(de对应的矩阵项为0;否则,G3有6条边,并且(d,e)的矩阵项对应于新创建的边的标号。由于该边的标号未知,我们需要对(d,e)考虑所有可能的边标号,从而大大增加了候选子图的个数。2.通过边增长产生候选在候选产生期间,边增长将一个新的边插入一个已经存在的频繁子图中。与顶点增长不同,结果子图的顶点个数不一定增加。图7-14显示了通过边增长策略合并G1和G2得到的两个可能的候选子图。第一个候选子图G3多了一个顶点,而第二个候选子图G4的顶点个数与原来的图一样。图中虚线框指出了图的核。 G3= merge(G1,G2) GI POG G4= merge(G1, G2)图7-14边增长策略
通过边增长产生候选子图的过程概括如下。通过边增长合并子图过程一个频繁子图g(1与另一个频繁子图g(2)合并,仅当从删除一条边得到的子图与从g2删除一条边得到的子图拓扑等价。合并后,结果子图是g(1),添加g2的那条额外的边。待合并的图可能包含许多互相拓扑等价( topologically equivalent)的顶点。为了解释顶点拓扑等价的概念,考虑图7-15中所示的图。图G1包含4个顶点,具有相同的标号“a”。如果一条新边附着到4个顶点的任何一个上,结果图看上去都一样。因此,G1的顶点都相互拓扑等价。p V1 pV2 p2 p'3 ppp V3pp G2图7-15顶点拓扑等价的解释图G2有两对拓扑等价的顶点:v1与v4,v2与v,尽管顶点和边的标号都相同。容易看出v不拓扑等价于v2,因为它们附着的边数不同。因此,附着一条新边到v将导致与附着同样的边到v2不同的图。然而,图G3没有任何拓扑等价的顶点。尽管v1和v具有相同的顶点标号和相等的附着边数,但是附着一条新边到v1与附着同样的边到v将导致不同的图。顶点拓扑等价的概念能够帮助我们理解,在边增长时为什么能够产生多个候选子图。考虑图7-16中(k-1)-子图G1和G2。为简单起见,它们的核(包含两个图的k-2条共同边)画成了一个矩形框。G1剩下的一条不在核中的边显示为连接顶点a和b的悬挂边。类似地,G2剩下的一条不在核中的边显示为连接顶点c和d的悬挂边。尽管G1和G2的核相同,但是a和c可能拓扑等价也可能不等价。如果a和c拓扑等价,我们将它们记作a=c。对于核外边的点,如果它们的标号相同,我们将它们记作b=d G1 G2abc核核图7-16通过边增长合并一对子图的一般方法下面的规则可以用来确定候选产生得到的候选子图。(1)如果a≠c并且b≠d,则只有一个可能的结果子图,如图7-17a所示。(2)如果a=c但是b≠d,则有两个可能的结果子图,如图7-17b所示。 PDG(3)如果a≠c但是b=d,则有两个可能的结果子图,如图7-17c所示。(4)如果a=c并且b=d,则有三个可能的结果子图,如图7-17d所示。
 G3=Merge(G1, G2) G3= Merge (G1, G2) G3=Merge(G1,G2)abaab核b○○dcd核c核(a)a≠c并且b≠d(b)a=c但是b≠d G3=Merge(G1,G2) G3=Merge(G1,G2)b核bd核aCc(c)a≠c但是b=d G3=Merge(G1, G2)ab核d G3=Merge(G1,G2) G3=Merge(G1,G2)abab核核(d)a=c并且b=d图7-17通过边增长产生的候选子图当与一对(k-1)子图相关联的核有多个时,还可能产生多个候选子图,如图7-18所示。带阴影的顶点对应于合并时形成核的顶点。每一个核都可能导致不同的候选子图集。原则上,如果合并一对频繁(k-1)-子图,则最多可以有k-2个核,每个核通过从被合并的图中删除一条边得到。尽管边增长过程可能产生多个候选子图,但是候选子图的数量趋向于比顶点增长策略产生的少。+⑥⑥图7-18候选产生过程中候选的多重性7.5.5候选剪枝产生候选k子图后,需要剪去(k-1)子图非频繁的候选。候选剪枝可以通过如下步骤实现:
相继从k-子图删除一条边,并检查对应的(-1)-子图是否连通且频繁。如果不是,则该候选k子图可以丢弃。为了检查(k-1)-子图是否频繁,需要将它与其他频繁(k-1)子图匹配。判定两个图是否拓扑等价(或同构)称为图同构(graph isomorphism)问题。为了解释解决图同构问题的困难性,考虑图7-19中的两个图。尽管这两个图看上去不同,但是它们是同构的,因为在这两个图的顶点之间存在一个1-1映射。ABBBBBB图7-19图同构处理图同构处理图同构问题的标准方法是,将每一个图都映射到一个唯一的串表达式,称作代码(code)或规范标号(canonical label)规范标号具有如下性质:如果两个图是同构的,则它们的代码一定相同。这个性质使得我们可以通过比较图的规范标号来检查图同构。构造图的规范标号的第一步是找出图的邻接矩阵表示。图7-20给出了一个给定图的邻接矩阵的例子。原则上,图可以有多种邻接矩阵表示,因为存在多种确定顶点次序的方法。在图7-20中,第一行和第一列对应于具有3条边的顶点a,第二行和第二列对应于另一个具有2条边的顶点a,如此等等。为了导出图的所有邻接矩阵表示,我们需要考虑矩阵行(和对应列)的所有可能的排列。e o pp qpr M=pp000q000a图7-20图的邻接矩阵表示数学上讲,每个排列都对应于初始邻接矩阵与一个对应的排列矩阵的乘积,如下面的例子所示。 PDG
例7.3考虑下面的矩阵:(1234)5678 M=910111213141516下面的排列矩阵可以用来交换M的第一行(和列)和第三行(和列)00100100P3=10000001其中,P13是通过交换单位矩阵的第一行和第三行得到的。为了交换M的第一和第三行(和列)排列矩阵与M相乘:(1110912)7658 M'=P13'xXMXP3=321415141316注意,M右乘P13交换M的第一列和第三列而M左乘P3交换M的第一行和第三行。如果三个矩阵相乘,则产生矩阵M,其第一、三行交换,第一、三列交换。第二步是确定每个邻接矩阵的串表示。由于邻接矩阵是对称的,因此只需要根据矩阵的上三角部分构造串表示就足够了。在图7-21所示的例子中,代码是通过逐列连接矩阵的上三角元素得到的。最后一步是比较图的所有串表达式并选出具有最小(最大)字典次序值的串。A(1)A(2)A(1)A(2)A(3)(B(5)B(6)B(7)B(8)B(5)B(6A(1)01101000A(2)10010100A(3)10010010A(4)01100001B(7)B(8)B51000010B7)0101001B(8)00010110A(3)A(4)Code=1100111000010010010100001011A(2)A(1)(1)(2)a(3)a(4)B(5)B(6)B(7)B(8)A(1)01010100B(7)B6A2101000103)0101000A(41010000B(5)B(8)B(5)001000B61000001701001100B800011100A(3)A(4)Code=101101001010000010011000111图7-21邻接矩阵的串表达式
前面的方法开销很大,因为它需要考察图的所有可能的邻接矩阵,并计算它们的串表达式,以便找出规范标号。具体地说,对于包含k个顶点的图,需要考虑k种排列。已经开发了一些方法来降低该任务的复杂度,包括存放先前计算的规范标号(这样,当对同一个图进行同构测试时,我们就不必重新计算它)以及通过加入诸如顶点标号和顶点的度数等附加信息来减少确定规范标号所需要的排列数。后一种方法已经超出了本书的范围,但是有兴趣的读者可以参考本章结尾的文献注释。7.5.6支持度计数支持度计数也可能是开销很大的操作,因为对于每个ge必须确定包含在G中的所有候选子图。加快该操作的一种方法是,维护一个与每个频繁(k-1)子图相关联的图I表。如果新的候选k-子图通过合并一对频繁(k-1)-子图而产生就对它们的对应图I表求交集。最后,子图同构检查就在表中的图上进行,确定它们是否包含特定的子图。7.6非频繁模式迄今为止,关联分析都基于这样的前提:项在事务中出现比不出现更重要。因此,数据库中很少出现的模式不是令人感兴趣的,并使用支持度度量将其删除。这种模式称为非频繁模式。定义7.7非频繁模式非频繁模式是一个项集或规则,其支持度小于阈值minsup尽管绝大部分非频繁模式都不是令人感兴趣的,但是其中的一些可能对于分析是有用的,特别是涉及到数据中的负相关时。例如,DVD和VCR一起销售的情况很少,因为购买DVD的顾客多半不会购买VCR,反之亦然。这种负相关模式有助于识别竞争项(competing item),即可以相互替代的项。竞争项的例子包括茶与咖啡、黄油与人造黄油、普通与节食苏打以及台式与便携式计算机。某些非频繁模式也可能暗示数据中出现了某些有趣的罕见事件或例外情况。例如,如果{火灾=yes}是频繁的,但{火灾=yes,警报=o}是非频繁的,则后者是有趣的非频繁模式,因为它可能指出警报系统的故障。为了检测这种不寻常情况,必须确定模式的期望支持度,以便当模式的支持度明显低于期望支持度时,可以声明它是一个有趣的非频繁模式。挖掘非频繁模式是一个挑战,因为可以从给定的数据集导出大量这种模式。具体地说,挖掘非频繁模式的关键问题是:(1)如何识别有趣的非频繁模式,(2)如何在大型数据集中有效地发现它们。为了获得对各种类型的有趣的非频繁模式的不同看法,7.6.1节和7.6.2节分别介绍两个相关的概念负模式和负相关模式。这些模式之间的关系在7.6.3节阐述。最后,7.6.5节和7.6.6节提供两类技术,用来挖掘有趣的非频繁模式。7.6.1负模式设1={i,2,…,i}是项的集合。负项表示项证不在给定的事务中出现。例如,如果事务中不包含咖啡,则咖啡是一个值为1的负项。定义7.8负项集负项集X是一个具有如下性质的项集:(1)X=AUB,其中A是正项的集合,而B是负项的集合,|B≥1;(2)s()≥minsup
定义7.9负关联规则负关联规则是具有如下性质的关联规则:(1)规则是从负项集提取的,(2)规则的支持度大于或等于 minsup,(3)规则的置信度大于或等于 minconf本章中,负项集和负关联规则统称负模式(negative pattern)负关联规则的一个例子是茶咖啡,它暗示喝茶的人倾向于不喝咖啡。7.6.2负相关模式6.7.1节介绍了如何使用相关分析来分析两个分类变量之间的联系。已经证明,对于发现正相关的项集,诸如兴趣因子公式(6-5)和系数公式(6-8)等度量是有用的。本节将这些讨论扩展到负相关模式。用X={x1,x2,x}表示k-项集,P()表示事务包含X的概率。在关联分析中,这个概率通常用项集的支持度s(估计定义7.10负相关项集项集X是负相关的,如果(X)<IIs(x; =() x(x2)(x)(7-3)其中,s(x)是项x的支持度。上面表达式的右端(x)给出了X中的所有项统计独立的概率估计。定义7.10暗示,项集是负相关的,如果它的支持度小于使用统计独立性假设计算出的期望支持度。s()越小,模式就越负相关。定义7.11负相关关联规则关联规则X→是负相关的,如果(<s()s((7-4)其中,X和Y是不相交的项集,即X∩Y=。上面的定义只提供了X中的项与Y中的项之间负相关的部分条件。负相关的完全条件可以表述如下: s(<() IIsov,)(7-5)其中,x∈X而y∈Y由于X中(Y中)的项通常是正相关的,因此使用部分条件而不是完全条件来定义负相关关联规则更实际。例如,尽管根据不等式(7-4),规则眼镜,镜头清洁剂}→{隐形眼镜,盐溶液}是负相关的,但是眼镜与镜头清洁剂是正相关的,隐形眼镜与盐溶液是正相关的。如果使用不等式(7-5),这样的规则可能发现不了,因为它可能不满足负相关的完全条件。负相关条件也可以用正项集和负项集的支持度表示。和Y分别表示X和Y的对应负项集,由于
 s(xun-s()s() =s()-[s()+( s()+s()] =s(X[1-s(X)-s(XUY)-s()]-s()( =s( s()-s()s()负相关条件可以表述如下: s(XUY) s()<s(XUY)s()(7-6)本章中,负相关项集和负相关关联规则统称负相关模式(negatively correlated pattern7.6.3非频繁模式、负模式和负相关模式比较非频繁模式、负模式和负相关模式是三个密切相关的概念。尽管非频繁模式和负相关模式只涉及包含正项的项集或模式,而负模式涉及包含正项和负项的项集或模式,但是这三个概念之间存在一定的共性,如图7-22所示。非频繁模式负模式负相关模式频繁模式图7-22非频繁模式、负模式和负相关模式比较首先,许多非频繁模式有对应的负模式。为了解释为什么出现这种情况,考虑表7-9显示的列联表。如果X是非频繁的,则除非 minsup太高,否则它很可能有对应的负项集。例如,假定 minsup≤0.25,如果XY是非频繁的,则项集x、xY和中至少有一个的支持度肯定大于 minsup,因为列联表中的支持度之和为1表7-9关联规则X→Y的二路列联表YX s( s() s( s( s() s() s( s(F)1其次,许多负相关模式也具有对应的负模式。考虑表7-9显示的列联表和不等式(7-6)所示的负相关条件。如果X和Y具有很强的负相关性,则(XUY)xs(Xun> s(() PDG
因此,当X和Y负相关时,XY或者XUY或者二者必然具有相对较高的支持度。这些项集对应于负模式。最后,由于XY的支持度越低,该模式就越负相关,因此非频繁的负相关模式趋向于比频繁的负相关模式更令人感兴趣。如图7-22所示,非频繁的、负相关的模式是这两类模式的重叠区域。7.6.4挖掘有趣的非频繁模式的技术原则上讲,非频繁项集是未被标准化的频繁项集产生算法(如 Apriori和FP增长)提取的所有项集。这些项集对应于图7-23所示的频繁项集边界之下的那些项集。 null极大频繁项集 (ab )(ac )(ad()( bc) cd)(ce )de abc)abd)(abe )acd)()()bce)(bde)( (abed)( Cabde )()bcde频繁的 Cabede频繁项集边界非频繁的图7-23频繁与非频繁项集由于非频繁模式的数量可能是指数的(特别是对于稀疏的、高维数据),因此,为挖掘非频繁模式而开发的技术着力于仅发现有趣的非频繁模式。这类模式的例子包括7.6.2节讨论过的负相关模式。这些模式可以通过删除那些不满足负相关条件(公式(7-3))的非频繁项集得到。这种方法可能是计算密集的,因为必须计算所有非频繁项集的支持度才能确定它们是否是负相关的。与挖掘频繁项集使用的支持度度量不同,挖掘负相关项集使用的基于相关性的度量不具有可以用于指数搜索空间剪枝的反单调性。尽管难以找到有效的解决方案,但是正如本章文献注释所述,已经开发了一些新颖算法。本章余下部分提供两类技术来挖掘有趣的非频繁模式。7.6.5节介绍挖掘数据中负模式的方法,而7.6.6节介绍基于支持度期望发现有趣的非频繁模式的方法。7.6.5基于挖掘负模式的技术为挖掘非频繁模式开发的第一类技术将每个项看作对称的二元变量。使用7.1节介绍的方法,PDG通过用负项增广,将事务数据二元化。图7-24显示了一个例子,将原始数据变换成具有正项和
负项的事务。对增广的事务使用已有的频繁项集产生算法(如 Apriori),可以推导出所有的负项集。T项集123 {,B}11010101 2{A,B,C)210101001 {C)301011001 4(B,C}401101001 5{B,D)5011110原来的事务包含负项的事务图7-24用负项增广事务仅当只有少量变量被视为对称的二元变量时(即负模式仅涉及少量负项),该方法才是可行的。如果每个项都必须视为对称的二元变量,则由于如下原因,该问题就成为难处理的计算问题。(1)当每个项都用对应的负项增广时,项的个数就加倍。待探测的项集格比2大得多(d是原数据集中项的个数),见本章习题21。(2)当增加进负项后,根据支持度的剪枝将不再有效。对于每个变量x,x或x的支持度大于等于50%。因此,即使支持度阈值达到50%,仍然有一半的项是频繁的。对于较低的阈值,更多的项和包含它们的可能项集都是频繁的。 Apriori使用的基于支持度的剪枝策略仅当大部分项集的支持度较低时才有效;否则的话,频繁项集的数量呈指数增长。(3)当增加进负项后,每个事务的宽度增加。假定原数据集中有d个项。对于像购物篮事务那样的稀疏数据集,每个事务的宽度趋向于远小于d。这样,频繁项集的最大长度wmax受限于最大事务的宽度,也趋向于相对较小。当包含负项时,事务的宽度增加到d,因为一个项要么在事务中,要么不在,而不会既在又不在。由于事务的宽度从max增加到d,这将指数地增加频繁项集的数量。其结果是,许多已有的算法用于扩展数据集时都将失败。前面的蛮力方法计算代价较高,因为它迫使我们确定大量正模式和负模式的支持度。另一种方法不是用负项增广数据集,而是根据对应的正项集计算负项集的支持度。例如,{p,q,}的支持度可以用如下方法计算: s((p,, ) =s({))-s({p,)-s({p, r))+s(lp,,)通常,项集XY的支持度可以用下式得到:()=s()+ l-1)' xs(X u2))(7-7)为了使用公式(7-7),必须对Y的每个子集Z确定s(XZ如果和Z的组合的支持度超过最小支持度值 minsup,则其支持度可以使用 Apriori算法得到,其他组合的支持度必须明确计算。例如,通过扫描整个事务数据集计算。另一种可能的方法是忽略非频繁项集XZ的支持度,或用支持度阈值近似。可以用若干优化策略来进一步提高挖掘算法的性能。首先,可以限制被视为对称二元变量的变量数。具体地说,仅当y频繁时才认为负项y是有趣的。该策略的理由是,稀有项易于产生大
量的非频繁模式,并且其中许多都不是令人感兴趣的。将公式(7-7)中的集合限制于其正项频繁的变量,挖掘算法考虑的候选负项集的个数可能大幅度减少另一种策略是限制负模式的类型。例如,算法考虑负模式XUY,如果它至少包含一个正项(即X≥1)。该策略的理由是,如果数据集包含少量支持度大于50%的正项,则大部分形如X的负模式都将是频繁的,这样就会降低挖掘算法的性能。7.6.6基于支持度期望的技术另一类技术仅当非频繁模式的支持度显著小于期望支持度时,才认为它是有趣的。对于负相关模式,期望支持度根据统计独立性假设计算本节介绍两种计算期望支持度的方法,使用概念分层和基于近邻的方法,称作间接关联(indirect association)。1.基于概念分层的支持度期望仅用客观度量还不足以删除不感兴趣的非频繁模式例如,假设面包和台式计算机是频繁项。即使项集{面包,台式计算机}是非频繁的,并且可能是负相关,它也不是有趣的,因为对于领域专家,它们的支持度低是显然的。因此,需要确定期望支持度的主观方法,避免产生这种非频繁模式。在上面的例子中,面包和台式计算机属于两个完全不同的产品类,因此发现他们的支持度低就毫不奇怪。这个例子也解释了使用领域知识剪裁不感兴趣的项的优点。对于购物篮数据,领域知识可以从诸如图7-25所显示的概念分层中推断。该方法的基本假设是,预期来自同一类产品的项与其他项具有类似的相互作用。例如,由于火腿和熏肉属于相同的产品族,我们预期火腿和薄片食物之间的关联与熏肉和薄片食物之间的关联类似如果任何一对的支持度小于其期望支持度,则非频繁模式是有趣的。食品点心肉类碳酸饮料薄片食物饼干猪肉鸡普通节食土豆片玉米面麦片巧克火腿熏肉鸡肉整鸡豆卷力块图7-25概念分层的例子为了解释如何计算期望支持度,考虑图7-26假定项集{C,G}是频繁的。用s()表示模式的实际支持度,而E()表示期望支持度。C和G的子女或兄弟的期望支持度可以用如下公式计算:(7-8) POG s(C)s(G)
 E(s(C,J)=s(C, G)x)(7-9) s(G) E(s(C, H))=s(C, G)x()(7-10) s(G)BD图7-26使用概念分层挖掘有趣的非频繁模式例如,碳酸饮料和点心是频繁的,则节食碳酸饮料和薄片食物的期望支持度可以使用公式(7-8)计算,因为这两个项分别是碳酸饮料和点心的子女。如果节食碳酸饮料和薄片食物的实际支持度明显低于它们的期望值,则节食碳酸饮料和薄片食物形成一个有趣的非频繁模式。2.基于间接关联的支持度期望考虑商品对(a,b),它们很少被顾客同时购买。如果a和b是不相关的商品,如面包和DVD播放机,则它们的支持度预期较低;如果a和b是相关的商品,则它们的支持度预期较高。前面使用概念分层计算期望支持度。本节提供一种确定两个商品之间的期望支持度的方法:考察通常与这两个商品一起购买的其他商品。例如,假定购买睡袋的顾客更有可能也购买其他野营设备,而买台式计算机的顾客也更有可能购买其他计算机附件,如光电鼠标或打印机。假定没有其他商品频繁地与睡袋和台式计算机一起购买,这些不相关的商品的支持度预期较低。另一方面,假定节食和普通碳酸饮料都经常与薄片食物和点心一起购买。即使不使用概念分层,这两种商品可望是相关的,并且它们的支持度应当较高。因为他们的实际支持度低,节食和普通碳酸饮料形成了一个有趣的非频繁模式。这样的模式称作间接关联(indirect association)模式。间接关联的高层解释见图7-27。项a和b对应于节食和普通碳酸饮料,而称作中介集(mediator set),包含诸如薄片食物和点心等商品。间接关联的形式定义在下面给出。Y y1所 POG图7-27一对项之间的间接关联定义7.12间接关联一对项a,b是通过中介集Y间接关联的,如果下列条件成立。
(1)s({a,b})<t(项对支持度条件)(2)3≠使得(a)s({a}=y并且s(b}≥y(中介支持度条件);(b)d({a},≥ta,d({b},≥ta,其中d(X,Z是X和Z之间关联的客观度量(中介依赖条件)注意,中介支持度和依赖条件用来确保Y中的项形成a和b的近邻。可以使用6.7.1节介绍的兴趣因子、余弦或IS、 Jaccard和其他依赖度量。间接关联有许多可能的应用。在购物篮分析中,a和b可以是竞争商品,如台式计算机和便携式计算机。在文本挖掘中,间接关联可以用来识别同义词、反义词,或用于不同上下文的词。例如,给定一个文档集族,词数据和黄金可以通过中介挖掘间接关联。该模式暗示,词挖掘可以用于两种不同的上下文数据挖掘与黄金挖掘。间接关联可以用如下方法产生。首先,使用诸如 Apriori和FP增长等标准算法生频繁项集。然后,合并每对频繁k项集得到候选间接关联(a,b,),其中a和b是一对项,而Y是它们的公共中介。例如,若{p,q,r}和{p,q,s}是频繁项集,则通过合并这对频繁项集得到候选间接关联(r,s,{p,q})产生候选之后,就要验证它是否满足定义7.12中的项对支持度和中介依赖条件。然而,中介支持度条件不必验证,因为候选间接关联是通过合并一对频繁项集得到的。该算法汇总在算法7.2中。算法7.2挖掘间接关联的算法1:产生频繁项集的集合F 2: for=2 to kmax do3:Ck={(a,b,{a}y∈f,b}yk,a≠b4:for每个候选(a,b, do5:ifs({a,b<td({a},≥tad(b},≥6:={(a, b, Y)} 7: end if 8: end for 9: end for 10: Result =UIk文献注释从分类和连续数据中挖掘关联规则的问题由 Srikant和 Agrawal提出[363]他们的策略是二元化分类属性,使用等频离散化连续属性。他们还提出了部分完备性(partial completeness)度量,来确定离散化导致的信息损失量。然后使用该度量来确定所需要的离散区间个数,以确保损失的信息量可以保持在一个期望水平沿着这一工作,学者们提出了许多挖掘量化关联规则的方法。 Aumann和 Lindell[343]开发了基于统计学的方法,识别在某些定量属性上展示有趣性质的总体段。该形式化方法被其他人进一步扩展,包括Webb[368]和 Zhang等[372]min- Apriori算法由Han等[349]提出,用来发现连续数据中的关联规则,而不进行离散化。还有许多研究者也在书中写过挖掘连续数据中关联规则的问题,他们是 Fukuda等[347]、Lent等[355]、Wang等[367]以及 Miller和Yang[357]。7.3节介绍的使用扩展的事务处理概念分层的方法由 Srikant和 Agrawal提出[362]。另一种可
供选择的算法由Han和Fu[350]提出,该算法一次产生一层频繁模式。具体地说,他们的算法在概念分层的顶层产生所有的频繁1-项集。频繁-项集的集合记作L(1,1).使用L(1,1)中的频繁1-项集,算法进一步产生第一层的所有频繁2项集L(1,2)重复该过程,直到提取了最高概念层中的所有频繁项集L(1,k)(k>1)。然后,基于L(1,1)中的频繁项集,算法继续提取下一个概念层中的频繁项集L(2,1)。继续该过程,直到处理完用户指定的最低概念层后停止。7.4节介绍的序列模式的形式化描述和算法由 Agrawal和 Srikant341,364]提出同样,Mannila等[356]引进了频繁周期模式的概念,用来从长事件流中挖掘序列模式。另一种形式的序列模式挖掘基于正则表达式,由 Garofalakis等[348]提出 Joshi等试图统一各种不同的序列模式表示[352],其结果是带有不同计数方案(7.4.4节介绍)的序列模式的通用表示。挖掘序列模式的其他可供选择的算法由Pei等[359]、 Ayres等[344]、Chng等[346]和Seno等[361]提出频繁子图挖掘问题最早由 Inokuchi等提出[31]。他们使用了顶点增长方法,由图数据集产生频繁的归约子图。边增长方法由 Kuramochi Karypis开发[353]他们还提出了一种类 Apriori算法FSG,处理诸如多重候选、规范标号、顶点变体等问题。另一种频繁子图挖掘算法称作 gSpan,由yang和Han开发[370]。这种算法试图使用最小化的DFS码对各种子图编码。频繁子图挖掘的其他变形由Zaki[371]、 Parthasarathy和 Coatney[358]以及 Kuramochi和 Karypis[3541提出许多研究者都考察了挖掘非频繁模式的问题。 Savasere等[360]考察了使用概念分层挖掘负相关规则。Tan等[365]提出了挖掘序列和非序列数据的间接关联的思想。挖掘负模式的有效算法由 Boulicaut等[345]、Teng等[366]、Wu等[39]以及 Antonie和 Zaiane[342]提出参考文献 [341] R. Agrawal and R. Srikant. Mining Sequential Patterns. In Proc. of Intl. Conf. on Data Engineering, pages 3-14, Taipei, Taiwan, 1995 [342] M.-L. Antonie and O.. Zaiane. Mining Positive and Negative Association Rules: An Approach for Confined Rules. In Proc. of the 8th European Conf. of Principles and Practice of Knowledge Discovery in Databases, pages 27-38, Pisa, Italy, September 2004 [343] Y. Aumann and Y. Lindell. A Statistical Theory for Quantitative Association Rules. In KDD99, pages 261-270, San Diego, CA, August 1999 [344] J. Ayres, J. Flannick, J. Gehrke, and T. Yiu. Sequential Pattern mining using a bitmap representation. In Proc. of the 8th Intl. Conf. on Knowledge Discovery and Data Mining, pages 429-435, Edmonton, Canada, July 2002 [345] J.-F. Boulicaut, A. Bykowski, and B. Jeudy. Towards the Tractable Discovery of Association Rules with Negations. In Proc. of the 4th Intl. Conf on Flexible Query Answering Systems FQAS'00, pages 425-434, Warsaw, Poland, October 2000 [346] H. Cheng, X. Yan, and J. Han. IncSpan incremental mining of sequential patterns in large database. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 527-532, Seattle, WA, August 2004 [347] T. Fukuda, Y. Morimoto, S. Morishita and T. Tokuyama. Mining Optimized Association Rules for Numeric Attributes. In Proc. of the 15th Symp. on Principles of Database Systems, pages 182-191, Montreal, Canada, June 1996. [348] M. N. Garofalakis, R. Rastogi, and K. Shim. SPIRIT: Sequential Pattern Mining with Regular Expression Constraints. In Proc. of the 25th VLDB Conf., pages 223-234, Edinburgh, Scotland1999 [349] E.-H. Han, G. Karypis, and V. Kumar. Min-Apriori: An Algorithm for Finding Association Rules in data with continuous attributes. http: //www.cs.umn.edu/  han, 1997.
 [350] J. Han and Y. Fu. Mining Multiple-Level Association Rules in Large Databases. IEEE Trans. on Knowledge and Data Engineering, 11(5): 798-804, 1999[351]A. Inokuchi,t. Washio,andh. Motoda. An Apriori-based- Algorithm for Mining Frequent Substructures from Graph Data. In Proc. of the 4th European Conf. of Principles and Practice of Knowledge Discovery in Databases, pages 13-23, Lyon, France, 2000 [352] M. V. Joshi, G. Karypis, and V. Kumar. A Universal Formulation of Sequential Patterns. In Proc. of the KDD'2001 workshop on Temporal Data Mining, San Francisco, CA, August 2001 [353] M. Kuramochi and G. Karypis. Frequent Subgraph Discovery. In Proc. of the 2001 IEEE Intl. Conf. on Data Mining, pages 313-320, San Jose, CA, November 2001. [354] M. Kuramochi and. Karypis. Discovering Frequent Geometric Subgraphs. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, pages 258-265 Maebashi City, Japan, December 2002 [355] B. Lent, A. Swami, and J. Widom. Clustering Association Rules. In Proc. of the 13th Intl. Conf. on Data Engineering, pages 220-231, Birmingham, U.K, April 1997. [356] H. Mannila, H. Toivonen, and A. I. Verkamo Discovery of Frequent Episodes in Event Sequences. Data Mining and Knowledge Discovery 1(3): 259-289, November 1997. [357] R. J. Miller and Y. Yang. Association Rules over Interval Data. In Proc. of 1997 ACM-SIGMOD Intl. Conf. on Management of Data, pages 452-461, Tucson, AZ, May 1997. [358] S. Parthasarathy and M. Coatney. Efficient Discovery of Common Substructures in Macromolecules. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, pages 362-369, Maebashi City, Japan, December 2002 [359] J. Pei, J. Han, B. Mortazavi-Asl, Q. Chen U. Dayal, and M. Hsu. PrefixSpan: Mining Sequential Patterns efficiently by prefix-projected pattern growth. In Proc of the 17th Intl. Conf. on Data Engineering, Heidelberg, Germany, April 2001. [360] A. Savasere, E. Omiecinski, and S. Navathe. Mining for Strong Negative Associations in a Large Database of Customer Transactions. In Proc. of the 14th Intl. Conf. on Data Engineering, pages 494- 502, Orlando, Florida, February 1998. [361] M. Seno and G. Karypis. SLPMiner: An Algorithm for Finding Frequent Sequential Patterns Usi Length-Decreasing Support Constraint. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, pages 418-425, Maebashi City, Japan, December 2002 [362]. Srikant and R. Agrawal. Mining Generalized Association Rules. In Proc. of the 21st VLDB Conf., pages 407-419, Zurich, Switzerland, 1995 [363] R. Srikant and R. Agrawal. Mining Quantitative Association Rules in Large Relational Tables.In Proc. of 1996 ACM-SIGMOD Intl. Conf. on Management of Data, pages 1-12, Montreal, Canada,1996 [364] R. Srikant and R. Agrawal. Mining Sequential Patterns: Generalizations and Performance Improvements. In Proc. of the 5th Intl Conf. on Extending Database Technology (EDBT'96), pages 18-32, Avignon, France, 1996 [365] P. N. Tan,. Kumar, and. Srivastava. Indirect Association: Mining Higher Order Dependencies in Data. In Proc. of the 4th European Conf. of Principles and Practice of Knowledge Discovery in Databases, pages 632-637, Lyon, France, 2000. [366] W. G. Teng, M. J. Hsieh, and M.-S. Chen. On the Mining of Substitution Rules for Statistically Dependent Items. In Proc. of the 2002 IEEE Intl. Conf. on Data Mining, pages 442-449, Maebashi City, Japan, December 2002 [367] K. Wang, S. H. Tay, and B. Liu. Interestingness-B Interval Merger for Numeric Association Rules. In Proc. of the 4th Intl. Conf. on Knowledge Discovery and Data Mining, pages 121-128, New York, NY, August 1998. [368] G. I. Webb. Discovering associations with numeric variables. In Proc. of the 7th Intl. Conf. on Knowledge Discovery and Data Mining, pages 383- 388, San Francisco, CA, August 2001 [369] X. Wu, C. Zhang, and S. Zhang. Mining Both Positive and Negative Association Rules. ACM Trans. caux: PDG on Information Systems, 22(3): 381 -405, 2004 [370] X. Yan and J. Han. gSpan: Graph-based Substructure Pattern Mining. In Proc. of the 2002 IEEE Intl.
 Conf. on Data Mining, pages 721 -724, Maebashi City, Japan, December 2002. [371] M. J. Zaki. Efficiently mining frequent trees in a forest. In Proc. of the 8th Intl. Conf. on Knowledge Discovery and Data Mining, pages 71-80, Edmonton Canada, July 2002. [372] H. Zhang, B. Padmanabhan, and A. Tuzhilin. On the Discovery of Significant Statistical Quantitative Rules. In Proc. of the 10th Intl. Conf. on Knowledge Discovery and Data Mining, pages 374-383., Seattle, WA, August 2004.习题1.考虑表7-10所示交通事故数据。表7-10交通事故数据天气条件驾驶员状况交通违章安全带损毁程度好饮酒超速较大清醒无较小清醒不遵守停车指示无有有有无有有有无无有有较小坏好好坏好坏好好坏好坏清醒超速较大清醒不遵守交通信号较大饮酒不遵守停车指示较小饮酒无较大清醒不遵守交通信号较大饮酒无较大清醒不遵守交通信号较大饮酒超速较大清醒不遵守停车指示较小(a)给出数据集的二元化版本。(b)在二元化数据中,每个事务的最大宽度是多少?(c)假定支持度阈值是30%,将产生多少候选和频繁项集?(d)创建一个数据集,只包含如下非对称二元属性:(天气条件=坏,驾驶员状况=饮酒,交通违章=是,安全带=无,损毁程度=较大)对于交通违章,无违章取值0,其余情况属性值均为1。假定支持度阈值是30%,将产生多少候选和频繁项集?(e)比较(c)和(d)产生的候选和频繁项集的数量。2.(a)考虑表7-11所示数据集。假定对数据集的连续属性使用如下离散化策略。D1:将每个连续属性的值域划分成3个等宽的箱。D2:将每个连续属性的值域划分成3个箱,每个箱包含的事务个数相同。对于每种策略,回答如下问题。i.构造数据集的二元化版本。ⅱ.导出支持度大于或等于30%的所有频繁项集。(b)连续属性也可以使用聚类方法进行离散化。1.为表7-11所示的数据点绘制温度与气压图i.从该图可以看出多少个自然聚类?对图中每个聚类赋予一个标号(C1、C2等)。 PDGi你认为可以使用何种类型的聚类算法来识别这些聚类?陈述你的理由。
iv.使用非对称的二元属性C1、C2等置换表7-11中的温度和气压属性。使用新的属性(连同警报1、警报2和警报3)构造一个变换矩阵。v.从二元化数据导出支持度大于或等于30%的频繁项集。表7-11习题2的数据集温度气压警报1警报2警报39511058510401234567891031090971084801038100108083102501110111101101100110101010186103010111003.考虑表7-12所示数据集。第一个属性是连续的而其余两个属性是非对称二元的。一个规则是强规则,如果它的支持度超过15%且置信度超过60%表7-12给出的数据支持如下两个强规则。(i){(≤A≤2),B=1}→{C=1(){(5≤A≤8),B=1}→{C=1}表7-12习题3的数据集A123456789B11111001000010C1100110100011112(a)计算这两个规则的支持度和置信度。(b)为了使用传统的Apriori算法找出这些规则,我们需要离散化连续属性A。假定我们使用等宽分箱方法离散化该数据,其中 bin-width=2,3,4.对于每个bin- width,上面两个规则是否能够被 Apriori算法发现?(注意,由于属性A可能具有较宽或较窄的区间,规则不一定与前面的规则完全相同。)对于每个与前面规则对应的规则,计算其支持度和置信度。(c)评述使用等宽分箱方法对上述数据集分类的有效性。是否有合适的箱宽度,以便很好地发现上面两个规则?如果没有,可以使用何种其他方法,以确保能够同时发现以上两个规则?
4.考虑表7-13所示数据集。表7-13习题4的数据集每周上网时数(B)年龄(A)0-55-1010-2020-3030-4010-1515-2522041025-3510351650553033232235-50(a)对于下面的每组规则,确定具有最高置信度的规则。1.15<A<25→10<B<20,10<A<2→10<B<20和15<A<35→10<B<20i.15<A<25→10<B<20,15<A<25→5<B<20和15<A<25→5<B<30i.15<A<25→10<B<20和10<a<35→5<b<30(b)假定我们希望找出年龄在15岁到25岁之间的因特网用户每周的平均上网小时数。写一个基于统计学的关联规则,来刻画这个年龄段的用户。为了计算平均上网小时数,用中点近似值来表示每个区间(例如,使用B=7.5来表示区间5<B<10)(c)通过将(b)中的平均上网小时数与不属于该年龄段的其他用户的平均上网小时数进行比较,检查(b)的量化关联规则是否具有统计意义。5.对于具有下面给出的属性的数据集,描述如何将它转换成适合于关联分析的二元事务数据集。具体地,指出原数据集中的每个属性(a)对应于事务数据集中多少个二元属性;(b)原属性的值如何映射到二元属性的值;(c)数据属性值中是否有分层结构可以用来将数据分组,形成少量二元属性。下面是该数据集的属性列表以及它们的可能值。假定所有的属性都在每个学生上收集。年级:一年级、二年级、三年级、四年级硕士研究生、博士研究生、专业人员。邮政编码:美国学生的家庭邮政编码,非美国学生的住处邮政编码。·院:农学、建筑学、继续教育、教育、文学工程、自然科学、商学、法律、医学、牙科、药学、护理学、兽医学。住校:如果学生住校为1,否则为0以下每个项是一个属性,如果学生说对应的语言,则取1,否则取0阿拉伯语孟加拉语汉语,英语葡萄牙语俄语一西班牙语6.考虑表7-14所示数据集。假定我们对提取如下形式的关联规则感兴趣: PDG{a1≤年龄≤a2,弹钢琴=是}→{喜欢古典音乐=是}
表7-14习题6的数据集年龄弹钢琴喜欢古典音乐9111417192125293339是是是是是否否是否否否否是是否否是否否是否是否是4147为了处理连续属性,我们使用等频方法,区间个数为3、4和6分类属性通过引进与分类值个数一样多的新的非对称二元属性来处理假定支持度阈值是10%,置信度阈值是70%(a)假定我们将年龄属性离散化成3个等频区间。找出满足最小支持度和最小置信度的a1和a2(b)将年龄属性离散化成4个等频区间,重复()将得到的规则与(a)得到的规则进行比较。(c)将年龄属性离散化成6个等频区间,重复(a)将得到的规则与(a)得到的规则进行比较。(d)由(a)、(b)和(c)的结果,讨论离散化区间的选择对关联规则挖掘算法所提取的规则的影响。7.考虑表7-15所示的事务,其中商品分类由图7-25给出。表7-15购物篮事务的例子事务ID购买的商品薄片食物,饼干,普通碳酸饮料,火腿1234567薄片食物,火腿,鸡肉,节食碳酸饮料火腿,熏肉,整鸡,普通碳酸饮料薄片食物,火腿,鸡肉,节食碳酸饮料薄片食物,熏肉,鸡肉薄片食物,火腿,熏肉,整鸡,普通碳酸饮料薄片食物,饼干,鸡肉,节食碳酸饮料(a)挖掘带产品分类的关联规则的主要挑战是什么?(b)考虑下面的方法:每个事务t用扩展的事务t替换,t包含t中所有商品和它们的祖先。例如,事务t={薄片食物,饼干}用t={薄片食物,饼干,点心,食品}替换。使用该方法导出所有支持度大于或等于70%的频繁项集(长度不超过4)(c)考虑另一种方法,其中频繁项集逐层产生。开始,产生分层结构顶层的所有频繁项集PDG然后,使用较高层发现的频繁项集,产生涉及较低层中项的候选项集。例如,仅当{点心,碳酸饮料}频繁时,才产生候选项集{薄片食物,节食碳酸饮料}。使用该方法导出
所有支持度大于或等于70%的频繁项集(长度不超过4)(d)比较(b)和(c)找出的频繁项集。评述算法的有效性和完全性。8.下面的问题考察关联规则的支持度和置信度,它们可以因概念分层而变化。(a)考虑给定概念分层中的项x令x1,x2,…,k表示概念分层中x的k个子女。证明s(x)≤(xi),其中s()是项的支持度。在什么条件下,不等式取等号?(b)设p和q是一对项,而和q是它们在概念分层中的对应父母如果s(p,q)>minsup,下面哪些项集肯定是频繁的?(i)s(p,q),(i)s({p,q),(i)s({p,q(c)考虑关联规则{p}→{q}。假定规则的置信度超过 minconf下面哪些规则的置信度肯定高于 minconf?(i){p}→{q},(i{p}→{a},(i){p}→{q}9.(a)假定没有时限约束,列举包含在下面数据序列中的所有4子序列:<{1,3}2}{2,3}{4}>(b)假定未施加任何时限约束,列举包含在a)的数据序列中的所有3子序列。(c)列举包含在(a)的数据序列中的所有4子序列(假定时限约束是灵活的)(d)列举包含在(a)的数据序列中的所有3-子序列(假定时限约束是灵活的)10.给定表7-16所示的序列数据库,找出支持度大于等于50%的所有频繁子序列。假定序列上没有施加时限约束。表7-16各种传感器产生的事件序列的例子传感器时间戳事件 S1 A,B D,E S21234123123412341234 A,B C,D S3 D,E S4 D,E S5EBABDCDCEBABA B,CD11.(a)对于下面给定的每个序列w=<ee2,确定它们是否是序列<{1,2,3}2,4}{2,4,5}{3,5}{6}>的子序列,时限约束为: mingap=0(e中最后一个事件和ei+1中第一个事件之间的间隔大于0) maxgap=3(e中第一个事件和e+1中最后一个事件之间的间隔小于等于3) maxspan=5(e1中第一个事件和中最后一个事件之间的间隔小于等于5) PDG wS=(e中第一个事件和最后一个事件之间的间隔小于等于1)
W=<{1}{2}{3}>●W=<{1,2,3,4}{5,6}>W=<{2,4}{2,4}{6}>w=<{1}{2,4}{6}>●W=<{1,2}{3,4}{5,6}>(b)确定上面每个子序列w是否是下面序列s的邻接子序列。●={1,2,3,4,5,6}{1,2,3,4,5,6}{1,2,3,4,5,6}S={1,2,3,4}{1,2,3,4,5,6}{3,4,5,6}S={1,2}{1,2,3,4}{3,4,5,6}{5,6}S={1,2,3}{2,3,4,5}{4,5,6}12.对于下面给定的每个序列w=<>,确定它们是否是数据序列{A,B){C,{A,B{C,D}{A,B}{C,D})的子序列,时限约束为: mingap=0(e中最后一个事件和ei+1中第一个事件之间的间隔大于0) maxgap=2(e中第一个事件和ei+1中最后一个事件之间的间隔小于等于2) maxspan=6(e1中第一个事件和elas中最后一个事件之间的间隔小于等于6) ws=1(e中第一个事件和最后一个事件之间的间隔小于等于1)(a)w={A}{B}{}{D(b)w={A}{B,C,D}{A}(c)w={}{,B,C,D}{a}(d)w={B,C}{,D}{,} (e)=([A, B, C, D) (A, B, C,)13.考虑下面各频繁3-序列:1,2,3}1,2}3}、{1}{2,3}(1,2}4}({1,3}{4}(1,2,4}{2,3}{3}{2,3}{4})、{2}{3}{3})和{2}{3}{4})。(a)列出GSP算法的候选产生步骤产生的所有候选4序列。(b)列出GSP算法的候选剪枝步骤剪掉的所有候选4序列(假定没有时限约束)(c)列出GSP算法的候选剪枝步骤剪掉的所有候选4-序列( maxgap假定=114.考虑表7-17所示的给定对象的数据序列。根据如下计数方法,对序列{p}{q}{r}的出现次数计数。表7-17习题14事件序列数据的例子时间戳事件,qr1234567890S p,所 r,s,rqs PDG:P,r.s
(a)CO(每个对象出现一次)(b)CWIN(每个滑动窗口出现一次)(c)CMINWIN(最小出现窗口数)(d)CDIST_(允许事件一时间戳重叠的不同出现)(e)CDIST(不允许事件时间戳重叠的不同出现)15.为了使频繁子图挖掘算法能够处理如下类型的图,讨论算法应做的必要修改。(a)有向图。(b)无标号图。(c)无环图。(d)非连通图。上面给定的图类型影响算法的哪些步骤(候选产生、候选剪枝和支持度计数),是否有进一步的优化有助于提高算法的性能。16.画出连接图7-28中的图对得到的所有候选子图。假定使用边增长算法扩展子图。(a)@a+(b)图7-28习题16的图17.画出连接图7-29中的图对得到的所有候选子图。假定使用边增长算法扩展子图。+(a)⑥+ baa(b)图7-29习题17的图 PDG18.(a)如果用归纳子图联系定义支持度证明如果允许g1和g2具有重叠的顶点集,则规
则g1→g2的置信度可能大于1。(b)确定具有|V|个顶点的图的规范标号的时间复杂性是多少?(c)子图的核可能是多重自同构的。这可能增加合并两个具有相同核的频繁子图后得到的候选子图的个数。确定由于k个顶点的核的自同构得到的候选子图的最大个数。(d)两个大小为k的频繁子图可能共享多个核。确定被两个频繁子图共享的核的最大个数。19.考虑一个图挖掘算法,它使用边增长方法合并如下所示两个无向、无权子图。+(a)绘制合并两个子图时得到的不同的核。(b)使用下面的核可以产生多少候选?20.原来的关联规则挖掘框架只考虑项在事务中同时出现。有时候非频繁的项集可能也是富含信息的。例如,项集{TV,DVD,VCR}示,许多购买电视和DVD的顾客不购买VCR。本题要求将关联规则框架扩充到负项集(即包含项的出现和不出现)。我们使用符号“一”表示缺失项。(a)一种导出负项集的朴素方法是扩充每个事务,使之包含缺失项,如表7-18所示。表7-18数值数据集的例子 TID T12Ⅳ11 -TV DVD -DVD VCR -VCR000100101i.假定事务数据库包含1000个不同的项。由这些项可能产生的正项集的总数是多少?(注意:正项集不包含任何负项。)i.由这些事务产生的频繁项集的最大个数是多少?(假定频繁项集可以包含正项、负项或二者。)i,解释为什么这种用负项扩充每个事务的朴素方法对于导出负项集不切实际。(b)考虑表7-15所示的数据库。如下涉及普通和节食碳酸饮料的负关联规则的支持度和置信度是多少?i.一普通碳酸饮料→节食碳酸饮料。.普通碳酸饮料→节食碳酸饮料。
ⅲ节食碳酸饮料→普通碳酸饮料。iv.节食碳酸饮料→普通碳酸饮料。21.假定我们想从包含d个项的数据集中提取正项集和负项集。(a)考虑一种方法,引进一个新的变量来表示每个负项。使用这种方法,项的个数从d增加到2d。假定项集可以包含同一个变量的正项和负项,项集格的大小是多少?(b)假定项集必须包含不同变量的正项和负项。例如,项集{a,a,b,c}是不合法的,因为它同时包含了变量a的正项和负项。项集格的大小是多少?22.对于下面定义的每种类型的模式,确定支持度度量随着项集大小的增长,是否是单调的、反单调的或非单调的(即既不单调,也不反单调)(a)包含正项和负项的项集,如{a,b,c,d}用于这样的模式,支持度度量是否是单调的、反单调的或非单调的?(b)诸如{(av bv),d,e}的布尔逻辑模式可能包含项的析取与合取。用于这样的模式,支持度度量是否是单调的、反单调的或非单调的?23.许多关联分析算法依赖类 Apriori方法找出频繁模式。算法的总体结构如下。算法7.3类 Apriori算法1:k=12:F={ii∈o(i)n≥minsup}找出频繁1-模式} 3:repeat4:k=k+15:ck= genCandidate(fk-1){候选产生}6:ck= pruneCandidate(ck,fr-1){候选剪枝}7 7: C= count(Ck, D){支持度计数}8:F= cEcnminsup提取频繁模式} 9: until= 10: Answer = Fk假定我们对发现形如{avb}→{c,d}的布尔逻辑规则感兴趣。其中,规则可能涉及项的析取与合取。对应的项集可以写成{(avb),c,d}(a)对于这样的项集,先验原理是否依然成立?(b)如何修改候选产生步骤,以便发现这样的模式?(c)如何修改候选剪枝步骤,以便发现这样的模式?(d)如何修改支持度计数步骤,以便发现这样的模式? POG

第章聚类分析:基本概念和算法聚类分析将数据划分成有意义或有用的组()。如果目标是划分成有意义的组,则簇应当捕获数据的自然结构。然而,在某种意义下,聚类分析只是解决其他问题(如数据汇总)的起点。,,无论是旨在理解还是实用,聚类分析都在广泛的领域扮演着重要角色。这些领域包括:心理学和其他社会科学、生物学、统计学、模式识别、信息检索、机器学习和数据挖掘。聚类分析在许多实际问题上都有应用。我们给出一些具体的例子,按聚类目的是为了理解还是实用来组织。旨在理解的聚类在对世界的分析和描述中,类,或在概念上有意义的具有公共特性的对象组,扮演着重要的角色。的确,人类擅长将对象划分成组(聚类),并将特定的对象指派到这些组分类)例如,即使很小的孩子也能很快地将图片上的对象标记为建筑物、车辆、人、动物、植物等。就理解数据而言,簇是潜在的类,而聚类分析是研究自动发现这些类的技术。下面是一些例子。生物学。生物学家花了许多年来创建所有生物体的系统分类学(层次结构的分类):界(kingdom)、门(phylum)、纲(class)、目(order)、科(family)、属(genus)和种(species)这样,或许并不奇怪,聚类分析早期的大部分工作都是在寻求创建可以自动发现分类结构的数学分类方法。最近,生物学家使用聚类分析大量的遗传信息。例如,聚类已经用来发现具有类似功能的基因组。·信息检索。万维网包含数以亿计的Web页面,网络搜索引擎可能返回数以千计的页面。可以使用聚类将搜索结果分成若干簇,每个簇捕获查询的某个特定方面。例如,查询“电影”返回的网页可以分成诸如评论、电影预告片、影星和电影院等类别。每一个类别(簇)又可以划分成若干子类别(子簇),从而产生一个层次结构,帮助用户进一步探索查询结果。·气候。理解地球气候需要发现大气层和海洋的模式。聚类分析已经用来发现对陆地气候具有显著影响的极地和海洋大气压力模式。心理学和医学。一种疾病或健康状况通常有多种变种,聚类分析可以用来发现这些子类别。例如,聚类已经用来识别不同类型的抑郁症。聚类分析也可以用来检测疾病的时间和空间分布模式。·商业。商业点收集当前和潜在顾客的大量信息。可以使用聚类将顾客划分成若干组,以便进一步分析和开展营销活动。旨在实用的聚类聚类分析提供由个别数据对象到数据对象所指派的簇的抽象。此外,一些聚类技术使用簇原型(即代表簇中其他对象的数据对象)来刻画簇特征。这些簇原型可以用作大
量数据分析和数据处理技术的基础。因此就实用而言,聚类分析是研究发现最有代表性的簇原型的技术。·汇总。许多数据分析技术,如回归和PC,都具有(m2)或更高的时间或空间复杂度(其中m是对象的个数)。因此,对于大型数据集,这些技术不切实际。然而,可以将算法用于仅包含簇原型的数据集,而不是整个数据集。依赖分析类型、原型个数和原型代表数据的精度,汇总结果可以与使用所有数据得到的结果相媲美。·压缩。簇原型可以用于数据压缩。例如,创建一个包含所有簇原型的表,即每个原型赋予一个整数值,作为它在表中的位置(索引)。每个对象用与它所在的簇相关联的原型的索引表示。这类压缩称作向量量化(vector quantization),并常常用于图像、声音和视频数据,此类数据的特点是:(1)许多数据对象之间高度相似,(2)某些信息丢失是可以接受的,(3)希望大幅度压缩数据量。有效地发现最近邻。找出最近邻可能需要计算所有点对点之间的距离。通常,可以更有效地发现簇和簇原型。如果对象相对地靠近簇的原型,则我们可以使用簇原型减少发现对象最近邻所需要计算的距离的数目直观地说,如果两个簇原型相距很远,则对应簇中的对象不可能互为近邻。这样,为了找出一个对象的最近邻,只需要计算到邻近簇中对象的距离,其中两个簇的邻近性用其原型之间的距离度量。这种思想的更详细描述见第2章习题25。本章是聚类分析导论。我们从概述聚类分析开始,包括将对象划分成簇的集合的各种方法和聚类的不同类型的讨论。然后介绍三种专门的聚类技术:K均值、凝聚的 DBSCAN层次聚类和它们代表一大类算法,并用于解释各种概念。本章的最后一节专门讨论簇的有效性评估聚类算法产生的簇的方法。更高级的聚类概念和算法将在第9章讨论。我们尽可能地讨论不同方案的优点和缺点。此外,文献注释提供更深入考察聚类分析的相关书籍和文章8.1概述在讨论具体的聚类技术之前,我们先提供必要的背景知识。首先,我们进一步定义聚类分析,解释它的困难所在,并阐述它与其他数据分组技术之间的关系。然后,考察两个重要问题:(1)将数据对象集划分成簇集合的不同方法,(2)簇的类型。8.1.1什么是聚类分析聚类分析仅根据在数据中发现的描述对象及其关系的信息,将数据对象分组其目标是,组内的对象相互之间是相似的(相关的),而不同组中的对象是不同的(不相关的)。组内的相似性(同质性)越大,组间差别越大,聚类就越好。在许多应用中,簇的概念都没有很好地加以定义。为了理解确定簇构造的困难性,考虑图8-1.该图显示了20个点和将它们划分成簇的3种不同方法。标记的形状指示簇的隶属关系。图8-1b和图8-1d分别将数据划分成两部分和六部分。然而,将2个较大的簇都划分成3个子簇可能是人的视觉系统造成的假象。此外,说这些点形成4个簇(如图8-1c所示)可能也不无道理。该图表明簇的定义是不精确的,而最好的定义依赖于数据的特性和期望的结果。聚类分析与其他将数据对象分组的技术相关。例如,聚类可以看作一种分类,它用类(簇)标号创建对象的标记。然而,只能从数据导出这些标号。相比之下,第4章的分类是监督分类
( supervised classification),即使用由类标号已知的对象开发的模型,对新的、无标记的对象赋予类标号。为此,有时称聚类分析为非监督分类(unsupervised classification)在数据挖掘中,不附加任何条件使用术语分类时,通常是指监督分类。∵a)原来的点(b)两个簇∵★★(c)四个簇(d)六个簇图8-1相同点集的不同聚类方法此外,尽管术语分割(segmentation)和划分(partitioning)有时也用作聚类的同义词,但是这些术语通常用来表示传统的聚类分析之外的方法。例如,术语划分通常用在与将图分成子图相关的技术,与聚类并无太大联系。分割通常指使用简单的技术将数据分组;例如,图像可以根据像素亮度或颜色分割,人可以根据他们的收入分组。尽管如此,图划分、图像分割和市场分割的许多工作都与聚类分析有关。8.1.2不同的聚类类型整个簇集合通常称作聚类,本节我们将区分不同类型的聚类:层次的(嵌套的)与划分的(非嵌套的),互斥的、重叠的与模糊的,完全的与部分的。层次的与划分的不同类型的聚类之间最常讨论的差别是:簇的集合是嵌套的,还是非嵌套的;或者用更传统的术语,是层次的还是划分的。划分聚类(partitional clustering)简单地将数据对象集划分成不重叠的子集(簇),使得每个数据对象恰在一个子集中。例如,图8-1(b~d)中每个簇集都是一个划分聚类。如果允许簇具有子簇,则我们得到一个层次聚类(hierarchical clustering)层次聚类是嵌套簇的集族,组织成一棵树除叶结点外,树中每一个结点(簇)都是其子女(子簇)的并,而树根是包含所有对象的簇。通常(但并非总是),树叶是单个数据对象的单元素簇。如果允许簇嵌套,则图8-1a的一种解释是:它有两个子簇(图8-1b),其中每个子簇又各自具有3个子簇(图8-1d).图8-1(ad)中显示的簇也依次形成一个层次聚类,每层分别具有1,2,4和6个簇。最后,层次聚类可以看作划分聚类的序列,划分聚类可以通过取序列的任意成员得到,即通过在一个特定层剪断层次树得到。互斥的、重叠的与模糊的图8-1显示的簇都是互斥的(exclusive),因为每个对象都指派到单个簇。在有些情况下,可以合理地将一个点放到多个簇中,这种情况可以被非互斥聚类更好地处理。在最一般的意义下,重叠的(overlapping)或非互斥的(non--exclusive)聚类用来反映一个对象同时属于多个组(类)这一事实。例如,在大学里,一个人可能既是学生,又是雇员。当对象在两个或多个簇“之间”,并且可以合理地指派到这些簇中的任何一个时,也常常可以使用非互斥聚类。想象一个点在图8-1的两个簇中间。将它放到所有“同样好”的簇中,而不是任意地将它指派到单个簇中。
